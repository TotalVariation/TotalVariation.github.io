<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Learning the Backward Pass of FlashAttention | Xin's Homepage </title> <meta name="author" content="Xin Cai"> <meta name="description" content="Part I Derivations"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://totalvariation.github.io/blog/2025/intro-flashattention-backward-part1/"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?0afe9f0ae161375728f7bcc5eb5b4ab4"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Learning the Backward Pass of FlashAttention",
            "description": "Part I Derivations",
            "published": "July 21, 2025",
            "authors": [
              
              {
                "author": "Xin Cai",
                "authorURL": "https://totalvariation.github.io/",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Xin's Homepage </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/publications/">publications</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/projects/">projects</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/blog/">blog</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Learning the Backward Pass of FlashAttention</h1> <p>Part I Derivations</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#revisiting-derivatives">Revisiting Derivatives</a> </div> <div> <a href="#deriving-gradients-of-standard-attention">Deriving Gradients of Standard Attention</a> </div> <ul> <li> <a href="#by-matrix-calculus">By Matrix Calculus</a> </li> <li> <a href="#by-two-alternative-methods">By Two Alternative Methods</a> </li> </ul> <div> <a href="#concluding-remarks">Concluding Remarks</a> </div> <div> <a href="#acknowledgement">Acknowledgement</a> </div> </nav> </d-contents> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/essence-of-diff-blog-flashatten-480.webp 480w,/assets/img/essence-of-diff-blog-flashatten-800.webp 800w,/assets/img/essence-of-diff-blog-flashatten-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/essence-of-diff-blog-flashatten.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Image Credit: <a href="https://arxiv.org/abs/2501.14787" rel="external nofollow noopener" target="_blank">Matrix Calculus (for Machine Learning and Beyond)</a>. </div> <p>Scaling Transformers to longer sequence lengths has long been hindered by the computational bottleneck of self-attention, whose runtime and memory complexity scale quadratically with sequence length. FlashAttention and its subsequent versions <d-cite key="dao2022flashattention"></d-cite> <d-cite key="dao2023flashattention"></d-cite> have achieved dramatic memory savings and wall-clock speedups through a suite of carefully engineered techniques that minimize memory reads and writes between GPU high-bandwidth memory (HBM) and on-chip SRAM—with no approximation.</p> <p>While there are many excellent tutorials available online that provide an in-depth introduction to FlashAttention, such as the <a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&amp;t=22388s" rel="external nofollow noopener" target="_blank">YouTube video by Umar Jamil</a>, the backward pass remains relatively underexplored. For someone like me, who has grown accustomed to relying on automatic differentiation (AD) engines embedded in modern scientific computing frameworks like PyTorch, JAX, and Julia to handle derivatives, understanding the backward pass of FlashAttention can be a bit daunting.</p> <p>In this blog, I aim to give a detailed explanation about the backward pass of FlashAttention, breaking it down into two parts. In the first part, I will derive the relevant gradients using matrix calculus and validate the results using two alternative methods to highlight the elegance of matrix calculus. In the second part, we will walk through the Triton implementation of the backward pass to strengthen our understanding.</p> <h2 id="revisiting-derivatives">Revisiting Derivatives</h2> <p>Recall from single variable calculus, a real-valued function \(f\) defined in a neighbourhood of \(a \in \mathbb{R}\) is said to be differentiable at \(a\) if the limit</p> \[f^{\prime}(a) = \lim_{h \to 0} \frac{f(a+h) - f(a)}{h}\] <p>exists in \(\mathbb{R}\). However, such an expression does not easily lend itself to a more generalised definition of derivatives beyond scalar inputs, such as vectors, matrices, or functions. A more useful and fundamental way to view derivatives is the linear approximation of functions near a small neighbourhood of input values: \(\delta f = f(x + \delta x) - f(x) = f^{\prime}(x)\delta x + o(\delta x)\), or the differential form: \(df = f(x + dx) - f(x) = f^{\prime}(x)dx\). For example, let \(f : U \to \mathbb{R}\), where \(U \subseteq \mathbb{R}^n\) is open, i.e., a scalar-valued function which takes in a (column) vector \(x \in \mathbb{R}^n\) and produces a scalar in \(\mathbb{R}\). From college calculus, we know that \(df = \underbrace{\nabla f(x)^T}_{f^\prime(x)} dx\).</p> <p>More generally, by the Riesz Representation Theorem <d-footnote>I recommend checking out this self-contained article <a href="https://math.uchicago.edu/~may/REU2021/REUPapers/Adler.pdf" rel="external nofollow noopener" target="_blank">HILBERT SPACES AND THE RIESZ REPRESENTATION THEOREM</a> by Ben Adler, which gives a concise introduction to Hilbert spaces and the Riesz Representation theorem.</d-footnote>, for any continuous linear functional \(\phi\) on a Hilbert space <d-footnote>If a normed vector space $ V $ is a complete metric space and the norm itself is induced by an inner product on $ V $, we say $ V $ is a Hilbert space.</d-footnote> \(V\), there exists a unique \(u \in V\) such that \(\phi(v) = \langle u, v \rangle\) for all \(v \in V\), where \(\langle \cdot \rangle\) denotes inner product. Therefore, the derivative can be generalised to any Hilbert space as \(df = \langle \underbrace{\text{some vector}}_{\nabla f(x)},\: dx \rangle\).</p> <p>For the vector space consisting of matrices \(A \in \mathbb{R}^{m \times n}\), the default inner product is defined as \(\operatorname{Tr}(A^T B) = \operatorname{vec}(A)^T \operatorname{vec}(B) = \sum_{i, j}A_{ij}B_{ij}\), which is referred to as the Frobenius inner product, in order to make it a valid Hilbert space. Therefore, for a scalar-valued function that takes in matrices \(f(A)\), its derivative can be expressed as \(df = \operatorname{Tr}(f^\prime(A)^T dA)\).</p> <p>Next, we will leverage this trick from matrix calculus to derive the formulas for the backpropagation of standard attention.</p> <h2 id="deriving-gradients-of-standard-attention">Deriving Gradients of Standard Attention</h2> <h3 id="by-matrix-calculus">By Matrix Calculus</h3> <p>Given input sequences \(Q,\: K,\: V,\: \in \mathbb{R}^{N\times d}\) where \(N\) is the sequence length and \(d\) is the head dimension, the standard attention output \(O \in \mathbb{R}^{N\times d}\) is calculated as follows (forward pass):</p> \[S=QK^T \in \mathbb{R}^{N\times N}\quad P = \operatorname{softmax}(S) \quad O=PV \in \mathbb{R}^{N\times d}\] <p>where \(\operatorname{softmax}\) is applied row-wise.</p> <p>Then, assuming a scalar-valued loss function \(L\), by the backpropagation (i.e., reverse mode of automatic differentiation (AD)), the gradients of \(L\) w.r.t various inputs are calculated as follows:</p> \[\frac{\partial L}{\partial V} = P^T \frac{\partial L}{\partial O} \in \mathbb{R}^{N\times d}\] \[\frac{\partial L}{\partial P} = \frac{\partial L}{\partial O} V^T \in \mathbb{R}^{N\times N}\] \[\frac{\partial L}{\partial S} = \operatorname{dsoftmax}(\frac{\partial L}{\partial P}) \in \mathbb{R}^{N\times N}\] \[\frac{\partial L}{\partial Q} = \frac{\partial L}{\partial S}K \in \mathbb{R}^{N\times d}\] \[\frac{\partial L}{\partial K} = \frac{\partial L}{\partial S}^T Q \in \mathbb{R}^{N\times d}\] <p>First, we calculate the gradient w.r.t. \(V\) (\(\frac{\partial L}{\partial V} = P^T \frac{\partial L}{\partial O} \in \mathbb{R}^{N\times d}\))</p> <p>Fix \(P\) and vary \(V\). From above, \(dO = P(V + dV) - PV = P dV\). The differential of \(dL\) becomes:</p> \[dL = \text{Tr} \left( \left( \frac{\partial L}{\partial O} \right)^T dO \right) = \text{Tr} \left( \left( \frac{\partial L}{\partial O} \right)^T (P dV) \right).\] <p>Substitute \(dO = P dV\):</p> \[dL = \text{Tr} \left( \left( \frac{\partial L}{\partial O} \right)^T P dV \right) = \text{Tr} \left( \underbrace{\left( P^T \frac{\partial L}{\partial O} \right)^T}_{\frac{\partial L}{\partial V}} dV \right)\] <p>Therefore, we get \(\frac{\partial L}{\partial V} = P^T \frac{\partial L}{\partial O} \in \mathbb{R}^{N\times d}\).</p> <p>Similarly, the gradient w.r.t \(P\) (\(\frac{\partial L}{\partial P} = \frac{\partial L}{\partial O} V^T \in \mathbb{R}^{N\times N}\)) is derived as, given \(dO = (P + dP)V - PV = dP V\):</p> \[dL = \text{Tr} \left( \left( \frac{\partial L}{\partial O} \right)^T dO \right) = \text{Tr} \left( \left( \frac{\partial L}{\partial O} \right)^T dP V \right) = \text{Tr} \left( V \left( \frac{\partial L}{\partial O} \right)^T dP \right) = \text{Tr} \left( \underbrace{\left( \frac{\partial L}{\partial O} V^T \right)^T}_{\frac{\partial L}{\partial P}} dP \right)\] <p>where the cyclic property of trace is applied \(\operatorname{Tr}(ABC) = \operatorname{Tr}(BCA)\). Therefore, we get \(\frac{\partial L}{\partial P} = \frac{\partial L}{\partial O} V^T \in \mathbb{R}^{N\times N}\).</p> <p>In \(P = \operatorname{softmax}(S)\), as \(\operatorname{softmax}\) is applied row-wise, it is more appropriate to consider the input as each row of \(S\), i.e., \(P_{i, :} = \operatorname{softmax}(S_{i, :})\) when deriving the gradient formula.</p> \[dL = \left( \frac{\partial L}{\partial P_{i, :}} \right)^T dP_{i, :} = \left( \frac{\partial L}{\partial P_{i, :}} \right)^T \operatorname{dsoftmax}(dS_{i, :})\] <p>The derivative \(df_x\) of a function \(f\) mapping vectors from \(\mathbb{R}^n \to \mathbb{R}^n\) is a linear transformation \(df_x \in \mathcal{L}(\mathbb{R}^n, \mathbb{R}^n)\), which can be expressed in its matrix form (Jacobian matrix). The Jocobian matrix (i.e., the derivative) of \(y = \operatorname{softmax}(x)\), where \(x \in \mathbb{R}^n\) is a column vector, is an \(n \times n\) symmetric matrix, \(\operatorname{dsoftmax}=\text{diag}(y) - yy^T=\operatorname{dsoftmax}^T\).</p> <p>With the above derivation, we can proceed as follows:</p> \[dL = \left( \frac{\partial L}{\partial P_{i, :}} \right)^T \operatorname{dsoftmax}(dS_{i, :}) = \left( \operatorname{dsoftmax}^T \frac{\partial L}{\partial P_{i, :}} \right)^T dS_{i, :} = \left( \operatorname{dsoftmax} \frac{\partial L}{\partial P_{i, :}} \right)^T dS_{i, :}\] <p>Therefore, we arrive at \(\frac{\partial L}{\partial S_{i, :}} = \operatorname{dsoftmax}(\frac{\partial L}{\partial P_{i, :}})\). With a slight abuse of notation, it can be compactly written as \(\frac{\partial L}{\partial S} = \operatorname{dsoftmax}(\frac{\partial L}{\partial P}) \in \mathbb{R}^{N \times N}\).</p> <h3 id="by-two-alternative-methods">By Two Alternative Methods</h3> <p>Next, we will resort to two alternative methods to verify the correctness of our previous derivations. Yet you will find them a bit more cumbersome.</p> <p><strong>Component-wise</strong></p> <p>Recall that in multivariable calculus, Let \(U \subseteq \mathbb{R}^n\) and \(V \subseteq \mathbb{R}^m\) be open and \(f:\: U \to \mathbb{R}^m,\: g:\: V \to \mathbb{R}^k\) with \(f(U) \subseteq V\). Let \(f\) be differentiable on \(U\) and \(g\) differentiable on \(V\). Set \(y = f(x)\) and \(z = (g \circ f)(x) = g(y)\). Then the chain rule \((g \circ f)^\prime(x) = g^\prime(y)f^\prime(x)\) can be written in its matrix form:</p> \[\begin{bmatrix}\frac{\partial z_1}{\partial x_1} &amp; \dots &amp;\frac{\partial z_1}{\partial x_n}\\ \vdots &amp; &amp; \vdots\\ \frac{\partial z_k}{\partial x_1} &amp; \dots &amp;\frac{\partial z_k}{\partial x_n} \end{bmatrix} = \begin{bmatrix}\frac{\partial z_1}{\partial y_1} &amp; \dots &amp;\frac{\partial z_1}{\partial y_m}\\ \vdots &amp; &amp; \vdots\\ \frac{\partial z_k}{\partial y_1} &amp; \dots &amp;\frac{\partial z_k}{\partial y_m} \end{bmatrix} \begin{bmatrix}\frac{\partial y_1}{\partial x_1} &amp; \dots &amp;\frac{\partial y_1}{\partial x_n}\\ \vdots &amp; &amp; \vdots\\ \frac{\partial y_m}{\partial x_1} &amp; \dots &amp;\frac{\partial y_m}{\partial x_n} \end{bmatrix}\] <p>In the special case where \(g:\: V \to \mathbb{R}\), the matrix form of the chain rule is expressed as follows:</p> \[\left[\frac{\partial L}{\partial x_1}, \dots, \frac{\partial L}{\partial x_n} \right] = \left[ \frac{\partial L}{\partial y_1}, \dots, \frac{\partial L}{\partial y_m} \right] \begin{bmatrix}\frac{\partial y_1}{\partial x_1} &amp; \dots &amp;\frac{\partial y_1}{\partial x_n}\\ \vdots &amp; &amp; \vdots\\ \frac{\partial y_m}{\partial x_1} &amp; \dots &amp;\frac{\partial y_m}{\partial x_n} \end{bmatrix}\] <p>If written in component-wise form, we arrive at the familiar formula:</p> \[\frac{\partial L}{\partial x_j} = \sum_{k=1}^m \frac{\partial L}{\partial y_k}\frac{\partial y_k}{\partial x_j}\] <p>To prove \(\frac{\partial L}{\partial V} = P^T \frac{\partial L}{\partial O} \in \mathbb{R}^{N\times d}\) using the (multivariable) chain rule, as it only works for vectors as inputs, we should consider each column \(V_{:, j}\) seperately, i.e., \(O_{:, j} = P V_{:, j}\). Yet the derived outcomes from such a workaround can be effortlessly transferred to the matrix \(V\), as each column \(V_{:, j}\) shares the same Jacobian matrix \(P\).</p> <p>As \(O_{kj} = \sum_{m=1}^N P_{km} V_{mj}\), we get \(\frac{\partial O_{kj}}{\partial V_{ij}} = P_{ki}\). Or, it can be read off from the Jacobian matrix \(P\) as the k-th row and i-th column element (\(\frac{\partial O[:, j]_{k}}{\partial V[:, j]_{i}}\)).</p> <p>Applying the chain rule, we get</p> \[\frac{\partial L}{\partial V_{ij}} = \sum_{k=1}^N \frac{\partial L}{\partial O_{kj}} \frac{\partial O_{kj}}{\partial V_{ij}} = \sum_{k=1}^N \frac{\partial L}{\partial O_{kj}} P_{ki} = (P^T \frac{\partial L}{\partial O})_{ij}\] <p>Similarly, we can prove \(\frac{\partial L}{\partial P} = \frac{\partial L}{\partial O} V^T \in \mathbb{R}^{N\times N}\) by treating each row \(P_{i,:}\) independently which shares the same Jacobian matrix \(V^T\) (\(O^T = V^T P^T\)): \(O_{ik} = \sum_{m=1}^N P_{im} V_{mk}\), so \(\frac{\partial O_{ik}}{\partial P_{ij}} = V_{jk}\). Or, it can be read off from the Jacobian matrix \(V^T\) as the k-th row and j-th column element \(V^T_{kj}\) (\(\frac{\partial O[i, :]_{k}}{\partial P[i, :]_{j}}\)).</p> \[\frac{\partial L}{\partial P_{ij}} = \sum_{k=1}^d \frac{\partial L}{\partial O_{ik}} \frac{\partial O_{ik}}{\partial P_{ij}} = \sum_{k=1}^d \frac{\partial L}{\partial O_{ik}} V_{jk} = \left( \frac{\partial L}{\partial O} V^T \right)_{ij}\] <p><strong>Matrix vectorisation and the Kronecker product</strong></p> <p>Actually, it is legitimate to directly work with the Jacobian matrix of matrix inputs/outputs, as any linear operator that transforms vectors between finite-dimensional vector spaces can be expressed in its matrix form once the bases for the input and output vector spaces have been selected. The most common way to achieve this involves matrix vectorisation and the Kronecker product.</p> <p>The vectorization \(\text{vec}(A) \in \mathbb{R}^{mn}\) of any \(m \times n\) matrix \(A \in \mathbb{R}^{m \times n}\) is defined by simply stacking the columns of \(A\), from left to right, into a column vector \(\text{vec}(A)\). That is, if we denote the \(n\) columns of \(A\) by m-component vectors \(\overrightarrow{a_1}, \overrightarrow{a_2}, \dots, \overrightarrow{a_n} \in \mathbb{R}^m\), then</p> \[\text{vec}(A) = \text{vec}(\left[ \overrightarrow{a_1}, \overrightarrow{a_2}, \dots, \overrightarrow{a_n} \right]) = \begin{pmatrix} \overrightarrow{a_1}\\ \overrightarrow{a_2}\\ \vdots \\ \overrightarrow{a_n} \end{pmatrix} \in \mathbb{R}^{mn}\] <p>Then, the Kronecker-product identity that plays a key role in the derivation is:</p> \[(A \otimes B) \text{vec}(C) = \text{vec}(BCA^T)\] <p>where \(A, B, C\) are compactly sized matrices. There are two special cases derived from the above Kronecker-product identity:</p> \[(I \otimes B) \text{vec}(C) = \text{vec}(BC)\] \[(A \otimes I) \text{vec}(C) = \text{vec}(CA^T )\] <details><summary>Click here to know more</summary> <p>Proof:</p> <p>To prove \((I \otimes B) \text{vec}(C) = \text{vec}(BC)\), assume \(B \in \mathbb{R}^{n \times m}\) and \(C \in \mathbb{R}^{m \times k}\),</p> \[BC = B \left[ \overrightarrow{c_1}, \overrightarrow{c_2}, \dots, \overrightarrow{c_k} \right] = \left[ B\overrightarrow{c_1}, B\overrightarrow{c_2}, \dots, B\overrightarrow{c_k} \right]\] \[\text{vec}(BC) = \begin{pmatrix} B\overrightarrow{c_1}\\ B\overrightarrow{c_2}\\ \vdots \\ B\overrightarrow{c_k} \end{pmatrix} = \underbrace{\begin{pmatrix} B &amp; &amp; &amp; \\ &amp; B &amp; &amp; \\ &amp; &amp; \ddots &amp; \\ &amp; &amp; &amp; B \end{pmatrix}}_{\left( I \otimes B \right)} \begin{pmatrix} \overrightarrow{c_1}\\ \overrightarrow{c_2}\\ \vdots \\ \overrightarrow{c_k} \end{pmatrix}\] <p>where \(I \in \mathbb{R}^{k \times k}\).</p> <p>To prove \((A \otimes I) \text{vec}(C) = \text{vec}(CA^T )\), assume \(A \in \mathbb{R}^{n \times k}\) and \(C \in \mathbb{R}^{m \times k}\),</p> \[CA^T = \left[ \sum_{j=1}^k a_{1j} \overrightarrow{c_j}, \sum_{j=1}^k a_{2j} \overrightarrow{c_j}, \dots, \sum_{j=1}^k a_{nj} \overrightarrow{c_j} \right]\] \[\text{vec}(CA^T) = \begin{pmatrix} \sum_{j=1}^k a_{1j} \overrightarrow{c_j}\\ \sum_{j=1}^k a_{2j} \overrightarrow{c_j}\\ \vdots \\ \sum_{j=1}^k a_{nj} \overrightarrow{c_j} \end{pmatrix} = \underbrace{\begin{pmatrix} a_{11}I &amp; a_{12}I &amp; \cdots &amp; a_{1k}I \\ a_{21}I &amp; a_{22}I &amp; \cdots &amp; a_{2k}I \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ a_{n1}I &amp; a_{n2}I &amp; \cdots &amp; a_{nk}I \end{pmatrix}}_{(A \otimes I)} \begin{pmatrix} \overrightarrow{c_1} \\ \overrightarrow{c_2}\\ \vdots \\ \overrightarrow{c_k} \end{pmatrix}\] <p>where \(I \in \mathbb{R}^{m \times m}\).</p> <p>To prove \((A \otimes B) \text{vec}(C) = \text{vec}(BCA^T)\),</p> \[\text{vec}(BCA^T) = (I \otimes B) \text{vec}(CA^T) = (I \otimes B)(A \otimes I) \text{vec}(C) = (A \otimes B) \text{vec}(C)\] <p>where we used the property of the Kronecker product \((A \otimes B)(C \otimes D) = (AC \otimes BD)\).</p> </details> <p>We are now equipped with all the necessary tools to provide a slightly more elegant way to prove gradients involving matrix inputs/outputs than chunking matrices into column vectors as done previously.</p> <p>To prove \(\frac{\partial L}{\partial V} = P^T \frac{\partial L}{\partial O} \in \mathbb{R}^{N\times d}\), as \(dO = P dV\), by vectorisation,</p> \[\text{vec}(dO) = \text{vec}(P dV) = \underbrace{(I \otimes P)}_{\text{Jacobian matrix :}\; \frac{\partial O}{\partial V}} \text{vec}(dV)\] <p>Recall the special case of multivariable chain rule \(z = (g \circ f)(x) = g(y)\) where \(g:\: V \to \mathbb{R}\),</p> \[\left[\frac{\partial L}{\partial x_1}, \dots, \frac{\partial L}{\partial x_n} \right] = \left[ \frac{\partial L}{\partial y_1}, \dots, \frac{\partial L}{\partial y_m} \right] \begin{bmatrix}\frac{\partial y_1}{\partial x_1} &amp; \dots &amp;\frac{\partial y_1}{\partial x_n}\\ \vdots &amp; &amp; \vdots\\ \frac{\partial y_m}{\partial x_1} &amp; \dots &amp;\frac{\partial y_m}{\partial x_n} \end{bmatrix}\] <p>We get</p> \[\text{vec}({\frac{\partial L}{\partial V}})^T = \text{vec}({\frac{\partial L}{\partial O}})^T (I \otimes P)\] \[\text{vec}({\frac{\partial L}{\partial V}}) = (I \otimes P)^T \text{vec}({\frac{\partial L}{\partial O}}) = (I \otimes P^T) \text{vec}({\frac{\partial L}{\partial O}}) = \text{vec}(P^T \frac{\partial L}{\partial O}) \quad \text{QED}\] <p>where we used one property of Kronecker-product \((A \otimes B)^T = A^T \otimes B^T\).</p> <p>Similarly, the proof of \(\frac{\partial L}{\partial P} = \frac{\partial L}{\partial O} V^T \in \mathbb{R}^{N\times N}\) is shown as follows:</p> \[dO = dP V\] \[\text{vec}(dO) = \text{vec}(dP V) = \underbrace{(V^T \otimes I)}_{\text{Jacobian matrix :}\; \frac{\partial O}{\partial P}} \text{vec}(dP)\] \[\text{vec}({\frac{\partial L}{\partial P}})^T = \text{vec}({\frac{\partial L}{\partial O}})^T (V^T \otimes I)\] \[\text{vec}({\frac{\partial L}{\partial P}}) = (V^T \otimes I)^T \text{vec}({\frac{\partial L}{\partial O}}) = (V \otimes I) \text{vec}({\frac{\partial L}{\partial O}}) = \text{vec}({\frac{\partial L}{\partial O}} V^T) \quad \text{QED}\] <p>It can be seen that using the trick of matrix vectorisation is more conceptually clear than dealing with gradients involving matrices in a component-wise manner. Yet it is still a bit cumbersome compared to techniques in matrix calculus.</p> <h2 id="concluding-remarks">Concluding Remarks</h2> <p>While matrix calculus plays a pivotal role in machine learning and large-scale optimisation, the advent of automatic differentiation (AD) engines in modern scientific computing libraries has largely relieved practitioners from the burden of manually computing derivatives of complex structures like matrices and higher-order tensors. Nevertheless, matrix calculus remains a powerful tool, well worthy to be included in your analytical arsenal.</p> <p>In <a href="https://totalvariation.github.io/blog/2025/intro-flashattention-backward-part2/" rel="external nofollow noopener" target="_blank">part 2</a> of this blog, we will walk through the implementation of the backward pass of FlashAttention in Triton.</p> <h2 id="acknowledgement">Acknowledgement</h2> <p>We express our gratitude to Matrix Calculus (for Machine Learning and Beyond) <d-cite key="bright2025matrix"></d-cite>, a set of lecture notes compiled from the MIT OpenCourse <a href="https://ocw.mit.edu/courses/18-s096-matrix-calculus-for-machine-learning-and-beyond-january-iap-2023/" rel="external nofollow noopener" target="_blank">Matrix Calculus for Machine Learning and Beyond</a>. We warmly recommend these materials to readers seeking for a deeper, principle-driven understanding of differentiation in the context of large-scale computing.</p> <div class="language-html highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@misc{Cai2025flashattnbackward-1,
    author = {Xin Cai},
    title = {Learning the Backward Pass of FlashAttention: Part I Derivations},
    howpublished = {\url{https://totalvariation.github.io/blog/2025/intro-flashattention-backward-part1/}},
    note = {Accessed: 2025-07-21},
    year = {2025}
}
</code></pre></div></div> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/2025-07-21-intro-flashattention-backward-part1.bib"></d-bibliography> <div id="giscus_thread" style="max-width: 800px; margin: 0 auto;"> <script>let giscusTheme=localStorage.getItem("theme"),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"TotalVariation/TotalVariation.github.io","data-repo-id":"","data-category":"Comments","data-category-id":"","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,a])=>giscusScript.setAttribute(t,a)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Xin Cai. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>