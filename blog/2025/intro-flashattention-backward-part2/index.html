<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Learning the Backward Pass of FlashAttention | Xin's Homepage </title> <meta name="author" content="Xin Cai"> <meta name="description" content="Part II Implementation in Triton"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://totalvariation.github.io/blog/2025/intro-flashattention-backward-part2/"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?0afe9f0ae161375728f7bcc5eb5b4ab4"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Learning the Backward Pass of FlashAttention",
            "description": "Part II Implementation in Triton",
            "published": "July 21, 2025",
            "authors": [
              
              {
                "author": "Xin Cai",
                "authorURL": "https://totalvariation.github.io/",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Xin's Homepage </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/publications/">publications</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/projects/">projects</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/blog/">blog</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Learning the Backward Pass of FlashAttention</h1> <p>Part II Implementation in Triton</p> </d-title> <d-byline></d-byline> <d-article> <h2 id="recap-forward-and-backward-passes-of-standard-attention">Recap Forward and Backward Passes of Standard Attention</h2> <p>In the first part of this tutorial, we have walked through a detailed derivation of formulas used in the backward pass of standard attention. For ease of reference, they are included as follows:</p> <p>Given input sequences \(Q,\: K,\: V,\: \in \mathbb{R}^{N\times d}\) where \(N\) is the sequence length and \(d\) is the head dimension, the standard attention output \(O \in \mathbb{R}^{N\times d}\) is calculated as follows (forward pass):</p> \[S=QK^T \in \mathbb{R}^{N\times N}\quad P = \operatorname{softmax}(S) \quad O=PV \in \mathbb{R}^{N\times d}\] <p>where \(\operatorname{softmax}\) is applied row-wise.</p> <p>Then, assuming a scalar-valued loss function \(L\), by the backpropagation (i.e., reverse mode of automatic differentiation (AD)), the gradients of \(L\) w.r.t various inputs are calculated as follows:</p> \[\frac{\partial L}{\partial V} = P^T \frac{\partial L}{\partial O} \in \mathbb{R}^{N\times d}\] \[\frac{\partial L}{\partial P} = \frac{\partial L}{\partial O} V^T \in \mathbb{R}^{N\times N}\] \[\frac{\partial L}{\partial S} = \operatorname{dsoftmax}(\frac{\partial L}{\partial P}) \in \mathbb{R}^{N\times N}\] \[\frac{\partial L}{\partial Q} = \frac{\partial L}{\partial S}K \in \mathbb{R}^{N\times d}\] \[\frac{\partial L}{\partial K} = \frac{\partial L}{\partial S}^T Q \in \mathbb{R}^{N\times d}\] <h2 id="the-implementation-of-the-backward-pass-of-flashattention-in-triton">The Implementation of the Backward Pass of FlashAttention in Triton</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/flashattn-backward-pseudocode-480.webp 480w,/assets/img/flashattn-backward-pseudocode-800.webp 800w,/assets/img/flashattn-backward-pseudocode-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/flashattn-backward-pseudocode.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Image Credit: FlashAttention2 paper. </div> <p>To construct a direct correspondence between the mathematical equations and Triton code, we replace \(\frac{\partial L}{\partial V}\) with \(dV\) with a slight abuse of notation <d-footnote>Please note that $ dV $ hereafter will no longer denote differential.</d-footnote>, as in the backward pass, the matrix \(dV\) contains the gradient of scalar-valued loss function \(L\) w.r.t. \(V\), i.e., \(\frac{\partial L}{\partial V}\). By applying similar replacements to all the other variables, we therefore obtain the following equations adopted in the FlashAttention2 paper <d-cite key="dao2023flashattention"></d-cite>:</p> \[dV = P^T dO \in \mathbb{R}^{N\times d}\] \[dP = dOV^T \in \mathbb{R}^{N\times N}\] \[dS = \operatorname{dsoftmax}(dP) \in \mathbb{R}^{N\times N}\] \[dQ = dSK \in \mathbb{R}^{N\times d}\] \[dK = dS^T Q \in \mathbb{R}^{N\times d}\] <p>Another trick adopted in the FlashAttention paper <d-cite key="dao2022flashattention"></d-cite> is to simplify the calculation of \(dS = \operatorname{dsoftmax}(dP)\), which is clearly derived in its appendix.</p> <p>For self-containedness, it is included as follows, (Please note \(dS_{i,:}, dP_{i,:}\) are all column vectors):</p> \[dS_{i,:} = \operatorname{dsoftmax}dP_{i,:} = (\text{diag}(P_{i,:}) - P_{i,:}P_{i,:}^T)dP_{i,:} = P_{i,:} \circ dP_{i,:} - \left( P_{i,:}^T dP_{i,:} \right) P_{i,:}\] <p>where \(\circ\) denotes Hadamard product (i.e., pointwise multiplication).</p> <p>Recall that \(dP = dO V^T\), written in element-wise form, \(dP_{ij} = do_i^T v_j\), (Please note \(do_j, v_j, k_j\) here denote the j-th row of \(dO, V, K\) respectively, acting as a column vector.)</p> <p>Now, we can define</p> \[D_i = P_{i,:}^T dP_{i,:} = \sum_j \frac{\exp(q_i^T k_j)}{L_i} do_i^T v_j = do_i^T \sum_j \frac{\exp(q_i^T k_j)}{L_i} v_j = do_i^T o_i\] <p>then \(dS_{i,:} = P_{i,:} \circ dP_{i,:} - D_i P_{i,:}\).</p> <p>Readers seeking a comprehensive treatment (e.g., the online-softmax trick in the forward pass) of FlashAttention are encouraged to refer to the original papers <d-cite key="dao2022flashattention"></d-cite> <d-cite key="dao2023flashattention"></d-cite> or other tutorials available online focusing on the forward pass.</p> <p>Now, we are in a position to dive into the Triton implementation of the backward pass of FlashAttention2.</p> <p>We assume readers have a basic familiarity with Triton. Otherwise, there are many excellent Triton tutorials, including the <a href="https://triton-lang.org/main/getting-started/tutorials/index.html" rel="external nofollow noopener" target="_blank">official ones</a>, available online for your reference. In my view, figuring out how to move pointers to accurately access blocks of elements (i.e., load and store) in parallelly launched Triton programs is sufficient to grasp the core mechanisms of custom kernels developed in Triton.</p> <p>Instead of using <code class="language-plaintext highlighter-rouge">block pointer</code> defined by <code class="language-plaintext highlighter-rouge">make_block_ptr</code>, I find that directly working with N-dimensional pointers to access elements in memory is more straightforward. Furthermore, <code class="language-plaintext highlighter-rouge">mask</code> and <code class="language-plaintext highlighter-rouge">other</code> are implicitly broadcast to <code class="language-plaintext highlighter-rouge">pointer.shape</code> when using N-dimensional pointers, which can be conveniently used to handle boundary conditions.</p> <p>In the following, I will give some visual illustrations to facilitate your understanding of how <code class="language-plaintext highlighter-rouge">tl.load()</code> works, as there is no difference in read (<code class="language-plaintext highlighter-rouge">tl.load()</code>) and write (<code class="language-plaintext highlighter-rouge">tl.store()</code>) operations as long as their indexes are specified correctly.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

  <span class="n">N</span> <span class="o">=</span> <span class="mi">8</span>
  <span class="c1"># Here, the content of the array is made intentionally to be the exact same as offsets relative to the base pointer.
</span>  <span class="c1"># Please note that in Triton language, all Pytorch tensors are implicitly converted to base pointers.
</span>
  <span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">N</span> <span class="o">*</span> <span class="n">N</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span>
  <span class="nf">print</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
  
  <span class="p">[[</span> <span class="mi">0</span>  <span class="mi">1</span>  <span class="mi">2</span>  <span class="mi">3</span>  <span class="mi">4</span>  <span class="mi">5</span>  <span class="mi">6</span>  <span class="mi">7</span><span class="p">]</span>
   <span class="p">[</span> <span class="mi">8</span>  <span class="mi">9</span> <span class="mi">10</span> <span class="mi">11</span> <span class="mi">12</span> <span class="mi">13</span> <span class="mi">14</span> <span class="mi">15</span><span class="p">]</span>
   <span class="p">[</span><span class="mi">16</span> <span class="mi">17</span> <span class="mi">18</span> <span class="mi">19</span> <span class="mi">20</span> <span class="mi">21</span> <span class="mi">22</span> <span class="mi">23</span><span class="p">]</span>
   <span class="p">[</span><span class="mi">24</span> <span class="mi">25</span> <span class="mi">26</span> <span class="mi">27</span> <span class="mi">28</span> <span class="mi">29</span> <span class="mi">30</span> <span class="mi">31</span><span class="p">]</span>
   <span class="p">[</span><span class="mi">32</span> <span class="mi">33</span> <span class="mi">34</span> <span class="mi">35</span> <span class="mi">36</span> <span class="mi">37</span> <span class="mi">38</span> <span class="mi">39</span><span class="p">]</span>
   <span class="p">[</span><span class="mi">40</span> <span class="mi">41</span> <span class="mi">42</span> <span class="mi">43</span> <span class="mi">44</span> <span class="mi">45</span> <span class="mi">46</span> <span class="mi">47</span><span class="p">]</span>
   <span class="p">[</span><span class="mi">48</span> <span class="mi">49</span> <span class="mi">50</span> <span class="mi">51</span> <span class="mi">52</span> <span class="mi">53</span> <span class="mi">54</span> <span class="mi">55</span><span class="p">]</span>
   <span class="p">[</span><span class="mi">56</span> <span class="mi">57</span> <span class="mi">58</span> <span class="mi">59</span> <span class="mi">60</span> <span class="mi">61</span> <span class="mi">62</span> <span class="mi">63</span><span class="p">]]</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="n">BLOCK_M</span> <span class="o">=</span> <span class="mi">2</span>
  <span class="n">col_dim</span> <span class="o">=</span> <span class="n">N</span>

  <span class="n">stride_row</span> <span class="o">=</span> <span class="n">N</span>
  <span class="n">stride_col</span> <span class="o">=</span> <span class="mi">1</span>

  <span class="n">offs_m</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">BLOCK_M</span><span class="p">)[:,</span> <span class="bp">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">stride_row</span> <span class="o">+</span> <span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">col_dim</span><span class="p">)[</span><span class="bp">None</span><span class="p">,</span> <span class="p">:]</span> <span class="o">*</span> <span class="n">stride_col</span>

  <span class="c1"># N-dimensional tensors are stored contiguously in memory. 
</span>  <span class="c1"># Otherwise, it would be recommended to call x.contiguous() before taking any tensor operations. 
</span>  <span class="c1"># Here, we mimic this feature with np.ndarray.flatten.
</span>
  <span class="c1"># illustrate loading tensors from memory
</span>  <span class="nf">print</span><span class="p">(</span><span class="n">A</span><span class="p">.</span><span class="nf">flatten</span><span class="p">()[</span><span class="n">offs_m</span><span class="p">])</span>

  <span class="p">[[</span> <span class="mi">0</span>  <span class="mi">1</span>  <span class="mi">2</span>  <span class="mi">3</span>  <span class="mi">4</span>  <span class="mi">5</span>  <span class="mi">6</span>  <span class="mi">7</span><span class="p">]</span>
   <span class="p">[</span> <span class="mi">8</span>  <span class="mi">9</span> <span class="mi">10</span> <span class="mi">11</span> <span class="mi">12</span> <span class="mi">13</span> <span class="mi">14</span> <span class="mi">15</span><span class="p">]]</span>

</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="c1"># illustrate moving blocks `step_size` rows down, which will be used in the for loop to 
</span>  <span class="c1"># traverse over one dimension of a tensor.
</span>  <span class="n">step_size</span> <span class="o">=</span> <span class="mi">2</span>
  <span class="nf">print</span><span class="p">(</span><span class="n">A</span><span class="p">.</span><span class="nf">flatten</span><span class="p">()[</span><span class="n">offs_m</span> <span class="o">+</span> <span class="n">step_size</span> <span class="o">*</span> <span class="n">N</span><span class="p">])</span>

  <span class="p">[[</span><span class="mi">16</span> <span class="mi">17</span> <span class="mi">18</span> <span class="mi">19</span> <span class="mi">20</span> <span class="mi">21</span> <span class="mi">22</span> <span class="mi">23</span><span class="p">]</span>
   <span class="p">[</span><span class="mi">24</span> <span class="mi">25</span> <span class="mi">26</span> <span class="mi">27</span> <span class="mi">28</span> <span class="mi">29</span> <span class="mi">30</span> <span class="mi">31</span><span class="p">]]</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="c1"># illustrate loading tensors directly in its transposed version and moving blocks accordingly
</span>  <span class="n">offs_m_T</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">BLOCK_M</span><span class="p">)[</span><span class="bp">None</span><span class="p">,</span> <span class="p">:]</span> <span class="o">*</span> <span class="n">stride_row</span> <span class="o">+</span> <span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">col_dim</span><span class="p">)[:,</span> <span class="bp">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">stride_col</span>
  <span class="nf">print</span><span class="p">(</span><span class="n">A</span><span class="p">.</span><span class="nf">flatten</span><span class="p">()[</span><span class="n">offs_m_T</span><span class="p">])</span>
  <span class="nf">print</span><span class="p">(</span><span class="n">A</span><span class="p">.</span><span class="nf">flatten</span><span class="p">()[</span><span class="n">offs_m_T</span> <span class="o">+</span> <span class="n">step_size</span> <span class="o">*</span> <span class="n">N</span><span class="p">])</span>

  <span class="p">[[</span> <span class="mi">0</span>  <span class="mi">8</span><span class="p">]</span>
   <span class="p">[</span> <span class="mi">1</span>  <span class="mi">9</span><span class="p">]</span>
   <span class="p">[</span> <span class="mi">2</span> <span class="mi">10</span><span class="p">]</span>
   <span class="p">[</span> <span class="mi">3</span> <span class="mi">11</span><span class="p">]</span>
   <span class="p">[</span> <span class="mi">4</span> <span class="mi">12</span><span class="p">]</span>
   <span class="p">[</span> <span class="mi">5</span> <span class="mi">13</span><span class="p">]</span>
   <span class="p">[</span> <span class="mi">6</span> <span class="mi">14</span><span class="p">]</span>
   <span class="p">[</span> <span class="mi">7</span> <span class="mi">15</span><span class="p">]]</span>

  <span class="p">[[</span><span class="mi">16</span> <span class="mi">24</span><span class="p">]</span>
   <span class="p">[</span><span class="mi">17</span> <span class="mi">25</span><span class="p">]</span>
   <span class="p">[</span><span class="mi">18</span> <span class="mi">26</span><span class="p">]</span>
   <span class="p">[</span><span class="mi">19</span> <span class="mi">27</span><span class="p">]</span>
   <span class="p">[</span><span class="mi">20</span> <span class="mi">28</span><span class="p">]</span>
   <span class="p">[</span><span class="mi">21</span> <span class="mi">29</span><span class="p">]</span>
   <span class="p">[</span><span class="mi">22</span> <span class="mi">30</span><span class="p">]</span>
   <span class="p">[</span><span class="mi">23</span> <span class="mi">31</span><span class="p">]]</span>
</code></pre></div></div> <p>Here, we analyse a simplified version of FlashAttention (technically, FlashAttention2) adapted from the official Triton tutorial <a href="https://triton-lang.org/main/getting-started/tutorials/06-fused-attention.html#fused-attention" rel="external nofollow noopener" target="_blank">Fused Attention</a>, accounting for both the ‘Causal’ and ‘Non-Causal’ modes.</p> <p>The implementation of the backward pass of FlashAttention can be generally grouped into three stages:</p> <ol> <li> <p>Calculate the matrix \(D\) first as a preprocessing step, where \(D_i = do_i^T o_i\), which corresponds to the variable <code class="language-plaintext highlighter-rouge">delta = torch.empty_like(M)</code>. Its size is <code class="language-plaintext highlighter-rouge">(Batch, Num_Heads, N_CTX)</code>, and is realised in the function <code class="language-plaintext highlighter-rouge">_attn_bwd_preprocess()</code>.</p> </li> <li> <p>Calculate \(dV, dK\) via the function <code class="language-plaintext highlighter-rouge">_attn_bwd_dkdv()</code>.</p> </li> <li> <p>Calculate \(dQ\) via the function <code class="language-plaintext highlighter-rouge">_attn_bwd_dq()</code>.</p> </li> </ol> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="nd">@triton.jit</span>
  <span class="k">def</span> <span class="nf">_attn_bwd_preprocess</span><span class="p">(</span><span class="n">O</span><span class="p">,</span> <span class="n">DO</span><span class="p">,</span>  <span class="c1">#
</span>                           <span class="n">Delta</span><span class="p">,</span>  <span class="c1">#
</span>                           <span class="n">Z</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">N_CTX</span><span class="p">,</span>  <span class="c1">#
</span>                           <span class="n">BLOCK_M</span><span class="p">:</span> <span class="n">tl</span><span class="p">.</span><span class="n">constexpr</span><span class="p">,</span> <span class="n">HEAD_DIM</span><span class="p">:</span> <span class="n">tl</span><span class="p">.</span><span class="n">constexpr</span>  <span class="c1">#
</span>                           <span class="p">):</span>
      <span class="n">off_m</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">program_id</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="o">*</span> <span class="n">BLOCK_M</span> <span class="o">+</span> <span class="n">tl</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">BLOCK_M</span><span class="p">)</span>
      <span class="n">off_hz</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">program_id</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
      <span class="n">off_n</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">HEAD_DIM</span><span class="p">)</span>
      <span class="c1"># load
</span>      <span class="n">o</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="n">O</span> <span class="o">+</span> <span class="n">off_hz</span> <span class="o">*</span> <span class="n">HEAD_DIM</span> <span class="o">*</span> <span class="n">N_CTX</span> <span class="o">+</span> <span class="n">off_m</span><span class="p">[:,</span> <span class="bp">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">HEAD_DIM</span> <span class="o">+</span> <span class="n">off_n</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="p">:]).</span><span class="nf">to</span><span class="p">(</span><span class="n">tl</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
      <span class="n">do</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="n">DO</span> <span class="o">+</span> <span class="n">off_hz</span> <span class="o">*</span> <span class="n">HEAD_DIM</span> <span class="o">*</span> <span class="n">N_CTX</span> <span class="o">+</span> <span class="n">off_m</span><span class="p">[:,</span> <span class="bp">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">HEAD_DIM</span> <span class="o">+</span> <span class="n">off_n</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="p">:]).</span><span class="nf">to</span><span class="p">(</span><span class="n">tl</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
      <span class="n">delta</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">o</span> <span class="o">*</span> <span class="n">do</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  
      <span class="n">tl</span><span class="p">.</span><span class="nf">store</span><span class="p">(</span><span class="n">Delta</span> <span class="o">+</span> <span class="n">off_hz</span> <span class="o">*</span> <span class="n">N_CTX</span> <span class="o">+</span> <span class="n">off_m</span><span class="p">,</span> <span class="n">delta</span><span class="p">)</span>

</code></pre></div></div> <p>where <code class="language-plaintext highlighter-rouge">delta = tl.sum(o * do, axis=1)</code> implements the equation \(D_i = do_i^T o_i\).</p> <p>To calculate \(dV, dK\), a block of elements of <code class="language-plaintext highlighter-rouge">k, v</code> is first loaded (sequence parallelisation), and then carries out a loop over the length dimension of <code class="language-plaintext highlighter-rouge">q</code>.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="n">start_n</span> <span class="o">=</span> <span class="n">pid</span> <span class="o">*</span> <span class="n">BLOCK_N1</span>
  <span class="n">offs_n</span> <span class="o">=</span> <span class="n">start_n</span> <span class="o">+</span> <span class="n">tl</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">BLOCK_N1</span><span class="p">)</span>
  <span class="c1"># load K and V: they stay in SRAM throughout the inner loop.
</span>  <span class="n">k</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="n">K</span> <span class="o">+</span> <span class="n">offs_n</span><span class="p">[:,</span> <span class="bp">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">stride_tok</span> <span class="o">+</span> <span class="n">offs_k</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="p">:]</span> <span class="o">*</span> <span class="n">stride_d</span><span class="p">)</span>
  <span class="n">v</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="n">V</span> <span class="o">+</span> <span class="n">offs_n</span><span class="p">[:,</span> <span class="bp">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">stride_tok</span> <span class="o">+</span> <span class="n">offs_k</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="p">:]</span> <span class="o">*</span> <span class="n">stride_d</span><span class="p">)</span>
</code></pre></div></div> <p>For the non-causal case, it is straightforward,</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="n">start_m</span> <span class="o">=</span> <span class="mi">0</span>
  <span class="n">num_steps</span> <span class="o">=</span> <span class="p">(</span><span class="n">N_CTX</span> <span class="o">-</span> <span class="n">start_m</span><span class="p">)</span> <span class="o">//</span> <span class="n">BLOCK_M1</span>
</code></pre></div></div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0" id="figure-1"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/kq_dotprod_mat-480.webp 480w,/assets/img/kq_dotprod_mat-800.webp 800w,/assets/img/kq_dotprod_mat-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/kq_dotprod_mat.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Fig-1 An illustration of $ S^T = KQ^T $. </div> <p>For the causal case (please note that causal modelling is only used in self-attention), the procedure is split into two steps:</p> <ol> <li>Calculate the non-masked blocks (yellow squares in the <a href="#figure-1">Fig-1</a>) by only changing <code>start_m = start_n + BLOCK_N1</code>.</li> <li> <p>Calculate the diagonal block (the green square in the <a href="#figure-1">Fig-1</a>) by setting</p> <div class="language-python highlighter-rouge"> <div class="highlight"><pre class="highlight"><code>  <span class="n">start_m</span> <span class="o">=</span> <span class="n">start_n</span>
  <span class="n">MASK_BLOCK_M1</span><span class="p">:</span> <span class="n">tl</span><span class="p">.</span><span class="n">constexpr</span> <span class="o">=</span> <span class="n">BLOCK_M1</span> <span class="o">//</span> <span class="n">BLK_SLICE_FACTOR</span>
  <span class="n">num_steps</span> <span class="o">=</span> <span class="n">BLOCK_N1</span> <span class="o">//</span> <span class="n">MASK_BLOCK_M1</span>
</code></pre></div> </div> </li> </ol> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="c1"># The main inner-loop logic for computing dK and dV.
</span>  <span class="nd">@triton.jit</span>
  <span class="k">def</span> <span class="nf">_attn_bwd_dkdv</span><span class="p">(</span><span class="n">dk</span><span class="p">,</span> <span class="n">dv</span><span class="p">,</span>  <span class="c1">#
</span>                     <span class="n">Q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">sm_scale</span><span class="p">,</span>  <span class="c1">#
</span>                     <span class="n">DO</span><span class="p">,</span>  <span class="c1">#
</span>                     <span class="n">M</span><span class="p">,</span> <span class="n">D</span><span class="p">,</span>  <span class="c1">#
</span>                     <span class="c1"># shared by Q/K/V/DO.
</span>                     <span class="n">stride_tok</span><span class="p">,</span> <span class="n">stride_d</span><span class="p">,</span>  <span class="c1">#
</span>                     <span class="n">H</span><span class="p">,</span> <span class="n">N_CTX</span><span class="p">,</span> <span class="n">BLOCK_M1</span><span class="p">:</span> <span class="n">tl</span><span class="p">.</span><span class="n">constexpr</span><span class="p">,</span>  <span class="c1">#
</span>                     <span class="n">BLOCK_N1</span><span class="p">:</span> <span class="n">tl</span><span class="p">.</span><span class="n">constexpr</span><span class="p">,</span>  <span class="c1">#
</span>                     <span class="n">HEAD_DIM</span><span class="p">:</span> <span class="n">tl</span><span class="p">.</span><span class="n">constexpr</span><span class="p">,</span>  <span class="c1">#
</span>                     <span class="c1"># Filled in by the wrapper.
</span>                     <span class="n">start_n</span><span class="p">,</span> <span class="n">start_m</span><span class="p">,</span> <span class="n">num_steps</span><span class="p">,</span>  <span class="c1">#
</span>                     <span class="n">MASK</span><span class="p">:</span> <span class="n">tl</span><span class="p">.</span><span class="n">constexpr</span><span class="p">):</span>
      <span class="n">offs_m</span> <span class="o">=</span> <span class="n">start_m</span> <span class="o">+</span> <span class="n">tl</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">BLOCK_M1</span><span class="p">)</span>
      <span class="n">offs_n</span> <span class="o">=</span> <span class="n">start_n</span> <span class="o">+</span> <span class="n">tl</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">BLOCK_N1</span><span class="p">)</span>
      <span class="n">offs_k</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">HEAD_DIM</span><span class="p">)</span>
      <span class="n">qT_ptrs</span> <span class="o">=</span> <span class="n">Q</span> <span class="o">+</span> <span class="n">offs_m</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="p">:]</span> <span class="o">*</span> <span class="n">stride_tok</span> <span class="o">+</span> <span class="n">offs_k</span><span class="p">[:,</span> <span class="bp">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">stride_d</span>
      <span class="n">do_ptrs</span> <span class="o">=</span> <span class="n">DO</span> <span class="o">+</span> <span class="n">offs_m</span><span class="p">[:,</span> <span class="bp">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">stride_tok</span> <span class="o">+</span> <span class="n">offs_k</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="p">:]</span> <span class="o">*</span> <span class="n">stride_d</span>
      <span class="c1"># BLOCK_N1 must be a multiple of BLOCK_M1, otherwise the code wouldn't work.
</span>      <span class="n">tl</span><span class="p">.</span><span class="nf">static_assert</span><span class="p">(</span><span class="n">BLOCK_N1</span> <span class="o">%</span> <span class="n">BLOCK_M1</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span>
      <span class="n">curr_m</span> <span class="o">=</span> <span class="n">start_m</span>
      <span class="n">step_m</span> <span class="o">=</span> <span class="n">BLOCK_M1</span>
      <span class="k">for</span> <span class="n">blk_idx</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_steps</span><span class="p">):</span>
          <span class="n">qT</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="n">qT_ptrs</span><span class="p">)</span>
          <span class="c1"># Load m before computing qk to reduce pipeline stall.
</span>          <span class="n">offs_m</span> <span class="o">=</span> <span class="n">curr_m</span> <span class="o">+</span> <span class="n">tl</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">BLOCK_M1</span><span class="p">)</span>
          <span class="n">m</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="n">M</span> <span class="o">+</span> <span class="n">offs_m</span><span class="p">)</span>
          <span class="n">sT</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">qT</span><span class="p">)</span>
          <span class="n">pT</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="n">math</span><span class="p">.</span><span class="nf">exp2</span><span class="p">(</span><span class="n">sT</span> <span class="o">-</span> <span class="n">m</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="p">:])</span>
          <span class="c1"># Autoregressive masking.
</span>          <span class="k">if</span> <span class="n">MASK</span><span class="p">:</span>
              <span class="n">mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">offs_m</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="p">:]</span> <span class="o">&gt;=</span> <span class="n">offs_n</span><span class="p">[:,</span> <span class="bp">None</span><span class="p">])</span>
              <span class="n">pT</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">where</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="n">pT</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span>
          <span class="n">do</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="n">do_ptrs</span><span class="p">)</span>
          <span class="c1"># Compute dV.
</span>          <span class="n">ppT</span> <span class="o">=</span> <span class="n">pT</span>
          <span class="n">ppT</span> <span class="o">=</span> <span class="n">ppT</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">tl</span><span class="p">.</span><span class="n">float16</span><span class="p">)</span>
          <span class="n">dv</span> <span class="o">+=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">ppT</span><span class="p">,</span> <span class="n">do</span><span class="p">)</span>
          <span class="c1"># D (= delta) is pre-divided by ds_scale.
</span>          <span class="n">Di</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="n">D</span> <span class="o">+</span> <span class="n">offs_m</span><span class="p">)</span>
          <span class="c1"># Compute dP and dS.
</span>          <span class="n">dpT</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">tl</span><span class="p">.</span><span class="nf">trans</span><span class="p">(</span><span class="n">do</span><span class="p">)).</span><span class="nf">to</span><span class="p">(</span><span class="n">tl</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
          <span class="n">dsT</span> <span class="o">=</span> <span class="n">pT</span> <span class="o">*</span> <span class="p">(</span><span class="n">dpT</span> <span class="o">-</span> <span class="n">Di</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="p">:])</span>
          <span class="n">dsT</span> <span class="o">=</span> <span class="n">dsT</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">tl</span><span class="p">.</span><span class="n">float16</span><span class="p">)</span>
          <span class="n">dk</span> <span class="o">+=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">dsT</span><span class="p">,</span> <span class="n">tl</span><span class="p">.</span><span class="nf">trans</span><span class="p">(</span><span class="n">qT</span><span class="p">))</span>
          <span class="c1"># Increment pointers.
</span>          <span class="n">curr_m</span> <span class="o">+=</span> <span class="n">step_m</span>
          <span class="n">qT_ptrs</span> <span class="o">+=</span> <span class="n">step_m</span> <span class="o">*</span> <span class="n">stride_tok</span>
          <span class="n">do_ptrs</span> <span class="o">+=</span> <span class="n">step_m</span> <span class="o">*</span> <span class="n">stride_tok</span>
      <span class="k">return</span> <span class="n">dk</span><span class="p">,</span> <span class="n">dv</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="n">qT</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="n">qT_ptrs</span><span class="p">)</span>
  <span class="c1"># Load m before computing qk to reduce pipeline stall.
</span>  <span class="n">offs_m</span> <span class="o">=</span> <span class="n">curr_m</span> <span class="o">+</span> <span class="n">tl</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">BLOCK_M1</span><span class="p">)</span>
  <span class="n">m</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="n">M</span> <span class="o">+</span> <span class="n">offs_m</span><span class="p">)</span>
  <span class="n">sT</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">qT</span><span class="p">)</span>
  <span class="n">pT</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="n">math</span><span class="p">.</span><span class="nf">exp2</span><span class="p">(</span><span class="n">sT</span> <span class="o">-</span> <span class="n">m</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="p">:])</span>
</code></pre></div></div> <p>This part of code recomputes \(S = QK^T\) and \(P = \operatorname{softmax}(S)\) (actually its transposed version, and therefore it needs to pay attention to the broadcast rule <code class="language-plaintext highlighter-rouge">m[None, :]</code>. <code class="language-plaintext highlighter-rouge">m</code> is stored in the forward pass for calculating softmax in a numerical stable manner.).</p> <p><code class="language-plaintext highlighter-rouge">dv += tl.dot(ppT, do)</code> implements the equation \(dV = P^T dO\). As the calculation \(dv_j = \sum_i P_{ij} do_i\), where \(dv_j, do_i\) denote the j-th and i-th row of \(V, O\) respectively, is chunked into multiple blocks, so do not forget the accumulation sum.</p> <p><code class="language-plaintext highlighter-rouge">dpT = tl.dot(v, tl.trans(do)).to(tl.float32)</code> implements the equation \(dP = dO V^T\) (its transposed version).</p> <p><code class="language-plaintext highlighter-rouge">dsT = pT * (dpT - Di[None, :])</code> implements the equation \(dS = \operatorname{dsoftmax}(dP) \in \mathbb{R}^{N\times N}\), which is further simplified to</p> \[dS_{i,:} = \operatorname{dsoftmax}dP_{i,:} = (\text{diag}(P_{i,:}) - P_{i,:}P_{i,:}^T)dP_{i,:} = P_{i,:} \circ dP_{i,:} - \left( P_{i,:}^T dP_{i,:} \right) P_{i,:} = P_{i,:} \circ dP_{i,:} - D_i P_{i,:}\] <p>as discussed above (its transposed version).</p> <p><code class="language-plaintext highlighter-rouge">dk += tl.dot(dsT, tl.trans(qT))</code> implements the equation \(dK = dS^T Q\).</p> <p>\(dQ\) is calculated similarly: a block of elements of <code class="language-plaintext highlighter-rouge">q</code> is first loaded (sequence parallelisation), and then carries out a loop over the length dimension of <code class="language-plaintext highlighter-rouge">k, v</code>.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="n">start_m</span> <span class="o">=</span> <span class="n">pid</span> <span class="o">*</span> <span class="n">BLOCK_M2</span>
  <span class="n">offs_m</span> <span class="o">=</span> <span class="n">start_m</span> <span class="o">+</span> <span class="n">tl</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">BLOCK_M2</span><span class="p">)</span>
  <span class="c1"># load q, do, m and Di: they stay in SRAM throughout the inner loop.
</span>  <span class="n">q</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="n">Q</span> <span class="o">+</span> <span class="n">offs_m</span><span class="p">[:,</span> <span class="bp">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">stride_tok</span> <span class="o">+</span> <span class="n">offs_k</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="p">:]</span> <span class="o">*</span> <span class="n">stride_d</span><span class="p">)</span>
  <span class="n">do</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="n">DO</span> <span class="o">+</span> <span class="n">offs_m</span><span class="p">[:,</span> <span class="bp">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">stride_tok</span> <span class="o">+</span> <span class="n">offs_k</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="p">:]</span> <span class="o">*</span> <span class="n">stride_d</span><span class="p">)</span>

  <span class="n">m</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="n">M</span> <span class="o">+</span> <span class="n">offs_m</span><span class="p">)</span>
  <span class="n">m</span> <span class="o">=</span> <span class="n">m</span><span class="p">[:,</span> <span class="bp">None</span><span class="p">]</span>

  <span class="n">Di</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="n">D</span> <span class="o">+</span> <span class="n">offs_m</span><span class="p">)</span>
  <span class="n">Di</span> <span class="o">=</span> <span class="n">Di</span><span class="p">[:,</span> <span class="bp">None</span><span class="p">]</span>
</code></pre></div></div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0" id="figure-2"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/qk_dotprod_mat-480.webp 480w,/assets/img/qk_dotprod_mat-800.webp 800w,/assets/img/qk_dotprod_mat-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/qk_dotprod_mat.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Fig-2 An illustration of $ S = QK^T $. </div> <p>For the causal case, the procedure is split into two steps:</p> <ol> <li>Calculate the non-masked blocks (yellow squares in the <a href="#figure-2">Fig-2</a>) by setting <code>end_n = start_m</code> <code>num_steps = end_n // BLOCK_N2</code> . So in the inner loop over <code>k, v</code>, the start and end indexes are <code>0</code> and <code>start_m</code>, respectively.</li> <li> <p>Calculate the diagonal block (the green square in the <a href="#figure-2">Fig-2</a>) by setting</p> <div class="language-python highlighter-rouge"> <div class="highlight"><pre class="highlight"><code>  <span class="n">MASK_BLOCK_N2</span><span class="p">:</span> <span class="n">tl</span><span class="p">.</span><span class="n">constexpr</span> <span class="o">=</span> <span class="n">BLOCK_N2</span> <span class="o">//</span> <span class="n">BLK_SLICE_FACTOR</span>
  <span class="n">num_steps</span> <span class="o">=</span> <span class="n">BLOCK_M2</span> <span class="o">//</span> <span class="n">MASK_BLOCK_N2</span>
</code></pre></div> </div> <p>And the start and end indexes are <code>start_m</code> and <code>start_m + BLOCK_M2</code> respectively.</p> </li> </ol> <p>For the non-causal case, in the inner loop over <code class="language-plaintext highlighter-rouge">k, v</code>, the start and end indexes are simply <code class="language-plaintext highlighter-rouge">0</code> and <code class="language-plaintext highlighter-rouge">N_CTX</code>, respectively. However, in my implementation, it is also split into two steps: 1) from <code class="language-plaintext highlighter-rouge">0</code> to <code class="language-plaintext highlighter-rouge">start_m</code>, and 2) from <code class="language-plaintext highlighter-rouge">start_m</code> to <code class="language-plaintext highlighter-rouge">N_CTX</code>.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="nd">@triton.jit</span>
  <span class="k">def</span> <span class="nf">_attn_bwd_dq</span><span class="p">(</span><span class="n">dq</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span>  <span class="c1">#
</span>                   <span class="n">do</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">Di</span><span class="p">,</span>
                   <span class="c1"># shared by Q/K/V/DO.
</span>                   <span class="n">stride_tok</span><span class="p">,</span> <span class="n">stride_d</span><span class="p">,</span>  <span class="c1">#
</span>                   <span class="n">H</span><span class="p">,</span> <span class="n">N_CTX</span><span class="p">,</span>  <span class="c1">#
</span>                   <span class="n">BLOCK_M2</span><span class="p">:</span> <span class="n">tl</span><span class="p">.</span><span class="n">constexpr</span><span class="p">,</span>  <span class="c1">#
</span>                   <span class="n">BLOCK_N2</span><span class="p">:</span> <span class="n">tl</span><span class="p">.</span><span class="n">constexpr</span><span class="p">,</span>  <span class="c1">#
</span>                   <span class="n">HEAD_DIM</span><span class="p">:</span> <span class="n">tl</span><span class="p">.</span><span class="n">constexpr</span><span class="p">,</span>  <span class="c1">#
</span>                   <span class="c1"># Filled in by the wrapper.
</span>                   <span class="n">start_m</span><span class="p">,</span> <span class="n">start_n</span><span class="p">,</span> <span class="n">num_steps</span><span class="p">,</span>  <span class="c1">#
</span>                   <span class="n">MASK</span><span class="p">:</span> <span class="n">tl</span><span class="p">.</span><span class="n">constexpr</span><span class="p">):</span>
      <span class="n">offs_m</span> <span class="o">=</span> <span class="n">start_m</span> <span class="o">+</span> <span class="n">tl</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">BLOCK_M2</span><span class="p">)</span>
      <span class="n">offs_n</span> <span class="o">=</span> <span class="n">start_n</span> <span class="o">+</span> <span class="n">tl</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">BLOCK_N2</span><span class="p">)</span>
      <span class="n">offs_k</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">HEAD_DIM</span><span class="p">)</span>
      <span class="n">kT_ptrs</span> <span class="o">=</span> <span class="n">K</span> <span class="o">+</span> <span class="n">offs_n</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="p">:]</span> <span class="o">*</span> <span class="n">stride_tok</span> <span class="o">+</span> <span class="n">offs_k</span><span class="p">[:,</span> <span class="bp">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">stride_d</span>
      <span class="n">vT_ptrs</span> <span class="o">=</span> <span class="n">V</span> <span class="o">+</span> <span class="n">offs_n</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="p">:]</span> <span class="o">*</span> <span class="n">stride_tok</span> <span class="o">+</span> <span class="n">offs_k</span><span class="p">[:,</span> <span class="bp">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">stride_d</span>
      <span class="c1"># BLOCK_M2 must be a multiple of BLOCK_N2, otherwise the code wouldn't work.
</span>      <span class="n">tl</span><span class="p">.</span><span class="nf">static_assert</span><span class="p">(</span><span class="n">BLOCK_M2</span> <span class="o">%</span> <span class="n">BLOCK_N2</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span>
      <span class="n">curr_n</span> <span class="o">=</span> <span class="n">start_n</span>
      <span class="n">step_n</span> <span class="o">=</span> <span class="n">BLOCK_N2</span>
      <span class="k">for</span> <span class="n">blk_idx</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_steps</span><span class="p">):</span>
          <span class="n">kT</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="n">kT_ptrs</span><span class="p">)</span>
          <span class="n">vT</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="n">vT_ptrs</span><span class="p">)</span>
          <span class="n">s</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">kT</span><span class="p">)</span>
          <span class="n">p</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="n">math</span><span class="p">.</span><span class="nf">exp2</span><span class="p">(</span><span class="n">s</span> <span class="o">-</span> <span class="n">m</span><span class="p">)</span>
          <span class="c1"># Autoregressive masking.
</span>          <span class="k">if</span> <span class="n">MASK</span><span class="p">:</span>
              <span class="n">offs_n</span> <span class="o">=</span> <span class="n">curr_n</span> <span class="o">+</span> <span class="n">tl</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">BLOCK_N2</span><span class="p">)</span>
              <span class="n">mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">offs_m</span><span class="p">[:,</span> <span class="bp">None</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="n">offs_n</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="p">:])</span>
              <span class="n">p</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">where</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span>
          <span class="c1"># Compute dP and dS.
</span>          <span class="n">dp</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">do</span><span class="p">,</span> <span class="n">vT</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">tl</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
          <span class="n">ds</span> <span class="o">=</span> <span class="n">p</span> <span class="o">*</span> <span class="p">(</span><span class="n">dp</span> <span class="o">-</span> <span class="n">Di</span><span class="p">)</span>
          <span class="n">ds</span> <span class="o">=</span> <span class="n">ds</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">tl</span><span class="p">.</span><span class="n">float16</span><span class="p">)</span>
          <span class="c1"># Compute dQ.
</span>          <span class="c1"># NOTE: We need to de-scale dq in the end, because kT was pre-scaled.
</span>          <span class="n">dq</span> <span class="o">+=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">ds</span><span class="p">,</span> <span class="n">tl</span><span class="p">.</span><span class="nf">trans</span><span class="p">(</span><span class="n">kT</span><span class="p">))</span>
          <span class="c1"># Increment pointers.
</span>          <span class="n">curr_n</span> <span class="o">+=</span> <span class="n">step_n</span>
          <span class="n">kT_ptrs</span> <span class="o">+=</span> <span class="n">step_n</span> <span class="o">*</span> <span class="n">stride_tok</span>
          <span class="n">vT_ptrs</span> <span class="o">+=</span> <span class="n">step_n</span> <span class="o">*</span> <span class="n">stride_tok</span>
      <span class="k">return</span> <span class="n">dq</span>

</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="n">kT</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="n">kT_ptrs</span><span class="p">)</span>
  <span class="n">vT</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="n">vT_ptrs</span><span class="p">)</span>
  <span class="n">s</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">kT</span><span class="p">)</span>
  <span class="n">p</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="n">math</span><span class="p">.</span><span class="nf">exp2</span><span class="p">(</span><span class="n">s</span> <span class="o">-</span> <span class="n">m</span><span class="p">)</span>
</code></pre></div></div> <p>This part of code recomputes \(S = QK^T\) and \(P = \operatorname{softmax}(S)\).</p> <p><code class="language-plaintext highlighter-rouge">dp = tl.dot(do, vT).to(tl.float32)</code> implements the equation \(dP = dO V^T\).</p> <p><code class="language-plaintext highlighter-rouge">ds = p * (dp - Di)</code> implements the equation \(dS_{i,:} = P_{i,:} \circ dP_{i,:} - D_i P_{i,:}\).</p> <p><code class="language-plaintext highlighter-rouge">dq += tl.dot(ds, tl.trans(kT))</code> implements the equation \(dQ = dS K\).</p> <h2 id="concluding-remarks">Concluding Remarks</h2> <p>Voila! We have walked through the core implementation of the backward pass of FlashAttention, where the Triton code shares a high similarity with matrix calculus equations. You can check out the Github repo <a href="https://github.com/TotalVariation/Learning-to-Learn-DL/tree/main/flash_attention" rel="external nofollow noopener" target="_blank">Learning-to-Learn-DL</a> containing an IPython notebook which is supposed to offer a more enhanced interactive experience and another notebook where a more flexible implementation of FlashAttention2 is given, which can handle both self-attention and cross-attention with arbitrary lengths. However, for practical usage, I recommend using the official <a href="https://github.com/Dao-AILab/flash-attention" rel="external nofollow noopener" target="_blank">FlashAttention Repo</a> written in CUDA. Furthermore, I believe this post will facilitate your understanding of the Triton implementation given in the official <a href="https://github.com/Dao-AILab/flash-attention" rel="external nofollow noopener" target="_blank">FlashAttention Repo</a>.</p> <div class="language-html highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@misc{Cai2025flashattnbackward-2,
    author = {Xin Cai},
    title = {Learning the Backward Pass of FlashAttention: Part II Implementation in Triton},
    howpublished = {\url{https://totalvariation.github.io/blog/2025/intro-flashattention-backward-part2/},
    note = {Accessed: 2025-07-21},
    year = {2025}
}
</code></pre></div></div> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/2025-07-21-intro-flashattention-backward-part2.bib"></d-bibliography> <div id="giscus_thread" style="max-width: 800px; margin: 0 auto;"> <script>let giscusTheme=localStorage.getItem("theme"),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"TotalVariation/TotalVariation.github.io","data-repo-id":"","data-category":"Comments","data-category-id":"","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,a])=>giscusScript.setAttribute(t,a)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Xin Cai. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>