<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Reconstructing the Scaffolding A Learner’s Notes on Ordinary Differential Equations | Xin's Homepage </title> <meta name="author" content="Xin Cai"> <meta name="description" content="Part Ⅱ Convergence and Order in One-Step (Runge–Kutta) Methods"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://totalvariation.github.io/blog/2025/concise-intro-odes-part2/"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?0afe9f0ae161375728f7bcc5eb5b4ab4"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}.theorem-box{border:4px solid ForestGreen;border-radius:8px;margin:20px 0;box-shadow:3px 3px 10px rgba(0,0,0,0.1);overflow:hidden;max-width:800px}.theorem-header{background-color:ForestGreen;color:white;padding:8px 15px;font-size:1.3em;font-weight:bold;text-transform:uppercase}.theorem-content{background-color:#f8f9fa;padding:15px;font-size:1.2em;line-height:1.6}.theorem-content p{margin:0 0 10px 0}.theorem-type{font-family:'Georgia',serif}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Reconstructing the Scaffolding A Learner’s Notes on Ordinary Differential Equations",
            "description": "Part &#8545 Convergence and Order in One-Step (Runge–Kutta) Methods",
            "published": "October 29, 2025",
            "authors": [
              
              {
                "author": "Xin Cai",
                "authorURL": "https://totalvariation.github.io/",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Xin's Homepage </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/publications/">publications</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/projects/">projects</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/blog/">blog</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Reconstructing the Scaffolding A Learner’s Notes on Ordinary Differential Equations</h1> <p>Part Ⅱ Convergence and Order in One-Step (Runge–Kutta) Methods</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#convergence-of-general-one-step-methods">Convergence of General One Step Methods</a> </div> <div> <a href="#general-runge-kutta-methods">General Runge-Kutta Methods</a> </div> <div> <a href="#a-few-illustrated-examples">A Few Illustrated Examples</a> </div> <div> <a href="#concluding-remarks">Concluding Remarks</a> </div> <div> <a href="#acknowledgement">Acknowledgement</a> </div> </nav> </d-contents> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <video src="/assets/video/traj-one-step-methods.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" autoplay="" controls=""></video> </figure> </div> </div> <div class="caption"> Animated Trajectories of One-Step Methods on Lotka-Volterra Equations </div> <p> This post is part of my ongoing series of learning notes on Ordinary Differential Equations (ODEs). In this instalment, I will first present a fundamental theorem on the convergence of general one-step methods, followed by a concise introduction to Runge–Kutta (RK) methods, including detailed derivations of the order conditions up to order four. </p> <p> Throughout this blog, we focus on the initial value problems (IVPs) for one-dimensional ODEs for notational simplicity $$ y'(t) = f(t, y(t)), \; y(t_0) = y_0, \; t \in [t_0, T] $$ with $ f $ a sufficiently smooth (as specified when needed) scalar function. In the more general setting, where $ f : \mathbb{R} \times \mathbb{R}^d \to \mathbb{R}^d $, the corresponding proofs remain essentially identical, with vector norms used in place of absolute values where appropriate. </p> <h2> Convergence of General One Step Methods </h2> <p> A general (explicit) one-step method can be written in the form: $$ y_{n+1} = y_n + h \Phi(t_n, y_n, h), \; n=0,1,\ldots, N-1, \; h = \frac{T - t_0}{N}, \; t_n = t_0 + nh, \; y_0 = y(t_0) $$ </p> <p> <strong>Definition Local Truncation Error (LTE):</strong> The local truncation error $ \tau_{n+1} $ at step $ t_{n+1} $ is defined as by assuming $ y_n = y(t_n) $ is exact: $$ \tau_{n+1} = \left[ y(t_{n+1}) - y(t_n) - h \Phi(t_n, y(t_n), h) \right] $$ </p> <p> The method is consistent if $ \max_{0 \leq n \leq N-1} \frac{1}{h} |\tau_{n+1}| \to 0 \; \text{as} \; h \to 0 $, which is equivalent to $ \Phi(t, y, 0) = f(t, y), \; \text{for all} \; t \in [0, T] $ as $ \lim_{h \to 0} \frac{1}{h} \tau_{n+1} = y'(t_n) - \Phi(t_n, y(t_n), 0) $. </p> <p> Yet, the convergence of a numerical scheme concerns the global error $ e_n = y(t_n) - y_n $, which accumulates over steps. The method is convergent if: $ \max_{0 \leq n \leq N} |e_n| \to 0 \; \text{as} \; h \to 0 $, with $ t \in [0, T] $. It is convergent of order $ p $ if $ \max |e_n| = O(h^p) $. The consistent condition alone does not guarantee convergence, we need an additional stability condition on $ \Phi $: $ |\Phi(t, y_1, h) - \Phi(t, y_2, h)| \leq L_{\Phi} |y_1 - y_2|, t \in [0, T], h \in [0, h_0] $, i.e., $ \Phi $ satisfying the Lipschitz condition in $ y $ and independent of $ h $. This ensures errors do not amplify excessively. </p> <div class="theorem-box"> <div class="theorem-header"> <span class="theorem-type">Theorem Convergence of One-Step Methods</span> </div> <div class="theorem-content"> <p>If the one-step method is consistent and $ \Phi $ is Lipschitz in $ y $ with constant $ L_{\Phi} $ independent of $ h $, then it is convergent with the global error expressed as: $ |e_n| \le \frac{\tau}{hL_{\Phi}} (e^{L_{\Phi}(T - t_0)} - 1) $, where $ \tau = \max_{0 \leq n \leq N} |\tau_n| $.</p> </div> </div> <p><strong>Proof:</strong></p> <p> By the definition of the LTE, the exact solution satisfies: $$ y(t_{n+1}) = y(t_n) + h \Phi(t_n, y(t_n), h) + \tau_{n+1} $$ Subtracting the numerical scheme $$ y_{n+1} = y_n + h \Phi(t_n, y_n, h) $$, we obtain a recursive formula for error function: $$ \begin{align*} e_{n+1} &amp;= y(t_{n+1}) - y_{n+1} = [y(t_n) - y_n] + h [\Phi(t_n, y(t_n), h) - \Phi(t_n, y_n, h)] + \tau_{n+1} \\ e_{n+1} &amp;= e_n + h [\Phi(t_n, y(t_n), h) - \Phi(t_n, y_n, h)] + \tau_{n+1} \\ |e_{n+1}| &amp;\le |e_n| + h |\Phi(t_n, y(t_n), h) - \Phi(t_n, y_n, h)| + |\tau| \end{align*} $$ </p> <p> By the Lipschitz condition on $ \Phi $, i.e., $ |\Phi(t_n, y(t_n), h) - \Phi(t_n, y_n, h)| \leq L_{\Phi} |y(t_n) - y_n| = L_{\Phi} |e_n| $, we get, $$ \begin{align*} |e_{n+1}| &amp;\le (1 + hL_{\Phi} ) |e_n| + |\tau| \\ |e_{n}| &amp;\le (1 + h L_{\Phi} )^n |e_0| + |\tau| \sum_{k=0}^{n-1} (1 + hL_{\Phi})^k \end{align*} $$ </p> <p> If $ |e_0| = y_0 - y(t_0) = 0 $, we get $$ |e_n| \leq |\tau| \cdot \frac{(1 + h L_{\Phi})^n - 1}{h L_{\Phi}} = \frac{|\tau|}{hL_{\Phi}} \left[ (1 + h L_{\Phi})^n - 1 \right] $$ </p> <p> Since $ 1 + x \leq e^x $ and $ n h = t_n - t_0 \le T - t_0 $, we have $ (1 + h L_{\Phi})^n \leq e^{n h L_{\Phi}} \leq e^{L_{\Phi} (T - t_0)} $, thus yielding the final expression: $$ |e_n| \leq \frac{|\tau|}{hL_{\Phi}} (e^{L_{\Phi} (T - t_0)} - 1) $$ As $ h \to 0 $, consistency implies $ \frac{1}{h}|\tau| \to 0 $, so $ |e_n| \to 0 $, proving convergence. If the method is of order $ p $, then $ |\tau| = O(h^{p+1}) $, so $ |e_n| = O(h^p) $, proving order $ p $ convergence. </p> <p> The main takeaway is, for one-step methods, <strong>consistency + one-step stability (Lipschitz continuity) $ \implies $ convergence</strong>. Furthermore, the LTE is $ \mathcal{O}(h^{p+1}) $ $ \implies $ the global error is $ \mathcal{O}(h^p) $. </p> <p> Next, we will examine the LTE of some common one-step methods: </p> <ol> <li> Explicit Euler $ y_{n+1} = y_n + hf(t_n, y_n) $. Use Taylor expansion of the exact solution: $$ y(t_{n+1}) = y(t_n) + h y'(t_n) + \mathcal{O}(h^2) $$ Given $ y'(t_n) = f(t_n, y(t_n)) $, so $$ \tau_{n+1} = y(t_{n+1}) - \left(y(t_n) + hf(t_n, y(t_n)) \right) = \mathcal{O}(h^2) $$ hence $ |\tau| \le Ch^2 $ (with $ C = \max_{t \in [t_0, T]} |y''(t)| $). Thus, explicit Euler has order $ p = 1 $. </li> <li> Improved Euler $ y_{n+1} = y_n + \frac{h}{2} \left[ f(t_n, y_n) + f(t_n + h, y_n + h f(t_n, y_n)) \right] $. Use Taylor expansion of the exact solution: $$ y(t_{n+1}) = y(t_n) + h y'(t_n) + \frac{h^2}{2} y''(t_n) + \mathcal{O}(h^3) $$ Given $ y'(t_n) = f(t_n, y(t_n)) $ and $$ y''(t_n) = f_t(t_n, y(t_n)) + f_y(t_n, y(t_n)) f(t_n, y(t_n)) $$ we have $$ y(t_{n+1}) = y(t_n) + h f(t_n, y(t_n)) + \frac{h^2}{2} \left[ f_t(t_n, y(t_n)) + f_y(t_n, y(t_n)) f(t_n, y(t_n)) \right] + \mathcal{O}(h^3) $$ Use multivariate Taylor expansion: $$ f(t_n + h, y(t_n) + h f(t_n, y(t_n))) = f(t_n, y(t_n)) + h f_t(t_n, y(t_n)) + h f_y(t_n, y(t_n)) f(t_n, y(t_n)) + \mathcal{O}(h^2) $$ so $$ \tau_{n+1} = y(t_{n+1}) - \left( y(t_n) + \frac{h}{2} \left( f(t_n, y(t_n)) + f(t_n + h, y(t_n) + h f(t_n, y(t_n))) \right) \right) = \mathcal{O}(h^3) $$ hence $ |\tau| \le Ch^3 $. Thus, improved Euler has order $ p = 2 $. </li> </ol> <p>It turns out that the methods we discussed above, i.e., explicit Euler and improved Euler, are special cases of Runge-Kutta methods, which we will inspect shortly.</p> <h2> General Runge-Kutta Methods </h2> <p> An $ s $-stage Runge-Kutta (RK) method computes stage derivatives $$ k_i = f \left(t_n + c_i h, y_n + h \sum_{j=1}^s a_{ij}k_j \right), i=1,\ldots,s, $$ and updates as per the following formula: $$ y_{n+1} = y_n + h \sum_{i=1}^s b_i k_i. $$ </p> <p> The coefficients $ \mathbf{A} = [a_{ij}]_{s \times s}, \mathbf{b} = (b_1, \cdots, b_s), \mathbf{c}=(c_1, \cdots, c_s)^T $ are real numbers arranged in the Butcher tableau $$ \begin{array}{c|ccc} c_1 &amp; a_{11} &amp; \cdots &amp; a_{1s} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ c_s &amp; a_{s1} &amp; \cdots &amp; a_{ss} \\ \hline &amp; b_1 &amp; \cdots &amp; b_s \end{array} $$ The $ k_i $ are slopes evaluated at intermediate points. If $ \mathbf{A} $ is strictly lower triangular ($ a_{ij} = 0 $ for $ j \ge i $), the method is explicit. Otherwise, it is implicit. </p> <p> To derive RK methods of order $ p $, the numerical solution must match the exact Taylor expansion up to $ O(h^{p+1}) $ (i.e., ensuring the LTE is $ O(h^{p+1}) $). Expand $ y_{n+1} $ and each $ k_i $ in Taylor series, then equate coefficients. This yields order conditions on $ \mathbf{b}, \mathbf{c}, A $. </p> <p> We will use one-dimensional ODEs for simplicity, but the methods extend naturally to systems of ODEs, i.e., $ y(t) \in \mathbb{R}^d, f : \mathbb{R} \times \mathbb{R}^d \to \mathbb{R}^d $ with higher-order derivatives generalised to multilinear maps (e.g., second order derivatives correspond to bilinear maps). </p> <p><strong>Taylor expansion of the exact solution</strong></p> <p> Fix $ t = t_n $. Denote $ f = f(t, y(t)), f_t = \frac{\partial f}{\partial t}, f_y = \frac{\partial f}{\partial y} $. Differentiating the ODE $ y' = f $ yields $$ \begin{align*} y'' &amp;= \frac{d}{dt} f(t,y) = f_t + f_y y' = f_t + f_y f \\ y^{(3)} &amp;= \frac{d}{dt} (f_t + f_y f) = f_{tt} + f_{ty}y' + \frac{d}{dt}(f_y f) \\ \frac{d}{dt}(f_y f) &amp;= (f_{yt} + f_{yy}y')f + f_y(f_t + f_y y') \end{align*} $$ Substitute $ y' = f $ to obtain $$ y^{(3)} = f_{tt} + 2f_{ty}f + f_{yy}ff + f_y f_t + f_y f_y f $$ Thus the Taylor expansion of the exact solution is $$ y(t + h) = y + hf + \frac{h^2}{2}(f_t + f_y f) + \frac{h^3}{6} (f_{tt} + 2f_{ty}f + f_{yy}ff + f_y f_t + f_y f_y f) + \mathcal{O}(h^4) $$ </p> <p> If $ f : \mathbb{R} \times \mathbb{R}^d \to \mathbb{R}^d $, then $ f_y \in \mathbb{R}^{d \times d} $ is the Jacobian matrix. Correspondingly, $ f_y f $ denotes matrix-vector multiplication, and $ f_{yy} f f := f_{yy}[f, f] $ denotes the bilinear map <d-footnote>We encourage readers seeking a deep understanding of this concept to consult the chapter 12 of the book Matrix Calculus (for Machine Learning and Beyond). <d-cite key="bright2025matrix"></d-cite> </d-footnote> acting on vectors, where $ f_{yy} \in \mathcal{L}(\mathbb{R}^d, \mathcal{L}(\mathbb{R}^d, \mathbb{R}^d)) $, $ f_{yy} \cdot e_i = H_{f_i} $ with $ H_{f_i} $ defined as per the Hessian matrix of a scalar-valued function. </p> <p><strong>Stage expansions</strong></p> <p> For small $ h $, expand each stage: $$ k_i = f \left(t + c_i h, y + h \sum_j a_{ij}k_j \right) $$ </p> <p><em>Zeroth order:</em> $ k_i = f + \mathcal{O}(h) $.</p> <p><em>First order:</em> $ k_i = f + c_i h f_t + h f_y \sum_j a_{ij} k_j + \mathcal{O}(h^2) $.</p> <p> Replace $ k_j = f + \mathcal{O}(h) $ inside the sum to get $$ k_i = f + c_i h f_t + h f_y \sum_j a_{ij} f + \mathcal{O}(h^2) $$ Assume the row sum condition $ c_i := \sum_j a_{ij} $, which holds true for all classic RK methods. Then $$ k_i = f + h(c_i f_t + c_i f_y f) + \mathcal{O}(h^2) $$ Insert the expansion of $ k_i $ into the numerical update $ y_{n+1} = y_n + h \sum_i b_i k_i $: $$ y_{n+1} = y_n + h \sum_i b_i f + h^2 \sum_i b_i (c_i f_t + \alpha_i f_y f) + \mathcal{O}(h^3) $$ Group terms: $$ y_{n+1} = y_n + h \left( \sum_i b_i \right) f + \frac{h^2}{2} \left(2 \sum_i b_i c_i \right) f_t + \frac{h^2}{2}\left( 2 \sum_i b_i c_i \right)f_y f + \mathcal{O}(h^3) $$ Matching with the exact Taylor series to order 2 $$ y(t + h) = y + hf + \frac{h^2}{2} (f_t + f_y f) + \mathcal{O}(h^3) $$ </p> <p>Equate coefficients of $ h $ and $ h^2 $:</p> <p> <strong>Order 1 condition:</strong> $$ \sum_{i=1}^s b_i = 1 $$ </p> <p> <strong>Order 2 condition:</strong> $$ \sum_{i=1}^s b_i c_i = \frac{1}{2} $$ </p> <p> <strong>Order 3 conditions:</strong> We need one higher order in the stage expansion. $$ k_i = f + c_i h f_t + h f_y \sum_j a_{ij} k_j + \frac{1}{2}(c_i h)^2 f_{tt} + c_i h f_{ty} \left( h \sum_j a_{ij} k_j \right) + \frac{1}{2} \left( h \sum_j a_{ij} k_j \right) f_{yy} \left( h \sum_{\ell} a_{i \ell} k_{\ell} \right) + \mathcal{O}(h^3) $$ Now substitute $ k_j = f + h \left( c_j f_t + c_j f_y f \right) + \mathcal{O}(h^2) $ into those sums, $$ \sum_j a_{ij} k_j = c_i f + h \sum_j a_{ij}\left( c_j f_t + c_j f_y f \right) + \mathcal{O}(h^2) $$ $$ \begin{align*} k_i &amp;= f + c_i h f_t + h f_y (c_i f) + \frac{h^2}{2}c^2_i f_{tt} + c_i h f_{ty} \left( h c_i f \right) + \frac{h^2}{2} (c_i f) f_{yy} (c_i f) \\ &amp; + h^2 \left( \sum_j a_{ij} c_j \right) f_y f_t + h^2 \left( \sum_j a_{ij} c_j \right) f_y f_y f + \mathcal{O}(h^3) \end{align*} $$ $$ \begin{align*} k_i &amp;= f + h (c_i f_t + c_i f_y f) \\ &amp;+ \frac{h^2}{2} c^2_i f_{tt} + h^2 c_i^2 f_{ty}f + \frac{h^2}{2} c^2_i f_{yy} f f \\ &amp;+ h^2 \left( \sum_j a_{ij} c_j \right) f_y f_t + h^2 \left( \sum_j a_{ij} c_j \right) f_y f_y f + \mathcal{O}(h^3) \end{align*} $$ Now substitute into the update $ y_{n+1} = y + h \sum_i b_i k_i $ to obtain, $$ \begin{align*} y_{n+1} &amp;= y + h \sum_i b_i f \\ &amp; + h^2 \sum_i b_i \left( c_i f_t + c_i f_y f \right) \\ &amp; + h^3 \left[ \frac{1}{2} \sum_i b_i c_i^2 f_{tt} + \sum_i b_i c_i^2 f_{ty}f + \frac{1}{2}\sum_i b_i c_i^2 f_{yy} f f \right. \\ &amp; \left. + \left(\sum_{i, j}b_i a_{ij} c_j \right) f_y f_t + \left(\sum_{i, j}b_i a_{ij} c_j \right) f_y f_y f \right] + \mathcal{O}(h^4). \end{align*} $$ Compare with the exact Taylor series to order 3: $$ y(t + h) = y + hf + \frac{h^2}{2} (f_t + f_y f) + \frac{h^3}{3} \left( f_{tt} + 2 f_{ty} f + f_{yy} f f + f_y f_t + f_y f_y f \right) + \mathcal{O}(h^4) $$ we get: $$ \begin{align*} \sum_i b_i &amp;= 1 \\ \sum_i b_i c_i &amp;= \frac{1}{2} \\ \sum_i b_i c_i^2 &amp;= \frac{1}{3} \\ \sum_{i,j} b_i a_{ij} c_j &amp;= \frac{1}{6} \end{align*} $$ So up to order 3, the independent order conditions are $$ \begin{align*} \sum_i b_i = 1, \sum_i b_i c_i = \frac{1}{2}, \sum_i b_i c_i^2 = \frac{1}{3}, \sum_{i,j} b_i a_{ij} c_j = \frac{1}{6} \end{align*} $$ </p> <p> <strong>Order 4 conditions:</strong> $$ \begin{align*} \sum_i b_i c_i^3 = \frac{1}{4} \\ \sum_{i, j} b_i c_i a_{ij} c_j = \frac{1}{8} \\ \sum_{i, j} b_i a_{ij} c_j^2 = \frac{1}{12} \\ \sum_{i, j, \ell} b_i a_{ij} a_{j \ell} c_{\ell} = \frac{1}{24} \end{align*} $$ </p> <p> <strong>Sketch of derivation</strong><d-footnote>In the following derivations, we assume the general setting, where $ y(t) \in \mathbb{R}^d, f : \mathbb{R} \times \mathbb{R}^d \to \mathbb{R}^d. $</d-footnote> </p> <p> Extend the stage expansion by one more order to order 4: $$ \begin{align*} k_i &amp;= f + h(c_i f_t + c_i f_y f) \\ &amp;+ h^2\left( (\sum_j a_{ij}c_j)f_y f_t + (\sum_j a_{ij}c_j)f_y f_y f + \frac{1}{2} c_i^2 f_{tt} + c_i^2 f_{ty}f + \frac{1}{2} c_i^2 f_{yy}(f, f) \right) \\ &amp;+ h^3 \left( \frac{1}{2} (\sum_j a_{ij}c_j^2) f_y f_{tt} + (\sum_j a_{ij}c_j^2) f_y f_{ty} f + \frac{1}{2}(\sum_j a_{ij}c_j^2) f_y f_{yy}(f, f) \right. \\ &amp;\left. + (\sum_{j \ell} a_{ij} a_{j \ell} c_{\ell}) f_y f_y f_t + (\sum_{j \ell} a_{ij} a_{j \ell} c_{\ell}) f_y f_y f_y f + c_i(\sum_j a_{ij}c_j)f_{ty}f_t + c_i(\sum_j a_{ij}c_j) f_{ty} f_y f \right. \\ &amp;\left. + c_i(\sum_j a_{ij}c_j) f_{yy} (f_t, f) + c_i(\sum_j a_{ij}c_j) f_{yy}(f, f_y f) \right. \\ &amp;\left. + \frac{1}{6} c_i^3 f_{ttt} + \frac{1}{6} c_i^3 f_{yyy}(f, f, f) + \frac{1}{2} c_i^3 f_{tyy}(f, f) + \frac{1}{2} c_i^3 f_{tty} f \right) + \mathcal{O}(h^4) \end{align*} $$ </p> <p> Substitute into the update: $$ \begin{align*} y_{n+1} &amp;= y_n + h (\sum_i b_i) f + h^2 \left( (\sum_i b_i c_i)f_t + (\sum_i b_i c_i)f_y f \right) \\ &amp; + h^3 \left( (\sum_{ij} b_i a_{ij} c_j )f_y f_t + (\sum_{ij} b_i a_{ij} c_j ) f_y f_y f + \right. \\ &amp;\left. + \frac{1}{2} (\sum_i b_i c_i^2) f_{tt} + (\sum_i b_i c_i^2) f_{ty} f + \frac{1}{2} (\sum_i b_i c_i^2) f_{yy}(f, f) \right) \\ &amp;+ h^4 \left( \frac{1}{2}(\sum_{ij} b_i a_{ij} c_j^2) f_y f_{tt} + (\sum_{ij} b_i a_{ij} c_j^2) f_y f_{ty} f + \frac{1}{2} (\sum_i b_i a_{ij} c_j^2) f_y f_{yy}(f, f) \right. \\ &amp;\left. + (\sum_{i j \ell} b_i a_{ij} a_{j\ell} c_{\ell})f_y f_y f_t + (\sum_{i j \ell} b_i a_{ij} a_{j\ell} c_{\ell}) f_y f_y f_y f + (\sum_{ij} b_i c_i a_{ij} c_j) f_{ty} f_t + (\sum_{ij} b_i c_i a_{ij} c_j)f_{ty} f_y f \right. \\ &amp; \left. + (\sum_{ij} b_i c_i a_{ij} c_j) f_{yy}(f_t, f) + (\sum_{ij} b_i c_i a_{ij} c_j) f_{yy}(f, f_y f) + \frac{1}{6} (\sum_i b_i c_i^3) f_{ttt} \right. \\ &amp;\left. + \frac{1}{6} (\sum_i b_i c_i^3) f_{yyy}(f, f, f) + \frac{1}{2} (\sum_i b_i c_i^3) f_{tyy}(f, f) + \frac{1}{2} (\sum_i b_i c_i^3) f_{tty}f \right) + \mathcal{O}(h^5) \end{align*} $$ </p> <p> Match with the exact Taylor series $$ \begin{align*} y(t + h) &amp;= y + hf + \frac{h^2}{2} \left(f_t + f_y f \right) + \frac{h^3}{6} \left(f_{tt} + 2f_{ty}f + f_{yy}(f, f) + f_y f_t + f_y f_y f \right) \\ &amp; + \frac{h^4}{24} \left( f_{ttt} + 3 f_{tty}f + 3 f_{tyy}(f, f) + 3 f_{ty}f_t + 3 f_{ty} f_y f + f_{yyy}(f, f, f) + 3f_{yy}(f_t, f) \right. \\ &amp;\left. + 3f_{yy} (f_y f, f) + f_y f_{tt} + 2 f_y f_{ty} f + f_y f_{yy}(f, f) + f_y f_y f_t + f_y f_y f_y f \right) + \mathcal{O}(h^5) \end{align*} $$ </p> <h2>A Few Illustrated Examples</h2> <p>Next, let us take a quick look at the classical RK4 method</p> <p> Butcher tableau: $$ \begin{array}{c|cccc} 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\ \frac{1}{2} &amp; \frac{1}{2} &amp; 0 &amp; 0 &amp; 0 \\ \frac{1}{2} &amp; 0 &amp; \frac{1}{2} &amp; 0 &amp; 0 \\ 1 &amp; 0 &amp; 0 &amp; 1 &amp; 0 \\ \hline &amp; \frac{1}{6} &amp; \frac{1}{3} &amp; \frac{1}{3} &amp; \frac{1}{6} \end{array} $$ </p> <p> Stage derivatives: $$ \begin{align*} k_1 &amp;= f(t_n ,y_n) \\ k_2 &amp;= f(t_n + \frac{1}{2}h, y_n + \frac{1}{2}h k_1)\\ k_3 &amp;= f(t_n + \frac{1}{2}h, y_n + \frac{1}{2}h k_2)\\ k_4 &amp;= f(t_n + h ,y_n + h k_3) \end{align*} $$ </p> <p> The numerical update: $$ y_{n+1} = y_n + \frac{h}{6} \left(k_1 + 2 k_2 + 2 k_3 + k_4 \right) $$ </p> <p>Next, we will construct a general 2-stage, order-2 explicit RK by using order conditions derived above.</p> <p> Let $$ \begin{array}{c|cc} 0 &amp; 0 &amp; 0 \\ c_2 &amp; a_{21} &amp; 0 \\ \hline &amp; b_1 &amp; b_2 \end{array} \quad (c_2 = a_{21}). $$ </p> <p> Order conditions: $$ b_1 + b_2 = 1, \; b_2 c_2 = \frac{1}{2} $$ </p> <p> Choose a free parameter $ c_2 = \alpha \neq 0 $. Then $$ b_2 = \frac{1}{2 \alpha}, \; b_1 = 1 - \frac{1}{2 \alpha} $$ Thus the Butcher tableau of the 2-stage explicit RK is $$ \begin{array}{c|cc} 0 &amp; 0 &amp; 0 \\ \alpha &amp; \alpha &amp; 0 \\ \hline &amp; 1 - \frac{1}{2 \alpha} &amp; \frac{1}{2 \alpha} \end{array} \quad (\alpha \neq 0) $$ </p> <p> Stage derivatives: $$ \begin{align*} k_1 &amp;= f(t_n, y_n) \\ k_2 &amp;= f(t_n + \alpha h, y_n + \alpha h k_1) \end{align*} $$ when $ \alpha = \frac{1}{2} $, we recover the midpoint method: $$ y_{n+1} = y_n + h f(t_n + \frac{1}{2}h, y_n + \frac{1}{2}h f(t_n, y_n)) $$ when $ \alpha = 1 $, we recover Heun's method (improved Euler): $$ y_{n+1} = y_n + \frac{h}{2}\left(f(t_n, y_n) + f(t_n +h, y_n + hf(t_n, y_n)) \right) $$ </p> <p>Last, let us take a look at an implicit RK2 method (i.e., implicit trapezoidal method or Crank-Nicolson method):</p> <p> Butcher tableau: $$ \begin{array}{c|cc} 0 &amp; 0 &amp; 0\\ 1 &amp; \frac{1}{2} &amp; \frac{1}{2}\\ \hline &amp; \frac{1}{2} &amp; \frac{1}{2} \end{array} $$ </p> <p> Update equations: $$ \begin{align*} k_1 &amp;= f(t_n, y_n) \\ k_2 &amp;= f(t_n + h, y_n +h(\frac{1}{2}k_1 + \frac{1}{2}k_2))\\ y_{n+1} &amp;= y_n + \frac{h}{2}(k_1 + k_2) \end{align*} $$ </p> <p> We can verify order conditions $$ \begin{align*} \sum_i b_i &amp;= \frac{1}{2} + \frac{1}{2} = 1 \\ \sum_i b_i c_i &amp;= \frac{1}{2} \cdot 0 + \frac{1}{2} \cdot 1 = \frac{1}{2} \end{align*} $$ </p> <h2> Concluding Remarks </h2> <p> Voilà! This concludes a concise yet mathematically rigorous introduction to general one-step methods for numerically solving IVPs of ODEs. We have carefully derived the key convergence theorem—that <strong>consistency + stability (Lipschitz continuity) $ \implies $ convergence</strong>—and established the order conditions (up to order four) for general Runge–Kutta (RK) methods. </p> <p> As an aside, current large language models (LLMs) still tend to struggle with the detailed derivation of higher-order RK conditions, particularly beyond third order. For instance, expanding the stage derivatives $ k_i = f \left(t_n + c_i h, y_n + h \sum_{j=1}^s a_{ij}k_j \right) $ to order four using multivariate Taylor series, as we did above, is challenging for LLMs. I suspect this limitation arises from the relative scarcity of third-order differential terms (i.e., $ f_{ttt}, f_{tty}f, f_{tyy}(f, f), f_{yyy}(f, f, f), f_y f_{tt}, f_y f_{ty} f,f_y f_{yy}(f, f), f_y f_y f_t, f_y f_y f_y f $) in their pre-training corpora. In such cases, symbolic computation systems, also known as computer algebra systems (CAS), remain more suitable tools for handling the intricate algebraic manipulations required. Nevertheless, I believe that as long as LLMs can function as faithful interpolating functions of existing knowledge, they hold immense potential to transform the educational landscape, particularly by democratising access to advanced mathematical and scientific knowledge. </p> <h2> Acknowledgement </h2> <p>This blog was developed from a self-contained, step-by-step tutorial generated by OpenAI ChatGPT, which also assisted in refining the phrasing and expressions, as well as in suggesting suitable titles.</p> <div class="language-html highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@misc{Cai2025odelearningnotes-2,
    author = {Xin Cai},
    title = {Reconstructing the Scaffolding A Learner’s Notes on Ordinary Differential Equations: Part 2 Convergence and Order in One-Step (Runge–Kutta) Methods},
    howpublished = {\url{https://totalvariation.github.io/blog/2025/concise-intro-odes-part2/}},
    note = {Accessed: 2025-10-29},
    year = {2025}
}
</code></pre></div></div> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/2025-10-29-concise-intro-odes-part2.bib"></d-bibliography> <div id="giscus_thread" style="max-width: 800px; margin: 0 auto;"> <script>let giscusTheme=localStorage.getItem("theme"),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"TotalVariation/TotalVariation.github.io","data-repo-id":"","data-category":"Comments","data-category-id":"","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,a])=>giscusScript.setAttribute(t,a)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Xin Cai. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>