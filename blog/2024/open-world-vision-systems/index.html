<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Random Thoughts on Open World Vision Systems | Xin's Homepage </title> <meta name="author" content="Xin Cai"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://totalvariation.github.io/blog/2024/open-world-vision-systems/"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?0afe9f0ae161375728f7bcc5eb5b4ab4"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Random Thoughts on Open World Vision Systems",
            "description": "",
            "published": "February 01, 2024",
            "authors": [
              
              {
                "author": "Xin Cai",
                "authorURL": "https://totalvariation.github.io/",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Xin's Homepage </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/publications/">publications</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/projects/">projects</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/blog/">blog</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Random Thoughts on Open World Vision Systems</h1> <p></p> </d-title> <d-byline></d-byline> <d-article> <blockquote> <p><b>It is not the strongest of the species that survives, nor the most intelligent that survives. It is the one that is most adaptable to change.</b></p> <footer>- Charles Darwin</footer> </blockquote> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ante-hamersmit-qM8zX1celvc-unsplash-480.webp 480w,/assets/img/ante-hamersmit-qM8zX1celvc-unsplash-800.webp 800w,/assets/img/ante-hamersmit-qM8zX1celvc-unsplash-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/ante-hamersmit-qM8zX1celvc-unsplash.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Photo by <a href="https://unsplash.com/@ante_kante?utm_content=creditCopyText&amp;utm_medium=referral&amp;utm_source=unsplash" rel="external nofollow noopener" target="_blank">Ante Hamersmit</a> on <a href="https://unsplash.com/photos/person-holding-reptile-qM8zX1celvc?utm_content=creditCopyText&amp;utm_medium=referral&amp;utm_source=unsplash" rel="external nofollow noopener" target="_blank">Unsplash</a> </div> <p>Over the last decade, there has been remarkable progress in visual perception algorithms, driven by the development of layered differentiable models optimized in an end-to-end fashion. Despite these advancements, the deployment of such algorithms in open-world scenarios poses significant challenges. To effectively operate in open-world settings, these algorithms must incorporate capabilities for open-set recognition and open vocabulary learning, while also being robust to distribution shifts. Moreover, Open World Visual Understanding Systems (OWVUS) are expected to handle continuous streams of real-world data, adapting to non-stationary environments with minimal labelling requirements. This suggests the need for semi- or fully-automatic data engines that facilitate model learning alongside incremental data curation and labelling, potentially assisted with human intervention. The foundational pillars of building OWVUS lie in data and annotation efficiency, robustness, and generalization. These challenges can potentially be addressed through a unified framework known as Open-set Unsupervised Domain Adaptation (OUDA).</p> <p>In general, visual signals can be decomposed into low- and high-frequency components. The former enables the unlocking of open-vocabulary capabilities in Vision-Language Models (VLMs) <d-cite key="radford2021learning"></d-cite> <d-cite key="li2021align"></d-cite> by establishing a well-aligned visual-semantic space. Conversely, the latter plagues the generalizability of deep neural networks on unseen or novel domains, as they may overfit to specific domain styles and therefore capture spurious correlations. OUDA serves as a nexus that connects a broad spectrum of research fields, including generative modelling <d-cite key="hoffman2018cycada"></d-cite> <d-cite key="ilse2020diva"></d-cite> <d-cite key="mahajan2020latent"></d-cite>, semi-supervised learning <d-cite key="berthelot2021adamatch"></d-cite> <d-cite key="zhang2020label"></d-cite>, meta-learning <d-cite key="shu2021open"></d-cite> <d-cite key="kim2022pin"></d-cite> <d-cite key="zhao2021learning"></d-cite>, open-set recognition <d-cite key="saito2021ovanet"></d-cite> <d-cite key="saito2020universal"></d-cite>, open-vocabulary learning <d-cite key="yu2023open"></d-cite> <d-cite key="zara2023autolabel"></d-cite> <d-cite key="wu2023towards"></d-cite>, out-of-distribution detection <d-cite key="shu2023clipood"></d-cite> <d-cite key="wang2023clipn"></d-cite>, few-/zero-shot learning <d-cite key="wu2022style"></d-cite> <d-cite key="fahes2023poda"></d-cite>, contrastive learning <d-cite key="da2022dual"></d-cite> <d-cite key="sahoo2021contrast"></d-cite> <d-cite key="zara2023simplifying"></d-cite>, unsupervised representation learning <d-cite key="hoyer2023mic"></d-cite> <d-cite key="vray2024distill"></d-cite>, active learning <d-cite key="zhang2022bamboo"></d-cite> <d-cite key="wu2022entropy"></d-cite>, continual learning <d-cite key="atanyan2024continuous"></d-cite> <d-cite key="lin2022prototype"></d-cite> <d-cite key="wang2302comprehensive"></d-cite>, and disentanglement of factors of variation <d-cite key="wu2022single"></d-cite> <d-cite key="wei2022unsupervised"></d-cite>. Its primary objective is to transfer knowledge from annotated source domains or pre-trained models (in the case of source-free UDA <d-cite key="liang2020we"></d-cite>) to unlabelled target domains. This transfer process requires addressing both covariate and semantic shifts, which correspond to variations in the high- and low-frequency components of visual signals, respectively. In essence, OUDA leverages methods developed within the aforementioned research areas to combat covariate or semantic shifts. On the other hand, addressing distribution shifts becomes unavoidable when extending any research problem from these areas to a more general and practical context.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0" id="figure-1"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/open_world_vision_post_uda_methods-480.webp 480w,/assets/img/open_world_vision_post_uda_methods-800.webp 800w,/assets/img/open_world_vision_post_uda_methods-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/open_world_vision_post_uda_methods.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Fig-1 The taxonomy of UDA methods. </div> <p>OUDA integrates open-set recognition/open-vocabulary learning and domain adaptation/generalization within a unified framework, aiming to address both high-level semantic shift and low-level covariate shift simultaneously and therefore presenting compounded challenges that stem from both research domains. Recent advancements have demonstrated rapid progress in constructing sophisticated open-vocabulary detectors or segmentors <d-cite key="zhu2023survey"></d-cite>, facilitated by VLMs trained on web-scale image-text pairs which offer a comprehensive prior of the real-world. Central to this advancement is the endeavour to bridge the granularity gap between coarse-grained visual-semantic associations and fine-grained visual understanding objectives. Predominant solutions can fall into three main categories: (1) incorporating fine-grained awareness into pre-training recipes <d-cite key="li2022grounded"></d-cite> <d-cite key="zhong2022regionclip"></d-cite> <d-cite key="rao2022denseclip"></d-cite>; (2) transferring knowledge from VLMs to downstream fine-grained visual understanding tasks <d-cite key="gu2021open"></d-cite> <d-cite key="kuo2022f"></d-cite> <d-cite key="wu2023cora"></d-cite> <d-cite key="he2023clip"></d-cite>; (3) the amalgamation of Vision Foundation Models (VFMs) by leveraging their complementary expertise resulting from distinct pretraining objectives <d-cite key="han2023boosting"></d-cite> <d-cite key="wang2023sam"></d-cite>. Besides, the pursuit of handling diverse vision tasks with a unified architecture and a single suite of weights in the open-set scenario <d-cite key="zou2023generalized"></d-cite> <d-cite key="zhang2023simple"></d-cite> has garnered increasing attention as a step towards constructing general-purpose vision foundation models.</p> <p>In the emerging task of Open-World Object Detection (OWOD) <d-cite key="joseph2021towards"></d-cite> <d-cite key="gupta2022ow"></d-cite> <d-cite key="wang2023detecting"></d-cite>, which combines open-set recognition with class incremental learning, the inherent open-vocabulary capability of VLMs offers convenience in identifying unknown classes. However, specialized components remain indispensable for the discovery of novel classes. Essentially, <strong>equipping a neural network with the ability to say NO when facing unfamiliar input</strong>, even with models like CLIP <d-cite key="wang2023clipn"></d-cite>, presents significant challenges. Particularly in vision tasks, developing a class-agnostic object localizer capable of generalizing to novel classes remains an open question <d-cite key="kim2022learning"></d-cite>. This challenge proves critical for two-stage open-world detectors or segmentors, as the generation of high-quality proposals for unknown classes is pivotal. Recently, there is a promising trend where class-agnostic object discovery is tackled without requiring any manual annotation by leveraging pre-trained features of self-supervised Vision Transformers (ViTs) <d-cite key="simeoni2023unsupervised"></d-cite>. However, these algorithms still struggle in complex scene-centric scenarios. OUDA introduces a more complicated pipeline compared to UDA with a close-set assumption, requiring the detection or rejection of unknown classes followed by cross-domain alignment with dominant solutions illustrated in <a href="#figure-1">Fig-1</a>. It has been demonstrated that overlooking unknown classes during domain alignment can lead to negative transfer or even catastrophic misalignment. As far as I know, existing methods for open-set recognition or novel class discovery largely rely on heuristic approaches, such as one-vs-all classifiers <d-cite key="saito2021ovanet"></d-cite>, entropy-based separation <d-cite key="saito2020universal"></d-cite>, inter-class distance or margin-based methods <d-cite key="miller2021class"></d-cite>, and leveraging zero-shot predictions from VLMs <d-cite key="yu2023open"></d-cite> <d-cite key="zara2023autolabel"></d-cite>. Furthermore, effectively separating (target-)private samples into semantically coherent clusters <d-cite key="zara2023autolabel"></d-cite>, rather than treating them indiscriminately as a generic unknown class, presents an even more formidable challenge, requiring the utilization of intrinsic structures within unseen or novel classes.</p> <p>Scaling up pre-training data with minimal human intervention has proven to be critical to foundation models, such as GLIP <d-cite key="li2022grounded"></d-cite> and SAM <d-cite key="kirillov2023segment"></d-cite>. Particularly in scenarios where manual annotation is resource-intensive <d-cite key="delatolas2024learning"></d-cite>, there’s a pressing need for an automatic data annotation framework. Such a framework should not only generate reliable pseudo labels but also continually expand the concept pool, thereby necessitating resilience to domain shifts stemming from heterogeneous data sources and open-set recognition capability. In the context of video action recognition, this task is referred to as Open-set Video Domain Adaptation (OUVDA) <d-cite key="zara2023simplifying"></d-cite> <d-cite key="zara2023autolabel"></d-cite>, which remains largely unexplored. This emerging research direction is inherently more complex due to the additional temporal dimension and the scarcity of large-scale and diverse datasets, presenting unique challenges that warrant further investigation. The closed learning loop, which involves the simultaneous evolution of model updating and dataset expansion, lays the groundwork for OWVUS capable of continual self-development over time. From a data-centric standpoint, the challenge revolves around constructing a dynamic dataset capable of consistently absorbing novel semantic categories and selecting relevant samples continually <d-cite key="de2021continual"></d-cite> and actively <d-cite key="zhang2022bamboo"></d-cite> with human-machine synergy. Continual learning <d-cite key="wang2302comprehensive"></d-cite>, characterized by rapid adaptation to evolving data distributions and the potential encounter with unseen classes while avoiding catastrophic forgetting, can thus be integrated with OUDA to fulfil this objective.</p> <p>To conclude, the prospect of unfolding possibilities and burgeoning potential in the field of OUDA and its synergies with other areas like continual learning and active learning fills me with anticipation and enthusiasm.</p> <div class="language-html highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@misc{Cai2024OpenWorld,
    author = {Xin Cai},
    title = {Random Thoughts on Open World Vision Systems},
    howpublished = {\url{https://totalvariation.github.io/blog/2024/open-world-vision-systems/}},
    note = {Accessed: 2024-02-08},
    year = {2024}
}
</code></pre></div></div> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/2024-02-01-open-world-vision-systems.bib"></d-bibliography> <div id="giscus_thread" style="max-width: 800px; margin: 0 auto;"> <script>let giscusTheme=localStorage.getItem("theme"),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"TotalVariation/TotalVariation.github.io","data-repo-id":"","data-category":"Comments","data-category-id":"","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,a])=>giscusScript.setAttribute(t,a)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Xin Cai. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>