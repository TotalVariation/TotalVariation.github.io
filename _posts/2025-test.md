---
layout: distill
output:
  distill::distill_article:
    toc: true
    toc_depth: 2
title: test 
description:
tags: RLHF, LRMs, RL
giscus_comments: true
date: 2025-03-25
featured: true

authors:
  - name: Xin Cai
    url: "https://totalvariation.github.io/"

bibliography:

# Optionally, you can add a table of contents to your post.
# NOTES:
#   - make sure that TOC names match the actual section names
#     for hyperlinks within the post to work correctly.
#   - we may want to automate TOC generation in the future using
#     jekyll-toc plugin (https://github.com/toshimaru/jekyll-toc).
#toc:
#  - name: Equations
#    # if a section has subsections, you can add them as follows:
#    # subsections:
#    #   - name: Example Child Subsection 1
#    #   - name: Example Child Subsection 2
#  - name: Citations
#  - name: Footnotes
#  - name: Code Blocks
#  - name: Interactive Plots
#  - name: Layouts
#  - name: Other Typography?

toc:
  - name: Background
  - name: Technical Overview
    subsections:
      - name: Architectures
      - name: Training Strategies and Data
      - name: Evaluation
  - name: Concluding Remarks
  - name: Acknowledgement

# Below is an example of injecting additional post-specific styles.
# If you use this post as a template, delete this _styles block.
_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }
---

## Overview on Reinforcement Learning From Human Feedback (RLHF)

The pipeline of reinforcement learning from human feedback (RLHF) comprises three stages:

- **Supervised Fine-Tuning (SFT)**: A large language model (LLM) pre-trained on the internet-scale corpora with next-token prediction loss is further fine-tuned using cross-entropy loss on a comparatively smaller dataset consisting of prompt-answer pairs (i.e., instruction-tuning dataset) and characterized by high-quality of responses, resulting in \( \pi_{\text{ref}} \).
- **Reward Modelling**: A human preference dataset $$ \mathcal{D} = \{(\mathbf{x}, \mathbf{y}^+, \mathbf{y}^-)\}_{i=1}^N $$, where \(\mathbf{x}\) denotes input prompts, \(\mathbf{y}^+\) denotes favourable answers, and \(\mathbf{y}^-\) denotes unfavourable answers, is first collected by having human annotators to rank different responses generated by \(\pi_{\text{ref}}\). Then, a reward model \(r_{\theta}(\cdot)\) is trained by minimizing the following objective,

$$\begin{equation}
\mathcal{L}_{\text{RM}} = -\mathbb{E}_{(\mathbf{x}, \mathbf{y}^+, \mathbf{y}^-)\sim \mathcal{D}}\left[\log\sigma(r_{\theta}(\mathbf{x}, \mathbf{y}^+) - r_{\theta}(\mathbf{x}, \mathbf{y}^-)) \right]\label{eq: 3-1-1}
\end{equation}$$

where \(\sigma\) denotes the logistic function.

- **RL Fine-Tuning**: In this stage, the model is optimized with the following objective through reinforcement learning, typically using Proximal Policy Optimization (PPO),

\begin{equation}
\max_{\theta}\mathbb{E}_{\mathbf{x}\sim\mathcal{D}, \mathbf{y}\sim\pi_{\theta}(\mathbf{y}\,|\,\mathbf{x})} \left[r_{\theta}(\mathbf{x}, \mathbf{y}) - \beta D_{\text{KL}}(\pi_{\theta}(\mathbf{y}\,|\,\mathbf{x})\,\|\,\pi_{\text{ref}}(\mathbf{y}\,|\,\mathbf{x})) \right]\label{eq: 3-1-2}
\end{equation}

where the reward model \(r_{\theta}(\cdot)\) trained previously is used to provide numerical scores on generated answers, and a KL-regularized term is introduced to prevent \(\pi_{\theta}\) from concentrating probability mass on a few highest-reward responses, i.e., by keeping \(\pi_{\theta}\) close to \(\pi_{\text{ref}}\) in order to maintain generation diversity. 

# Algorithm 1









