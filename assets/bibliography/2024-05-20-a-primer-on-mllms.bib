@article{luo2022understanding,
	title={Understanding diffusion models: A unified perspective},
	author={Luo, Calvin},
	journal={arXiv preprint arXiv:2208.11970},
	year={2022}
}

@article{ribeiro2024demystifying,
	title={Demystifying Variational Diffusion Models},
	author={Ribeiro, Fabio De Sousa and Glocker, Ben},
	journal={arXiv preprint arXiv:2401.06281},
	year={2024}
}

@article{chan2024tutorial,
	title={Tutorial on Diffusion Models for Imaging and Vision},
	author={Chan, Stanley H},
	journal={arXiv preprint arXiv:2403.18103},
	year={2024}
}

@inproceedings{rombach2022high,
	title={High-resolution image synthesis with latent diffusion models},
	author={Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj{\"o}rn},
	booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
	pages={10684--10695},
	year={2022}
}

@article{li2023multimodal,
	title={Multimodal foundation models: From specialists to general-purpose assistants},
	author={Li, Chunyuan and Gan, Zhe and Yang, Zhengyuan and Yang, Jianwei and Li, Linjie and Wang, Lijuan and Gao, Jianfeng},
	journal={arXiv preprint arXiv:2309.10020},
	volume={1},
	number={2},
	pages={2},
	year={2023}
}

@article{zhao2023survey,
	title={A survey of large language models},
	author={Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and others},
	journal={arXiv preprint arXiv:2303.18223},
	year={2023}
}

@inproceedings{radford2021learning,
	title={Learning transferable visual models from natural language supervision},
	author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
	booktitle={International conference on machine learning},
	pages={8748--8763},
	year={2021},
	organization={PMLR}
}

@article{brown2020language,
	title={Language models are few-shot learners},
	author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
	journal={Advances in neural information processing systems},
	volume={33},
	pages={1877--1901},
	year={2020}
}

@article{wei2022chain,
	title={Chain-of-thought prompting elicits reasoning in large language models},
	author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
	journal={Advances in neural information processing systems},
	volume={35},
	pages={24824--24837},
	year={2022}
}

@article{alayrac2022flamingo,
	title={Flamingo: a visual language model for few-shot learning},
	author={Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katherine and Reynolds, Malcolm and others},
	journal={Advances in neural information processing systems},
	volume={35},
	pages={23716--23736},
	year={2022}
}

@article{yin2023survey,
	title={A survey on multimodal large language models},
	author={Yin, Shukang and Fu, Chaoyou and Zhao, Sirui and Li, Ke and Sun, Xing and Xu, Tong and Chen, Enhong},
	journal={arXiv preprint arXiv:2306.13549},
	year={2023}
}

@article{sun2023eva,
	title={Eva-clip: Improved training techniques for clip at scale},
	author={Sun, Quan and Fang, Yuxin and Wu, Ledell and Wang, Xinlong and Cao, Yue},
	journal={arXiv preprint arXiv:2303.15389},
	year={2023}
}

@inproceedings{girdhar2023imagebind,
	title={Imagebind: One embedding space to bind them all},
	author={Girdhar, Rohit and El-Nouby, Alaaeldin and Liu, Zhuang and Singh, Mannat and Alwala, Kalyan Vasudev and Joulin, Armand and Misra, Ishan},
	booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
	pages={15180--15190},
	year={2023}
}

@article{han2023imagebind,
	title={Imagebind-llm: Multi-modality instruction tuning},
	author={Han, Jiaming and Zhang, Renrui and Shao, Wenqi and Gao, Peng and Xu, Peng and Xiao, Han and Zhang, Kaipeng and Liu, Chris and Wen, Song and Guo, Ziyu and others},
	journal={arXiv preprint arXiv:2309.03905},
	year={2023}
}

@article{oord2018representation,
	title={Representation learning with contrastive predictive coding},
	author={Oord, Aaron van den and Li, Yazhe and Vinyals, Oriol},
	journal={arXiv preprint arXiv:1807.03748},
	year={2018}
}

@article{dosovitskiy2020image,
	title={An image is worth 16x16 words: Transformers for image recognition at scale},
	author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
	journal={arXiv preprint arXiv:2010.11929},
	year={2020}
}

@article{chen2023internvl,
	title={Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks},
	author={Chen, Zhe and Wu, Jiannan and Wang, Wenhai and Su, Weijie and Chen, Guo and Xing, Sen and Muyan, Zhong and Zhang, Qinglong and Zhu, Xizhou and Lu, Lewei and others},
	journal={arXiv preprint arXiv:2312.14238},
	year={2023}
}

@inproceedings{dehghani2023scaling,
	title={Scaling vision transformers to 22 billion parameters},
	author={Dehghani, Mostafa and Djolonga, Josip and Mustafa, Basil and Padlewski, Piotr and Heek, Jonathan and Gilmer, Justin and Steiner, Andreas Peter and Caron, Mathilde and Geirhos, Robert and Alabdulmohsin, Ibrahim and others},
	booktitle={International Conference on Machine Learning},
	pages={7480--7512},
	year={2023},
	organization={PMLR}
}

@article{liu2023improved,
	title={Improved baselines with visual instruction tuning},
	author={Liu, Haotian and Li, Chunyuan and Li, Yuheng and Lee, Yong Jae},
	journal={arXiv preprint arXiv:2310.03744},
	year={2023}
}

@article{li2024mini,
	title={Mini-Gemini: Mining the Potential of Multi-modality Vision Language Models},
	author={Li, Yanwei and Zhang, Yuechen and Wang, Chengyao and Zhong, Zhisheng and Chen, Yixin and Chu, Ruihang and Liu, Shaoteng and Jia, Jiaya},
	journal={arXiv preprint arXiv:2403.18814},
	year={2024}
}

@article{schuhmann2022laion,
	title={Laion-5b: An open large-scale dataset for training next generation image-text models},
	author={Schuhmann, Christoph and Beaumont, Romain and Vencu, Richard and Gordon, Cade and Wightman, Ross and Cherti, Mehdi and Coombes, Theo and Katta, Aarush and Mullis, Clayton and Wortsman, Mitchell and others},
	journal={Advances in Neural Information Processing Systems},
	volume={35},
	pages={25278--25294},
	year={2022}
}

@inproceedings{liu2022convnet,
	title={A convnet for the 2020s},
	author={Liu, Zhuang and Mao, Hanzi and Wu, Chao-Yuan and Feichtenhofer, Christoph and Darrell, Trevor and Xie, Saining},
	booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
	pages={11976--11986},
	year={2022}
}

@article{touvron2023llama,
	title={Llama: Open and efficient foundation language models},
	author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
	journal={arXiv preprint arXiv:2302.13971},
	year={2023}
}

@article{touvron2023llama2,
	title={Llama 2: Open foundation and fine-tuned chat models},
	author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
	journal={arXiv preprint arXiv:2307.09288},
	year={2023}
}

@article{chiang2023vicuna,
	title={Vicuna: An open-source chatbot impressing gpt-4 with 90\%* chatgpt quality},
	author={Chiang, Wei-Lin and Li, Zhuohan and Lin, Zi and Sheng, Ying and Wu, Zhanghao and Zhang, Hao and Zheng, Lianmin and Zhuang, Siyuan and Zhuang, Yonghao and Gonzalez, Joseph E and others},
	journal={See https://vicuna. lmsys. org (accessed 14 April 2023)},
	volume={2},
	number={3},
	pages={6},
	year={2023}
}

@article{han2024parameter,
	title={Parameter-efficient fine-tuning for large models: A comprehensive survey},
	author={Han, Zeyu and Gao, Chao and Liu, Jinyang and Zhang, Sai Qian and others},
	journal={arXiv preprint arXiv:2403.14608},
	year={2024}
}

@article{hu2021lora,
	title={Lora: Low-rank adaptation of large language models},
	author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
	journal={arXiv preprint arXiv:2106.09685},
	year={2021}
}

@article{shen2023mixture,
	title={Mixture-of-experts meets instruction tuning: A winning combination for large language models},
	author={Shen, Sheng and Hou, Le and Zhou, Yanqi and Du, Nan and Longpre, Shayne and Wei, Jason and Chung, Hyung Won and Zoph, Barret and Fedus, William and Chen, Xinyun and others},
	journal={arXiv preprint arXiv:2305.14705},
	year={2023}
}

@article{jiang2024mixtral,
	title={Mixtral of experts},
	author={Jiang, Albert Q and Sablayrolles, Alexandre and Roux, Antoine and Mensch, Arthur and Savary, Blanche and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Hanna, Emma Bou and Bressand, Florian and others},
	journal={arXiv preprint arXiv:2401.04088},
	year={2024}
}

@inproceedings{li2023blip,
	title={Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models},
	author={Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven},
	booktitle={International conference on machine learning},
	pages={19730--19742},
	year={2023},
	organization={PMLR}
}

@article{liu2024visual,
	title={Visual instruction tuning},
	author={Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
	journal={Advances in neural information processing systems},
	volume={36},
	year={2024}
}

@article{zhang2023video,
	title={Video-llama: An instruction-tuned audio-visual language model for video understanding},
	author={Zhang, Hang and Li, Xin and Bing, Lidong},
	journal={arXiv preprint arXiv:2306.02858},
	year={2023}
}

@article{zhang2023llama,
	title={Llama-adapter: Efficient fine-tuning of language models with zero-init attention},
	author={Zhang, Renrui and Han, Jiaming and Liu, Chris and Gao, Peng and Zhou, Aojun and Hu, Xiangfei and Yan, Shilin and Lu, Pan and Li, Hongsheng and Qiao, Yu},
	journal={arXiv preprint arXiv:2303.16199},
	year={2023}
}

@article{li2023videochat,
	title={Videochat: Chat-centric video understanding},
	author={Li, KunChang and He, Yinan and Wang, Yi and Li, Yizhuo and Wang, Wenhai and Luo, Ping and Wang, Yali and Wang, Limin and Qiao, Yu},
	journal={arXiv preprint arXiv:2305.06355},
	year={2023}
}

@article{sun2023generative,
	title={Generative pretraining in multimodality},
	author={Sun, Quan and Yu, Qiying and Cui, Yufeng and Zhang, Fan and Zhang, Xiaosong and Wang, Yueze and Gao, Hongcheng and Liu, Jingjing and Huang, Tiejun and Wang, Xinlong},
	journal={arXiv preprint arXiv:2307.05222},
	year={2023}
}

@article{wu2023next,
	title={Next-gpt: Any-to-any multimodal llm},
	author={Wu, Shengqiong and Fei, Hao and Qu, Leigang and Ji, Wei and Chua, Tat-Seng},
	journal={arXiv preprint arXiv:2309.05519},
	year={2023}
}

@article{chen2023sharegpt4v,
	title={Sharegpt4v: Improving large multi-modal models with better captions},
	author={Chen, Lin and Li, Jisong and Dong, Xiaoyi and Zhang, Pan and He, Conghui and Wang, Jiaqi and Zhao, Feng and Lin, Dahua},
	journal={arXiv preprint arXiv:2311.12793},
	year={2023}
}

@article{zhang2023instruction,
	title={Instruction tuning for large language models: A survey},
	author={Zhang, Shengyu and Dong, Linfeng and Li, Xiaoya and Zhang, Sen and Sun, Xiaofei and Wang, Shuhe and Li, Jiwei and Hu, Runyi and Zhang, Tianwei and Wu, Fei and others},
	journal={arXiv preprint arXiv:2308.10792},
	year={2023}
}

@article{chen2024allava,
	title={ALLaVA: Harnessing GPT4V-synthesized Data for A Lite Vision-Language Model},
	author={Chen, Guiming Hardy and Chen, Shunian and Zhang, Ruifei and Chen, Junying and Wu, Xiangbo and Zhang, Zhiyi and Chen, Zhihong and Li, Jianquan and Wan, Xiang and Wang, Benyou},
	journal={arXiv preprint arXiv:2402.11684},
	year={2024}
}

@article{zeng2023matters,
	title={What matters in training a gpt4-style language model with multimodal inputs?},
	author={Zeng, Yan and Zhang, Hanbo and Zheng, Jiani and Xia, Jiangnan and Wei, Guoqiang and Wei, Yang and Zhang, Yuchen and Kong, Tao},
	journal={arXiv preprint arXiv:2307.02469},
	year={2023}
}

@article{ziegler2019fine,
	title={Fine-tuning language models from human preferences},
	author={Ziegler, Daniel M and Stiennon, Nisan and Wu, Jeffrey and Brown, Tom B and Radford, Alec and Amodei, Dario and Christiano, Paul and Irving, Geoffrey},
	journal={arXiv preprint arXiv:1909.08593},
	year={2019}
}

@article{ouyang2022training,
	title={Training language models to follow instructions with human feedback},
	author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
	journal={Advances in neural information processing systems},
	volume={35},
	pages={27730--27744},
	year={2022}
}

@article{rafailov2024direct,
	title={Direct preference optimization: Your language model is secretly a reward model},
	author={Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea},
	journal={Advances in Neural Information Processing Systems},
	volume={36},
	year={2024}
}

@misc{fu2024mme,
	title={MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models}, 
	author={Chaoyou Fu and Peixian Chen and Yunhang Shen and Yulei Qin and Mengdan Zhang and Xu Lin and Jinrui Yang and Xiawu Zheng and Ke Li and Xing Sun and Yunsheng Wu and Rongrong Ji},
	year={2024},
	eprint={2306.13394},
	archivePrefix={arXiv},
	primaryClass={cs.CV}
}

@article{liu2023mmbench,
	title={Mmbench: Is your multi-modal model an all-around player?},
	author={Liu, Yuan and Duan, Haodong and Zhang, Yuanhan and Li, Bo and Zhang, Songyang and Zhao, Wangbo and Yuan, Yike and Wang, Jiaqi and He, Conghui and Liu, Ziwei and others},
	journal={arXiv preprint arXiv:2307.06281},
	year={2023}
}

@article{li2023evaluating,
	title={Evaluating object hallucination in large vision-language models},
	author={Li, Yifan and Du, Yifan and Zhou, Kun and Wang, Jinpeng and Zhao, Wayne Xin and Wen, Ji-Rong},
	journal={arXiv preprint arXiv:2305.10355},
	year={2023}
}

@article{li2023mvbench,
	title={Mvbench: A comprehensive multi-modal video understanding benchmark},
	author={Li, Kunchang and Wang, Yali and He, Yinan and Li, Yizhuo and Wang, Yi and Liu, Yi and Wang, Zun and Xu, Jilan and Chen, Guo and Luo, Ping and others},
	journal={arXiv preprint arXiv:2311.17005},
	year={2023}
}

@article{leng2023mitigating,
	title={Mitigating object hallucinations in large vision-language models through visual contrastive decoding},
	author={Leng, Sicong and Zhang, Hang and Chen, Guanzheng and Li, Xin and Lu, Shijian and Miao, Chunyan and Bing, Lidong},
	journal={arXiv preprint arXiv:2311.16922},
	year={2023}
}

@article{zhai2023halle,
	title={Halle-switch: Rethinking and controlling object existence hallucinations in large vision language models for detailed caption},
	author={Zhai, Bohan and Yang, Shijia and Zhao, Xiangchen and Xu, Chenfeng and Shen, Sheng and Zhao, Dongdi and Keutzer, Kurt and Li, Manling and Yan, Tan and Fan, Xiangjun},
	journal={arXiv preprint arXiv:2310.01779},
	year={2023}
}

@inproceedings{liu2023mitigating,
	title={Mitigating hallucination in large multi-modal models via robust instruction tuning},
	author={Liu, Fuxiao and Lin, Kevin and Li, Linjie and Wang, Jianfeng and Yacoob, Yaser and Wang, Lijuan},
	booktitle={The Twelfth International Conference on Learning Representations},
	year={2023}
}

@article{li2022contrastive,
	title={Contrastive decoding: Open-ended text generation as optimization},
	author={Li, Xiang Lisa and Holtzman, Ari and Fried, Daniel and Liang, Percy and Eisner, Jason and Hashimoto, Tatsunori and Zettlemoyer, Luke and Lewis, Mike},
	journal={arXiv preprint arXiv:2210.15097},
	year={2022}
}

@article{yin2023woodpecker,
	title={Woodpecker: Hallucination correction for multimodal large language models},
	author={Yin, Shukang and Fu, Chaoyou and Zhao, Sirui and Xu, Tong and Wang, Hao and Sui, Dianbo and Shen, Yunhang and Li, Ke and Sun, Xing and Chen, Enhong},
	journal={arXiv preprint arXiv:2310.16045},
	year={2023}
}

@article{liu2023grounding,
	title={Grounding dino: Marrying dino with grounded pre-training for open-set object detection},
	author={Liu, Shilong and Zeng, Zhaoyang and Ren, Tianhe and Li, Feng and Zhang, Hao and Yang, Jie and Li, Chunyuan and Yang, Jianwei and Su, Hang and Zhu, Jun and others},
	journal={arXiv preprint arXiv:2303.05499},
	year={2023}
}

@article{you2023ferret,
	title={Ferret: Refer and ground anything anywhere at any granularity},
	author={You, Haoxuan and Zhang, Haotian and Gan, Zhe and Du, Xianzhi and Zhang, Bowen and Wang, Zirui and Cao, Liangliang and Chang, Shih-Fu and Yang, Yinfei},
	journal={arXiv preprint arXiv:2310.07704},
	year={2023}
}

@article{chen2023shikra,
	title={Shikra: Unleashing Multimodal LLM's Referential Dialogue Magic},
	author={Chen, Keqin and Zhang, Zhao and Zeng, Weili and Zhang, Richong and Zhu, Feng and Zhao, Rui},
	journal={arXiv preprint arXiv:2306.15195},
	year={2023}
}

@article{yuan2023osprey,
	title={Osprey: Pixel Understanding with Visual Instruction Tuning},
	author={Yuan, Yuqian and Li, Wentong and Liu, Jian and Tang, Dongqi and Luo, Xinjie and Qin, Chi and Zhang, Lei and Zhu, Jianke},
	journal={arXiv preprint arXiv:2312.10032},
	year={2023}
}

@article{lai2023lisa,
	title={Lisa: Reasoning segmentation via large language model},
	author={Lai, Xin and Tian, Zhuotao and Chen, Yukang and Li, Yanwei and Yuan, Yuhui and Liu, Shu and Jia, Jiaya},
	journal={arXiv preprint arXiv:2308.00692},
	year={2023}
}

@inproceedings{mao2016generation,
	title={Generation and comprehension of unambiguous object descriptions},
	author={Mao, Junhua and Huang, Jonathan and Toshev, Alexander and Camburu, Oana and Yuille, Alan L and Murphy, Kevin},
	booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
	pages={11--20},
	year={2016}
}

@article{mani2020point,
	title={Point and ask: Incorporating pointing into visual question answering},
	author={Mani, Arjun and Yoo, Nobline and Hinthorn, Will and Russakovsky, Olga},
	journal={arXiv preprint arXiv:2011.13681},
	year={2020}
}

@article{wang2023chatvideo,
	title={Chatvideo: A tracklet-centric multimodal and versatile video understanding system},
	author={Wang, Junke and Chen, Dongdong and Luo, Chong and Dai, Xiyang and Yuan, Lu and Wu, Zuxuan and Jiang, Yu-Gang},
	journal={arXiv preprint arXiv:2304.14407},
	year={2023}
}

@article{he2024ma,
	title={MA-LMM: Memory-Augmented Large Multimodal Model for Long-Term Video Understanding},
	author={He, Bo and Li, Hengduo and Jang, Young Kyun and Jia, Menglin and Cao, Xuefei and Shah, Ashish and Shrivastava, Abhinav and Lim, Ser-Nam},
	journal={arXiv preprint arXiv:2404.05726},
	year={2024}
}

@inproceedings{gutmann2010noise,
	title={Noise-contrastive estimation: A new estimation principle for unnormalized statistical models},
	author={Gutmann, Michael and Hyv{\"a}rinen, Aapo},
	booktitle={Proceedings of the thirteenth international conference on artificial intelligence and statistics},
	pages={297--304},
	year={2010},
	organization={JMLR Workshop and Conference Proceedings}
}

@article{park2022vision,
	title={How do vision transformers work?},
	author={Park, Namuk and Kim, Songkuk},
	journal={arXiv preprint arXiv:2202.06709},
	year={2022}
}

@article{dosovitskiy2020image,
	title={An image is worth 16x16 words: Transformers for image recognition at scale},
	author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
	journal={arXiv preprint arXiv:2010.11929},
	year={2020}
}

@article{wang2022self,
	title={Self-instruct: Aligning language models with self-generated instructions},
	author={Wang, Yizhong and Kordi, Yeganeh and Mishra, Swaroop and Liu, Alisa and Smith, Noah A and Khashabi, Daniel and Hajishirzi, Hannaneh},
	journal={arXiv preprint arXiv:2212.10560},
	year={2022}
}

@article{gudibande2023false,
	title={The false promise of imitating proprietary llms},
	author={Gudibande, Arnav and Wallace, Eric and Snell, Charlie and Geng, Xinyang and Liu, Hao and Abbeel, Pieter and Levine, Sergey and Song, Dawn},
	journal={arXiv preprint arXiv:2305.15717},
	year={2023}
}

@article{kung2023models,
	title={Do models really learn to follow instructions? an empirical study of instruction tuning},
	author={Kung, Po-Nien and Peng, Nanyun},
	journal={arXiv preprint arXiv:2305.11383},
	year={2023}
}

@article{wang2023all,
	title={The all-seeing project: Towards panoptic visual recognition and understanding of the open world},
	author={Wang, Weiyun and Shi, Min and Li, Qingyun and Wang, Wenhai and Huang, Zhenhang and Xing, Linjie and Chen, Zhe and Li, Hao and Zhu, Xizhou and Cao, Zhiguo and others},
	journal={arXiv preprint arXiv:2308.01907},
	year={2023}
}

@article{kiela2021dynabench,
	title={Dynabench: Rethinking benchmarking in NLP},
	author={Kiela, Douwe and Bartolo, Max and Nie, Yixin and Kaushik, Divyansh and Geiger, Atticus and Wu, Zhengxuan and Vidgen, Bertie and Prasad, Grusha and Singh, Amanpreet and Ringshia, Pratik and others},
	journal={arXiv preprint arXiv:2104.14337},
	year={2021}
}

@inproceedings{li2022grounded,
	title={Grounded language-image pre-training},
	author={Li, Liunian Harold and Zhang, Pengchuan and Zhang, Haotian and Yang, Jianwei and Li, Chunyuan and Zhong, Yiwu and Wang, Lijuan and Yuan, Lu and Zhang, Lei and Hwang, Jenq-Neng and others},
	booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
	pages={10965--10975},
	year={2022}
}

@article{yu2022coca,
	title={Coca: Contrastive captioners are image-text foundation models},
	author={Yu, Jiahui and Wang, Zirui and Vasudevan, Vijay and Yeung, Legg and Seyedhosseini, Mojtaba and Wu, Yonghui},
	journal={arXiv preprint arXiv:2205.01917},
	year={2022}
}

