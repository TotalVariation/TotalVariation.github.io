<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://totalvariation.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://totalvariation.github.io/" rel="alternate" type="text/html" hreflang="en" /><updated>2025-08-26T10:58:35+00:00</updated><id>https://totalvariation.github.io/feed.xml</id><title type="html">Xin’s Homepage</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
</subtitle><entry><title type="html">Learning the Backward Pass of FlashAttention</title><link href="https://totalvariation.github.io/blog/2025/intro-flashattention-backward-part1/" rel="alternate" type="text/html" title="Learning the Backward Pass of FlashAttention" /><published>2025-07-21T00:00:00+00:00</published><updated>2025-07-21T00:00:00+00:00</updated><id>https://totalvariation.github.io/blog/2025/intro-flashattention-backward-part1</id><content type="html" xml:base="https://totalvariation.github.io/blog/2025/intro-flashattention-backward-part1/"><![CDATA[<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/essence-of-diff-blog-flashatten-480.webp 480w,/assets/img/essence-of-diff-blog-flashatten-800.webp 800w,/assets/img/essence-of-diff-blog-flashatten-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/essence-of-diff-blog-flashatten.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
</div>
<div class="caption">
    Image Credit: <a href="https://arxiv.org/abs/2501.14787">Matrix Calculus (for Machine Learning and Beyond)</a>.
</div>

<p>Scaling Transformers to longer sequence lengths has long been hindered by the computational bottleneck of self-attention, whose runtime and memory complexity scale quadratically with sequence length. FlashAttention and its subsequent versions <d-cite key="dao2022flashattention"></d-cite> <d-cite key="dao2023flashattention"></d-cite> have achieved dramatic memory savings and wall-clock speedups through a suite of carefully engineered techniques that minimize memory reads and writes between GPU high-bandwidth memory (HBM) and on-chip SRAM—with no approximation.</p>

<p>While there are many excellent tutorials available online that provide an in-depth introduction to FlashAttention, such as the <a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&amp;t=22388s">YouTube video by Umar Jamil</a>, the backward pass remains relatively underexplored. For someone like me, who has grown accustomed to relying on automatic differentiation (AD) engines embedded in modern scientific computing frameworks like PyTorch, JAX, and Julia to handle derivatives, understanding the backward pass of FlashAttention can be a bit daunting.</p>

<p>In this blog, I aim to give a detailed explanation about the backward pass of FlashAttention, breaking it down into two parts. In the first part, I will derive the relevant gradients using matrix calculus and validate the results using two alternative methods to highlight the elegance of matrix calculus. In the second part, we will walk through the Triton implementation of the backward pass to strengthen our understanding.</p>

<h2 id="revisiting-derivatives">Revisiting Derivatives</h2>

<p>Recall from single variable calculus, a real-valued function \(f\) defined in a neighbourhood of \(a \in \mathbb{R}\) is said to be differentiable at \(a\) if the limit</p>

\[f^{\prime}(a) = \lim_{h \to 0} \frac{f(a+h) - f(a)}{h}\]

<p>exists in \(\mathbb{R}\). However, such an expression does not easily lend itself to a more generalised definition of derivatives beyond scalar inputs, such as vectors, matrices, or functions. A more useful and fundamental way to view derivatives is the linear approximation of functions near a small neighbourhood of input values: \(\delta f = f(x + \delta x) - f(x) = f^{\prime}(x)\delta x + o(\delta x)\), or the differential form: \(df = f(x + dx) - f(x) = f^{\prime}(x)dx\). For example, let \(f : U \to \mathbb{R}\), where \(U \subseteq \mathbb{R}^n\) is open, i.e., a scalar-valued function which takes in a (column) vector \(x \in \mathbb{R}^n\) and produces a scalar in \(\mathbb{R}\). From college calculus, we know that \(df = \underbrace{\nabla f(x)^T}_{f^\prime(x)} dx\).</p>

<p>More generally, by the Riesz Representation Theorem <d-footnote>I recommend checking out this self-contained article <a href="https://math.uchicago.edu/~may/REU2021/REUPapers/Adler.pdf">HILBERT SPACES AND THE RIESZ REPRESENTATION THEOREM</a> by Ben Adler, which gives a concise introduction to Hilbert spaces and the Riesz Representation theorem.</d-footnote>, for any continuous linear functional \(\phi\) on a Hilbert space <d-footnote>If a normed vector space $ V $ is a complete metric space and the norm itself is induced by an inner product on $ V $, we say $ V $ is a Hilbert space.</d-footnote> \(V\), there exists a unique \(u \in V\) such that \(\phi(v) = \langle u, v \rangle\) for all \(v \in V\), where \(\langle \cdot \rangle\) denotes inner product. Therefore, the derivative can be generalised to any Hilbert space as \(df = \langle \underbrace{\text{some vector}}_{\nabla f(x)},\: dx \rangle\).</p>

<p>For the vector space consisting of matrices \(A \in \mathbb{R}^{m \times n}\), the default inner product is defined as \(\operatorname{Tr}(A^T B) = \operatorname{vec}(A)^T \operatorname{vec}(B) = \sum_{i, j}A_{ij}B_{ij}\), which is referred to as the Frobenius inner product, in order to make it a valid Hilbert space. Therefore, for a scalar-valued function that takes in matrices \(f(A)\), its derivative can be expressed as \(df = \operatorname{Tr}(f^\prime(A)^T dA)\).</p>

<p>Next, we will leverage this trick from matrix calculus to derive the formulas for the backpropagation of standard attention.</p>

<h2 id="deriving-gradients-of-standard-attention">Deriving Gradients of Standard Attention</h2>

<h3 id="by-matrix-calculus">By Matrix Calculus</h3>

<p>Given input sequences \(Q,\: K,\: V,\: \in \mathbb{R}^{N\times d}\) where \(N\) is the sequence length and \(d\) is the head dimension, the standard attention output \(O \in \mathbb{R}^{N\times d}\) is calculated as follows (forward pass):</p>

\[S=QK^T \in \mathbb{R}^{N\times N}\quad P = \operatorname{softmax}(S) \quad O=PV \in \mathbb{R}^{N\times d}\]

<p>where \(\operatorname{softmax}\) is applied row-wise.</p>

<p>Then, assuming a scalar-valued loss function \(L\), by the backpropagation (i.e., reverse mode of automatic differentiation (AD)), the gradients of \(L\) w.r.t various inputs are calculated as follows:</p>

\[\frac{\partial L}{\partial V} = P^T \frac{\partial L}{\partial O} \in \mathbb{R}^{N\times d}\]

\[\frac{\partial L}{\partial P} = \frac{\partial L}{\partial O} V^T \in \mathbb{R}^{N\times N}\]

\[\frac{\partial L}{\partial S} = \operatorname{dsoftmax}(\frac{\partial L}{\partial P}) \in \mathbb{R}^{N\times N}\]

\[\frac{\partial L}{\partial Q} = \frac{\partial L}{\partial S}K \in \mathbb{R}^{N\times d}\]

\[\frac{\partial L}{\partial K} = \frac{\partial L}{\partial S}^T Q \in \mathbb{R}^{N\times d}\]

<p>First, we calculate the gradient w.r.t. \(V\) (\(\frac{\partial L}{\partial V} = P^T \frac{\partial L}{\partial O} \in \mathbb{R}^{N\times d}\))</p>

<p>Fix \(P\) and vary \(V\). From above, \(dO = P(V + dV) - PV = P dV\). The differential of \(dL\) becomes:</p>

\[dL = \text{Tr} \left( \left( \frac{\partial L}{\partial O} \right)^T dO \right) = \text{Tr} \left( \left( \frac{\partial L}{\partial O} \right)^T (P dV) \right).\]

<p>Substitute \(dO = P dV\):</p>

\[dL = \text{Tr} \left( \left( \frac{\partial L}{\partial O} \right)^T P dV \right) = \text{Tr} \left( \underbrace{\left( P^T \frac{\partial L}{\partial O} \right)^T}_{\frac{\partial L}{\partial V}} dV \right)\]

<p>Therefore, we get \(\frac{\partial L}{\partial V} = P^T \frac{\partial L}{\partial O} \in \mathbb{R}^{N\times d}\).</p>

<p>Similarly, the gradient w.r.t \(P\) (\(\frac{\partial L}{\partial P} = \frac{\partial L}{\partial O} V^T \in \mathbb{R}^{N\times N}\)) is derived as, given \(dO = (P + dP)V - PV = dP V\):</p>

\[dL = \text{Tr} \left( \left( \frac{\partial L}{\partial O} \right)^T dO \right) = \text{Tr} \left( \left( \frac{\partial L}{\partial O} \right)^T dP V \right) = \text{Tr} \left( V \left( \frac{\partial L}{\partial O} \right)^T dP \right) = \text{Tr} \left( \underbrace{\left( \frac{\partial L}{\partial O} V^T \right)^T}_{\frac{\partial L}{\partial P}} dP \right)\]

<p>where the cyclic property of trace is applied \(\operatorname{Tr}(ABC) = \operatorname{Tr}(BCA)\). Therefore, we get \(\frac{\partial L}{\partial P} = \frac{\partial L}{\partial O} V^T \in \mathbb{R}^{N\times N}\).</p>

<p>In \(P = \operatorname{softmax}(S)\), as \(\operatorname{softmax}\) is applied row-wise, it is more appropriate to consider the input as each row of \(S\), i.e., \(P_{i, :} = \operatorname{softmax}(S_{i, :})\) when deriving the gradient formula.</p>

\[dL = \left( \frac{\partial L}{\partial P_{i, :}} \right)^T dP_{i, :} = \left( \frac{\partial L}{\partial P_{i, :}} \right)^T \operatorname{dsoftmax}(dS_{i, :})\]

<p>The derivative \(df_x\) of a function \(f\) mapping vectors from \(\mathbb{R}^n \to \mathbb{R}^n\) is a linear transformation \(df_x \in \mathcal{L}(\mathbb{R}^n, \mathbb{R}^n)\), which can be expressed in its matrix form (Jacobian matrix). The Jocobian matrix (i.e., the derivative) of \(y = \operatorname{softmax}(x)\), where \(x \in \mathbb{R}^n\) is a column vector, is an \(n \times n\) symmetric matrix, \(\operatorname{dsoftmax}=\text{diag}(y) - yy^T=\operatorname{dsoftmax}^T\).</p>

<p>With the above derivation, we can proceed as follows:</p>

\[dL = \left( \frac{\partial L}{\partial P_{i, :}} \right)^T \operatorname{dsoftmax}(dS_{i, :}) = \left( \operatorname{dsoftmax}^T \frac{\partial L}{\partial P_{i, :}} \right)^T dS_{i, :} = \left( \operatorname{dsoftmax} \frac{\partial L}{\partial P_{i, :}} \right)^T dS_{i, :}\]

<p>Therefore, we arrive at \(\frac{\partial L}{\partial S_{i, :}} = \operatorname{dsoftmax}(\frac{\partial L}{\partial P_{i, :}})\). With a slight abuse of notation, it can be compactly written as \(\frac{\partial L}{\partial S} = \operatorname{dsoftmax}(\frac{\partial L}{\partial P}) \in \mathbb{R}^{N \times N}\).</p>

<h3 id="by-two-alternative-methods">By Two Alternative Methods</h3>

<p>Next, we will resort to two alternative methods to verify the correctness of our previous derivations. Yet you will find them a bit more cumbersome.</p>

<p><strong>Component-wise</strong></p>

<p>Recall that in multivariable calculus, Let \(U \subseteq \mathbb{R}^n\) and \(V \subseteq \mathbb{R}^m\) be open and \(f:\: U \to \mathbb{R}^m,\: g:\: V \to \mathbb{R}^k\) with \(f(U) \subseteq V\). Let \(f\) be differentiable on \(U\) and \(g\) differentiable on \(V\). Set \(y = f(x)\) and \(z = (g \circ f)(x) = g(y)\). Then the chain rule \((g \circ f)^\prime(x) = g^\prime(y)f^\prime(x)\) can be written in its matrix form:</p>

\[\begin{bmatrix}\frac{\partial z_1}{\partial x_1} &amp; \dots &amp;\frac{\partial z_1}{\partial x_n}\\ \vdots &amp; &amp; \vdots\\ \frac{\partial z_k}{\partial x_1} &amp; \dots &amp;\frac{\partial z_k}{\partial x_n} \end{bmatrix} = \begin{bmatrix}\frac{\partial z_1}{\partial y_1} &amp; \dots &amp;\frac{\partial z_1}{\partial y_m}\\ \vdots &amp; &amp; \vdots\\ \frac{\partial z_k}{\partial y_1} &amp; \dots &amp;\frac{\partial z_k}{\partial y_m} \end{bmatrix} \begin{bmatrix}\frac{\partial y_1}{\partial x_1} &amp; \dots &amp;\frac{\partial y_1}{\partial x_n}\\ \vdots &amp; &amp; \vdots\\ \frac{\partial y_m}{\partial x_1} &amp; \dots &amp;\frac{\partial y_m}{\partial x_n} \end{bmatrix}\]

<p>In the special case where \(g:\: V \to \mathbb{R}\), the matrix form of the chain rule is expressed as follows:</p>

\[\left[\frac{\partial L}{\partial x_1}, \dots, \frac{\partial L}{\partial x_n} \right] = \left[ \frac{\partial L}{\partial y_1}, \dots, \frac{\partial L}{\partial y_m} \right] \begin{bmatrix}\frac{\partial y_1}{\partial x_1} &amp; \dots &amp;\frac{\partial y_1}{\partial x_n}\\ \vdots &amp; &amp; \vdots\\ \frac{\partial y_m}{\partial x_1} &amp; \dots &amp;\frac{\partial y_m}{\partial x_n} \end{bmatrix}\]

<p>If written in component-wise form, we arrive at the familiar formula:</p>

\[\frac{\partial L}{\partial x_j} = \sum_{k=1}^m \frac{\partial L}{\partial y_k}\frac{\partial y_k}{\partial x_j}\]

<p>To prove \(\frac{\partial L}{\partial V} = P^T \frac{\partial L}{\partial O} \in \mathbb{R}^{N\times d}\) using the (multivariable) chain rule, as it only works for vectors as inputs, we should consider each column \(V_{:, j}\) seperately, i.e., \(O_{:, j} = P V_{:, j}\). Yet the derived outcomes from such a workaround can be effortlessly transferred to the matrix \(V\), as each column \(V_{:, j}\) shares the same Jacobian matrix \(P\).</p>

<p>As \(O_{kj} = \sum_{m=1}^N P_{km} V_{mj}\), we get \(\frac{\partial O_{kj}}{\partial V_{ij}} = P_{ki}\). Or, it can be read off from the Jacobian matrix \(P\) as the k-th row and i-th column element (\(\frac{\partial O[:, j]_{k}}{\partial V[:, j]_{i}}\)).</p>

<p>Applying the chain rule, we get</p>

\[\frac{\partial L}{\partial V_{ij}} = \sum_{k=1}^N \frac{\partial L}{\partial O_{kj}} \frac{\partial O_{kj}}{\partial V_{ij}} = \sum_{k=1}^N \frac{\partial L}{\partial O_{kj}} P_{ki} = (P^T \frac{\partial L}{\partial O})_{ij}\]

<p>Similarly, we can prove \(\frac{\partial L}{\partial P} = \frac{\partial L}{\partial O} V^T \in \mathbb{R}^{N\times N}\) by treating each row \(P_{i,:}\) independently which shares the same Jacobian matrix \(V^T\) (\(O^T = V^T P^T\)): \(O_{ik} = \sum_{m=1}^N P_{im} V_{mk}\), so \(\frac{\partial O_{ik}}{\partial P_{ij}} = V_{jk}\). Or, it can be read off from the Jacobian matrix \(V^T\) as the k-th row and j-th column element \(V^T_{kj}\) (\(\frac{\partial O[i, :]_{k}}{\partial P[i, :]_{j}}\)).</p>

\[\frac{\partial L}{\partial P_{ij}} = \sum_{k=1}^d \frac{\partial L}{\partial O_{ik}} \frac{\partial O_{ik}}{\partial P_{ij}} = \sum_{k=1}^d \frac{\partial L}{\partial O_{ik}} V_{jk} = \left( \frac{\partial L}{\partial O} V^T \right)_{ij}\]

<p><strong>Matrix vectorisation and the Kronecker product</strong></p>

<p>Actually, it is legitimate to directly work with the Jacobian matrix of matrix inputs/outputs, as any linear operator that transforms vectors between finite-dimensional vector spaces can be expressed in its matrix form once the bases for the input and output vector spaces have been selected. The most common way to achieve this involves matrix vectorisation and the Kronecker product.</p>

<p>The vectorization \(\text{vec}(A) \in \mathbb{R}^{mn}\) of any \(m \times n\) matrix \(A \in \mathbb{R}^{m \times n}\) is defined by simply stacking the columns of \(A\), from left to right, into a column vector \(\text{vec}(A)\). That is, if we denote the \(n\) columns of \(A\) by m-component vectors \(\overrightarrow{a_1}, \overrightarrow{a_2}, \dots, \overrightarrow{a_n} \in \mathbb{R}^m\), then</p>

\[\text{vec}(A) = \text{vec}(\left[ \overrightarrow{a_1}, \overrightarrow{a_2}, \dots, \overrightarrow{a_n} \right]) = \begin{pmatrix} \overrightarrow{a_1}\\ \overrightarrow{a_2}\\ \vdots \\ \overrightarrow{a_n}  \end{pmatrix} \in \mathbb{R}^{mn}\]

<p>Then, the Kronecker-product identity that plays a key role in the derivation is:</p>

\[(A \otimes B) \text{vec}(C) = \text{vec}(BCA^T)\]

<p>where \(A, B, C\) are compactly sized matrices. There are two special cases derived from the above Kronecker-product identity:</p>

\[(I \otimes B) \text{vec}(C) = \text{vec}(BC)\]

\[(A \otimes I) \text{vec}(C) = \text{vec}(CA^T )\]

<details><summary>Click here to know more</summary>
<p>Proof:</p>

<p>To prove \((I \otimes B) \text{vec}(C) = \text{vec}(BC)\), assume \(B \in \mathbb{R}^{n \times m}\) and \(C \in \mathbb{R}^{m \times k}\),</p>

\[BC = B \left[ \overrightarrow{c_1}, \overrightarrow{c_2}, \dots, \overrightarrow{c_k} \right] = \left[ B\overrightarrow{c_1}, B\overrightarrow{c_2}, \dots, B\overrightarrow{c_k} \right]\]

\[\text{vec}(BC) = \begin{pmatrix} B\overrightarrow{c_1}\\ B\overrightarrow{c_2}\\ \vdots \\ B\overrightarrow{c_k} \end{pmatrix} = \underbrace{\begin{pmatrix} B &amp; &amp; &amp; \\ &amp; B &amp; &amp; \\ &amp; &amp; \ddots &amp; \\ &amp; &amp; &amp; B \end{pmatrix}}_{\left( I \otimes B \right)} \begin{pmatrix} \overrightarrow{c_1}\\ \overrightarrow{c_2}\\ \vdots \\ \overrightarrow{c_k} \end{pmatrix}\]

<p>where \(I \in \mathbb{R}^{k \times k}\).</p>

<p>To prove \((A \otimes I) \text{vec}(C) = \text{vec}(CA^T )\), assume \(A \in \mathbb{R}^{n \times k}\) and \(C \in \mathbb{R}^{m \times k}\),</p>

\[CA^T = \left[ \sum_{j=1}^k a_{1j} \overrightarrow{c_j}, \sum_{j=1}^k a_{2j} \overrightarrow{c_j}, \dots, \sum_{j=1}^k a_{nj} \overrightarrow{c_j} \right]\]

\[\text{vec}(CA^T) = \begin{pmatrix} \sum_{j=1}^k a_{1j} \overrightarrow{c_j}\\ \sum_{j=1}^k a_{2j} \overrightarrow{c_j}\\ \vdots \\ \sum_{j=1}^k a_{nj} \overrightarrow{c_j}  \end{pmatrix} = \underbrace{\begin{pmatrix} a_{11}I &amp; a_{12}I &amp; \cdots &amp; a_{1k}I \\ a_{21}I &amp; a_{22}I &amp; \cdots &amp; a_{2k}I \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ a_{n1}I &amp; a_{n2}I &amp; \cdots &amp; a_{nk}I \end{pmatrix}}_{(A \otimes I)} \begin{pmatrix} \overrightarrow{c_1} \\ \overrightarrow{c_2}\\ \vdots \\ \overrightarrow{c_k}  \end{pmatrix}\]

<p>where \(I \in \mathbb{R}^{m \times m}\).</p>

<p>To prove \((A \otimes B) \text{vec}(C) = \text{vec}(BCA^T)\),</p>

\[\text{vec}(BCA^T) = (I \otimes B) \text{vec}(CA^T) = (I \otimes B)(A \otimes I) \text{vec}(C) = (A \otimes B) \text{vec}(C)\]

<p>where we used the property of the Kronecker product \((A \otimes B)(C \otimes D) = (AC \otimes BD)\).</p>
</details>

<p>We are now equipped with all the necessary tools to provide a slightly more elegant way to prove gradients involving matrix inputs/outputs than chunking matrices into column vectors as done previously.</p>

<p>To prove \(\frac{\partial L}{\partial V} = P^T \frac{\partial L}{\partial O} \in \mathbb{R}^{N\times d}\),  as \(dO = P dV\), by vectorisation,</p>

\[\text{vec}(dO) = \text{vec}(P dV) = \underbrace{(I \otimes P)}_{\text{Jacobian matrix :}\; \frac{\partial O}{\partial V}} \text{vec}(dV)\]

<p>Recall the special case of multivariable chain rule \(z = (g \circ f)(x) = g(y)\) where \(g:\: V \to \mathbb{R}\),</p>

\[\left[\frac{\partial L}{\partial x_1}, \dots, \frac{\partial L}{\partial x_n} \right] = \left[ \frac{\partial L}{\partial y_1}, \dots, \frac{\partial L}{\partial y_m} \right] \begin{bmatrix}\frac{\partial y_1}{\partial x_1} &amp; \dots &amp;\frac{\partial y_1}{\partial x_n}\\ \vdots &amp; &amp; \vdots\\ \frac{\partial y_m}{\partial x_1} &amp; \dots &amp;\frac{\partial y_m}{\partial x_n} \end{bmatrix}\]

<p>We get</p>

\[\text{vec}({\frac{\partial L}{\partial V}})^T = \text{vec}({\frac{\partial L}{\partial O}})^T (I \otimes P)\]

\[\text{vec}({\frac{\partial L}{\partial V}}) = (I \otimes P)^T \text{vec}({\frac{\partial L}{\partial O}}) = (I \otimes P^T) \text{vec}({\frac{\partial L}{\partial O}}) = \text{vec}(P^T \frac{\partial L}{\partial O}) \quad \text{QED}\]

<p>where we used one property of Kronecker-product \((A \otimes B)^T = A^T \otimes B^T\).</p>

<p>Similarly, the proof of \(\frac{\partial L}{\partial P} = \frac{\partial L}{\partial O} V^T \in \mathbb{R}^{N\times N}\) is shown as follows:</p>

\[dO = dP V\]

\[\text{vec}(dO) = \text{vec}(dP V) = \underbrace{(V^T \otimes I)}_{\text{Jacobian matrix :}\; \frac{\partial O}{\partial P}} \text{vec}(dP)\]

\[\text{vec}({\frac{\partial L}{\partial P}})^T = \text{vec}({\frac{\partial L}{\partial O}})^T (V^T \otimes I)\]

\[\text{vec}({\frac{\partial L}{\partial P}}) = (V^T \otimes I)^T \text{vec}({\frac{\partial L}{\partial O}}) = (V \otimes I) \text{vec}({\frac{\partial L}{\partial O}}) = \text{vec}({\frac{\partial L}{\partial O}} V^T)  \quad \text{QED}\]

<p>It can be seen that using the trick of matrix vectorisation is more conceptually clear than dealing with gradients involving matrices in a component-wise manner. Yet it is still a bit cumbersome compared to techniques in matrix calculus.</p>

<h2 id="concluding-remarks">Concluding Remarks</h2>

<p>While matrix calculus plays a pivotal role in machine learning and large-scale optimisation, the advent of automatic differentiation (AD) engines in modern scientific computing libraries has largely relieved practitioners from the burden of manually computing derivatives of complex structures like matrices and higher-order tensors. Nevertheless, matrix calculus remains a powerful tool, well worthy to be included in your analytical arsenal.</p>

<p>In <a href="https://totalvariation.github.io/blog/2025/intro-flashattention-backward-part2/">part 2</a> of this blog, we will walk through the implementation of the backward pass of FlashAttention in Triton.</p>

<h2 id="acknowledgement">Acknowledgement</h2>

<p>We express our gratitude to Matrix Calculus (for Machine Learning and Beyond) <d-cite key="bright2025matrix"></d-cite>, a set of lecture notes compiled from the MIT OpenCourse <a href="https://ocw.mit.edu/courses/18-s096-matrix-calculus-for-machine-learning-and-beyond-january-iap-2023/">Matrix Calculus for Machine Learning and Beyond</a>. We warmly recommend these materials to readers seeking for a deeper, principle-driven understanding of differentiation in the context of large-scale computing.</p>

<div class="language-html highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@misc{Cai2025flashattnbackward-1,
    author = {Xin Cai},
    title = {Learning the Backward Pass of FlashAttention: Part I Derivations},
    howpublished = {\url{https://totalvariation.github.io/blog/2025/intro-flashattention-backward-part1/}},
    note = {Accessed: 2025-07-21},
    year = {2025}
}
</code></pre></div></div>]]></content><author><name>Xin Cai</name></author><summary type="html"><![CDATA[Part I Derivations]]></summary></entry><entry><title type="html">Learning the Backward Pass of FlashAttention</title><link href="https://totalvariation.github.io/blog/2025/intro-flashattention-backward-part2/" rel="alternate" type="text/html" title="Learning the Backward Pass of FlashAttention" /><published>2025-07-21T00:00:00+00:00</published><updated>2025-07-21T00:00:00+00:00</updated><id>https://totalvariation.github.io/blog/2025/intro-flashattention-backward-part2</id><content type="html" xml:base="https://totalvariation.github.io/blog/2025/intro-flashattention-backward-part2/"><![CDATA[<h2 id="recap-forward-and-backward-passes-of-standard-attention">Recap Forward and Backward Passes of Standard Attention</h2>

<p>In the first part of this tutorial, we have walked through a detailed derivation of formulas used in the backward pass of standard attention. For ease of reference, they are included as follows:</p>

<p>Given input sequences \(Q,\: K,\: V,\: \in \mathbb{R}^{N\times d}\) where \(N\) is the sequence length and \(d\) is the head dimension, the standard attention output \(O \in \mathbb{R}^{N\times d}\) is calculated as follows (forward pass):</p>

\[S=QK^T \in \mathbb{R}^{N\times N}\quad P = \operatorname{softmax}(S) \quad O=PV \in \mathbb{R}^{N\times d}\]

<p>where \(\operatorname{softmax}\) is applied row-wise.</p>

<p>Then, assuming a scalar-valued loss function \(L\), by the backpropagation (i.e., reverse mode of automatic differentiation (AD)), the gradients of \(L\) w.r.t various inputs are calculated as follows:</p>

\[\frac{\partial L}{\partial V} = P^T \frac{\partial L}{\partial O} \in \mathbb{R}^{N\times d}\]

\[\frac{\partial L}{\partial P} = \frac{\partial L}{\partial O} V^T \in \mathbb{R}^{N\times N}\]

\[\frac{\partial L}{\partial S} = \operatorname{dsoftmax}(\frac{\partial L}{\partial P}) \in \mathbb{R}^{N\times N}\]

\[\frac{\partial L}{\partial Q} = \frac{\partial L}{\partial S}K \in \mathbb{R}^{N\times d}\]

\[\frac{\partial L}{\partial K} = \frac{\partial L}{\partial S}^T Q \in \mathbb{R}^{N\times d}\]

<h2 id="the-implementation-of-the-backward-pass-of-flashattention-in-triton">The Implementation of the Backward Pass of FlashAttention in Triton</h2>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/flashattn-backward-pseudocode-480.webp 480w,/assets/img/flashattn-backward-pseudocode-800.webp 800w,/assets/img/flashattn-backward-pseudocode-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/flashattn-backward-pseudocode.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
</div>
<div class="caption">
    Image Credit: FlashAttention2 paper.
</div>

<p>To construct a direct correspondence between the mathematical equations and Triton code, we replace \(\frac{\partial L}{\partial V}\) with \(dV\) with a slight abuse of notation <d-footnote>Please note that $ dV $ hereafter will no longer denote differential.</d-footnote>, as in the backward pass, the matrix \(dV\) contains the gradient of scalar-valued loss function \(L\) w.r.t. \(V\), i.e., \(\frac{\partial L}{\partial V}\). By applying similar replacements to all the other variables, we therefore obtain the following equations adopted in the FlashAttention2 paper <d-cite key="dao2023flashattention"></d-cite>:</p>

\[dV = P^T dO \in \mathbb{R}^{N\times d}\]

\[dP = dOV^T \in \mathbb{R}^{N\times N}\]

\[dS = \operatorname{dsoftmax}(dP) \in \mathbb{R}^{N\times N}\]

\[dQ = dSK \in \mathbb{R}^{N\times d}\]

\[dK = dS^T Q \in \mathbb{R}^{N\times d}\]

<p>Another trick adopted in the FlashAttention paper <d-cite key="dao2022flashattention"></d-cite> is to simplify the calculation of \(dS = \operatorname{dsoftmax}(dP)\), which is clearly derived in its appendix.</p>

<p>For self-containedness, it is included as follows, (Please note \(dS_{i,:}, dP_{i,:}\) are all column vectors):</p>

\[dS_{i,:} = \operatorname{dsoftmax}dP_{i,:} = (\text{diag}(P_{i,:}) - P_{i,:}P_{i,:}^T)dP_{i,:} = P_{i,:} \circ dP_{i,:} - \left( P_{i,:}^T dP_{i,:} \right) P_{i,:}\]

<p>where \(\circ\) denotes Hadamard product (i.e., pointwise multiplication).</p>

<p>Recall that \(dP = dO V^T\), written in element-wise form, \(dP_{ij} = do_i^T v_j\), (Please note \(do_j, v_j, k_j\) here denote the j-th row of \(dO, V, K\) respectively, acting as a column vector.)</p>

<p>Now, we can define</p>

\[D_i = P_{i,:}^T dP_{i,:} =  \sum_j \frac{\exp(q_i^T k_j)}{L_i} do_i^T v_j = do_i^T \sum_j \frac{\exp(q_i^T k_j)}{L_i} v_j = do_i^T o_i\]

<p>then \(dS_{i,:} = P_{i,:} \circ dP_{i,:} - D_i P_{i,:}\).</p>

<p>Readers seeking a comprehensive treatment (e.g., the online-softmax trick in the forward pass) of FlashAttention are encouraged to refer to the original papers <d-cite key="dao2022flashattention"></d-cite> <d-cite key="dao2023flashattention"></d-cite> or other tutorials available online focusing on the forward pass.</p>

<p>Now, we are in a position to dive into the Triton implementation of the backward pass of FlashAttention2.</p>

<p>We assume readers have a basic familiarity with Triton. Otherwise, there are many excellent Triton tutorials, including the <a href="https://triton-lang.org/main/getting-started/tutorials/index.html">official ones</a>, available online for your reference. In my view, figuring out how to move pointers to accurately access blocks of elements (i.e., load and store) in parallelly launched Triton programs is sufficient to grasp the core mechanisms of custom kernels developed in Triton.</p>

<p>Instead of using <code class="language-plaintext highlighter-rouge">block pointer</code> defined by <code class="language-plaintext highlighter-rouge">make_block_ptr</code>, I find that directly working with N-dimensional pointers to access elements in memory is more straightforward. Furthermore, <code class="language-plaintext highlighter-rouge">mask</code> and <code class="language-plaintext highlighter-rouge">other</code> are implicitly broadcast to <code class="language-plaintext highlighter-rouge">pointer.shape</code> when using N-dimensional pointers, which can be conveniently used to handle boundary conditions.</p>

<p>In the following, I will give some visual illustrations to facilitate your understanding of how <code class="language-plaintext highlighter-rouge">tl.load()</code> works, as there is no difference in read (<code class="language-plaintext highlighter-rouge">tl.load()</code>) and write (<code class="language-plaintext highlighter-rouge">tl.store()</code>) operations as long as their indexes are specified correctly.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

  <span class="n">N</span> <span class="o">=</span> <span class="mi">8</span>
  <span class="c1"># Here, the content of the array is made intentionally to be the exact same as offsets relative to the base pointer.
</span>  <span class="c1"># Please note that in Triton language, all Pytorch tensors are implicitly converted to base pointers.
</span>
  <span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">N</span> <span class="o">*</span> <span class="n">N</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span>
  <span class="nf">print</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
  
  <span class="p">[[</span> <span class="mi">0</span>  <span class="mi">1</span>  <span class="mi">2</span>  <span class="mi">3</span>  <span class="mi">4</span>  <span class="mi">5</span>  <span class="mi">6</span>  <span class="mi">7</span><span class="p">]</span>
   <span class="p">[</span> <span class="mi">8</span>  <span class="mi">9</span> <span class="mi">10</span> <span class="mi">11</span> <span class="mi">12</span> <span class="mi">13</span> <span class="mi">14</span> <span class="mi">15</span><span class="p">]</span>
   <span class="p">[</span><span class="mi">16</span> <span class="mi">17</span> <span class="mi">18</span> <span class="mi">19</span> <span class="mi">20</span> <span class="mi">21</span> <span class="mi">22</span> <span class="mi">23</span><span class="p">]</span>
   <span class="p">[</span><span class="mi">24</span> <span class="mi">25</span> <span class="mi">26</span> <span class="mi">27</span> <span class="mi">28</span> <span class="mi">29</span> <span class="mi">30</span> <span class="mi">31</span><span class="p">]</span>
   <span class="p">[</span><span class="mi">32</span> <span class="mi">33</span> <span class="mi">34</span> <span class="mi">35</span> <span class="mi">36</span> <span class="mi">37</span> <span class="mi">38</span> <span class="mi">39</span><span class="p">]</span>
   <span class="p">[</span><span class="mi">40</span> <span class="mi">41</span> <span class="mi">42</span> <span class="mi">43</span> <span class="mi">44</span> <span class="mi">45</span> <span class="mi">46</span> <span class="mi">47</span><span class="p">]</span>
   <span class="p">[</span><span class="mi">48</span> <span class="mi">49</span> <span class="mi">50</span> <span class="mi">51</span> <span class="mi">52</span> <span class="mi">53</span> <span class="mi">54</span> <span class="mi">55</span><span class="p">]</span>
   <span class="p">[</span><span class="mi">56</span> <span class="mi">57</span> <span class="mi">58</span> <span class="mi">59</span> <span class="mi">60</span> <span class="mi">61</span> <span class="mi">62</span> <span class="mi">63</span><span class="p">]]</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="n">BLOCK_M</span> <span class="o">=</span> <span class="mi">2</span>
  <span class="n">col_dim</span> <span class="o">=</span> <span class="n">N</span>

  <span class="n">stride_row</span> <span class="o">=</span> <span class="n">N</span>
  <span class="n">stride_col</span> <span class="o">=</span> <span class="mi">1</span>

  <span class="n">offs_m</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">BLOCK_M</span><span class="p">)[:,</span> <span class="bp">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">stride_row</span> <span class="o">+</span> <span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">col_dim</span><span class="p">)[</span><span class="bp">None</span><span class="p">,</span> <span class="p">:]</span> <span class="o">*</span> <span class="n">stride_col</span>

  <span class="c1"># N-dimensional tensors are stored contiguously in memory. 
</span>  <span class="c1"># Otherwise, it would be recommended to call x.contiguous() before taking any tensor operations. 
</span>  <span class="c1"># Here, we mimic this feature with np.ndarray.flatten.
</span>
  <span class="c1"># illustrate loading tensors from memory
</span>  <span class="nf">print</span><span class="p">(</span><span class="n">A</span><span class="p">.</span><span class="nf">flatten</span><span class="p">()[</span><span class="n">offs_m</span><span class="p">])</span>

  <span class="p">[[</span> <span class="mi">0</span>  <span class="mi">1</span>  <span class="mi">2</span>  <span class="mi">3</span>  <span class="mi">4</span>  <span class="mi">5</span>  <span class="mi">6</span>  <span class="mi">7</span><span class="p">]</span>
   <span class="p">[</span> <span class="mi">8</span>  <span class="mi">9</span> <span class="mi">10</span> <span class="mi">11</span> <span class="mi">12</span> <span class="mi">13</span> <span class="mi">14</span> <span class="mi">15</span><span class="p">]]</span>

</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="c1"># illustrate moving blocks `step_size` rows down, which will be used in the for loop to 
</span>  <span class="c1"># traverse over one dimension of a tensor.
</span>  <span class="n">step_size</span> <span class="o">=</span> <span class="mi">2</span>
  <span class="nf">print</span><span class="p">(</span><span class="n">A</span><span class="p">.</span><span class="nf">flatten</span><span class="p">()[</span><span class="n">offs_m</span> <span class="o">+</span> <span class="n">step_size</span> <span class="o">*</span> <span class="n">N</span><span class="p">])</span>

  <span class="p">[[</span><span class="mi">16</span> <span class="mi">17</span> <span class="mi">18</span> <span class="mi">19</span> <span class="mi">20</span> <span class="mi">21</span> <span class="mi">22</span> <span class="mi">23</span><span class="p">]</span>
   <span class="p">[</span><span class="mi">24</span> <span class="mi">25</span> <span class="mi">26</span> <span class="mi">27</span> <span class="mi">28</span> <span class="mi">29</span> <span class="mi">30</span> <span class="mi">31</span><span class="p">]]</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="c1"># illustrate loading tensors directly in its transposed version and moving blocks accordingly
</span>  <span class="n">offs_m_T</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">BLOCK_M</span><span class="p">)[</span><span class="bp">None</span><span class="p">,</span> <span class="p">:]</span> <span class="o">*</span> <span class="n">stride_row</span> <span class="o">+</span> <span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">col_dim</span><span class="p">)[:,</span> <span class="bp">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">stride_col</span>
  <span class="nf">print</span><span class="p">(</span><span class="n">A</span><span class="p">.</span><span class="nf">flatten</span><span class="p">()[</span><span class="n">offs_m_T</span><span class="p">])</span>
  <span class="nf">print</span><span class="p">(</span><span class="n">A</span><span class="p">.</span><span class="nf">flatten</span><span class="p">()[</span><span class="n">offs_m_T</span> <span class="o">+</span> <span class="n">step_size</span> <span class="o">*</span> <span class="n">N</span><span class="p">])</span>

  <span class="p">[[</span> <span class="mi">0</span>  <span class="mi">8</span><span class="p">]</span>
   <span class="p">[</span> <span class="mi">1</span>  <span class="mi">9</span><span class="p">]</span>
   <span class="p">[</span> <span class="mi">2</span> <span class="mi">10</span><span class="p">]</span>
   <span class="p">[</span> <span class="mi">3</span> <span class="mi">11</span><span class="p">]</span>
   <span class="p">[</span> <span class="mi">4</span> <span class="mi">12</span><span class="p">]</span>
   <span class="p">[</span> <span class="mi">5</span> <span class="mi">13</span><span class="p">]</span>
   <span class="p">[</span> <span class="mi">6</span> <span class="mi">14</span><span class="p">]</span>
   <span class="p">[</span> <span class="mi">7</span> <span class="mi">15</span><span class="p">]]</span>

  <span class="p">[[</span><span class="mi">16</span> <span class="mi">24</span><span class="p">]</span>
   <span class="p">[</span><span class="mi">17</span> <span class="mi">25</span><span class="p">]</span>
   <span class="p">[</span><span class="mi">18</span> <span class="mi">26</span><span class="p">]</span>
   <span class="p">[</span><span class="mi">19</span> <span class="mi">27</span><span class="p">]</span>
   <span class="p">[</span><span class="mi">20</span> <span class="mi">28</span><span class="p">]</span>
   <span class="p">[</span><span class="mi">21</span> <span class="mi">29</span><span class="p">]</span>
   <span class="p">[</span><span class="mi">22</span> <span class="mi">30</span><span class="p">]</span>
   <span class="p">[</span><span class="mi">23</span> <span class="mi">31</span><span class="p">]]</span>
</code></pre></div></div>

<p>Here, we analyse a simplified version of FlashAttention (technically, FlashAttention2) adapted from the official Triton tutorial <a href="https://triton-lang.org/main/getting-started/tutorials/06-fused-attention.html#fused-attention">Fused Attention</a>, accounting for both the ‘Causal’ and ‘Non-Causal’ modes.</p>

<p>The implementation of the backward pass of FlashAttention can be generally grouped into three stages:</p>

<ol>
  <li>
    <p>Calculate the matrix \(D\) first as a preprocessing step, where \(D_i = do_i^T o_i\), which corresponds to the variable <code class="language-plaintext highlighter-rouge">delta = torch.empty_like(M)</code>. Its size is <code class="language-plaintext highlighter-rouge">(Batch, Num_Heads, N_CTX)</code>, and is realised in the function <code class="language-plaintext highlighter-rouge">_attn_bwd_preprocess()</code>.</p>
  </li>
  <li>
    <p>Calculate \(dV, dK\) via the function <code class="language-plaintext highlighter-rouge">_attn_bwd_dkdv()</code>.</p>
  </li>
  <li>
    <p>Calculate \(dQ\) via the function <code class="language-plaintext highlighter-rouge">_attn_bwd_dq()</code>.</p>
  </li>
</ol>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="nd">@triton.jit</span>
  <span class="k">def</span> <span class="nf">_attn_bwd_preprocess</span><span class="p">(</span><span class="n">O</span><span class="p">,</span> <span class="n">DO</span><span class="p">,</span>  <span class="c1">#
</span>                           <span class="n">Delta</span><span class="p">,</span>  <span class="c1">#
</span>                           <span class="n">Z</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">N_CTX</span><span class="p">,</span>  <span class="c1">#
</span>                           <span class="n">BLOCK_M</span><span class="p">:</span> <span class="n">tl</span><span class="p">.</span><span class="n">constexpr</span><span class="p">,</span> <span class="n">HEAD_DIM</span><span class="p">:</span> <span class="n">tl</span><span class="p">.</span><span class="n">constexpr</span>  <span class="c1">#
</span>                           <span class="p">):</span>
      <span class="n">off_m</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">program_id</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="o">*</span> <span class="n">BLOCK_M</span> <span class="o">+</span> <span class="n">tl</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">BLOCK_M</span><span class="p">)</span>
      <span class="n">off_hz</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">program_id</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
      <span class="n">off_n</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">HEAD_DIM</span><span class="p">)</span>
      <span class="c1"># load
</span>      <span class="n">o</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="n">O</span> <span class="o">+</span> <span class="n">off_hz</span> <span class="o">*</span> <span class="n">HEAD_DIM</span> <span class="o">*</span> <span class="n">N_CTX</span> <span class="o">+</span> <span class="n">off_m</span><span class="p">[:,</span> <span class="bp">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">HEAD_DIM</span> <span class="o">+</span> <span class="n">off_n</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="p">:]).</span><span class="nf">to</span><span class="p">(</span><span class="n">tl</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
      <span class="n">do</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="n">DO</span> <span class="o">+</span> <span class="n">off_hz</span> <span class="o">*</span> <span class="n">HEAD_DIM</span> <span class="o">*</span> <span class="n">N_CTX</span> <span class="o">+</span> <span class="n">off_m</span><span class="p">[:,</span> <span class="bp">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">HEAD_DIM</span> <span class="o">+</span> <span class="n">off_n</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="p">:]).</span><span class="nf">to</span><span class="p">(</span><span class="n">tl</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
      <span class="n">delta</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">o</span> <span class="o">*</span> <span class="n">do</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  
      <span class="n">tl</span><span class="p">.</span><span class="nf">store</span><span class="p">(</span><span class="n">Delta</span> <span class="o">+</span> <span class="n">off_hz</span> <span class="o">*</span> <span class="n">N_CTX</span> <span class="o">+</span> <span class="n">off_m</span><span class="p">,</span> <span class="n">delta</span><span class="p">)</span>

</code></pre></div></div>

<p>where <code class="language-plaintext highlighter-rouge">delta = tl.sum(o * do, axis=1)</code> implements the equation \(D_i = do_i^T o_i\).</p>

<p>To calculate \(dV, dK\), a block of elements of <code class="language-plaintext highlighter-rouge">k, v</code> is first loaded (sequence parallelisation), and then carries out a loop over the length dimension of <code class="language-plaintext highlighter-rouge">q</code>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="n">start_n</span> <span class="o">=</span> <span class="n">pid</span> <span class="o">*</span> <span class="n">BLOCK_N1</span>
  <span class="n">offs_n</span> <span class="o">=</span> <span class="n">start_n</span> <span class="o">+</span> <span class="n">tl</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">BLOCK_N1</span><span class="p">)</span>
  <span class="c1"># load K and V: they stay in SRAM throughout the inner loop.
</span>  <span class="n">k</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="n">K</span> <span class="o">+</span> <span class="n">offs_n</span><span class="p">[:,</span> <span class="bp">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">stride_tok</span> <span class="o">+</span> <span class="n">offs_k</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="p">:]</span> <span class="o">*</span> <span class="n">stride_d</span><span class="p">)</span>
  <span class="n">v</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="n">V</span> <span class="o">+</span> <span class="n">offs_n</span><span class="p">[:,</span> <span class="bp">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">stride_tok</span> <span class="o">+</span> <span class="n">offs_k</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="p">:]</span> <span class="o">*</span> <span class="n">stride_d</span><span class="p">)</span>
</code></pre></div></div>

<p>For the non-causal case, it is straightforward,</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="n">start_m</span> <span class="o">=</span> <span class="mi">0</span>
  <span class="n">num_steps</span> <span class="o">=</span> <span class="p">(</span><span class="n">N_CTX</span> <span class="o">-</span> <span class="n">start_m</span><span class="p">)</span> <span class="o">//</span> <span class="n">BLOCK_M1</span>
</code></pre></div></div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0" id="figure-1">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/kq_dotprod_mat-480.webp 480w,/assets/img/kq_dotprod_mat-800.webp 800w,/assets/img/kq_dotprod_mat-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/kq_dotprod_mat.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
</div>
<div class="caption">
    Fig-1 An illustration of $ S^T = KQ^T $.
</div>

<p>For the causal case (please note that causal modelling is only used in self-attention), the procedure is split into two steps:</p>

<ol>
  <li>Calculate the non-masked blocks (yellow squares in the <a href="#figure-1">Fig-1</a>) by only changing <code>start_m = start_n + BLOCK_N1</code>.</li>
  <li>
    <p>Calculate the diagonal block (the green square in the <a href="#figure-1">Fig-1</a>) by setting</p>

    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="n">start_m</span> <span class="o">=</span> <span class="n">start_n</span>
  <span class="n">MASK_BLOCK_M1</span><span class="p">:</span> <span class="n">tl</span><span class="p">.</span><span class="n">constexpr</span> <span class="o">=</span> <span class="n">BLOCK_M1</span> <span class="o">//</span> <span class="n">BLK_SLICE_FACTOR</span>
  <span class="n">num_steps</span> <span class="o">=</span> <span class="n">BLOCK_N1</span> <span class="o">//</span> <span class="n">MASK_BLOCK_M1</span>
</code></pre></div>    </div>
  </li>
</ol>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="c1"># The main inner-loop logic for computing dK and dV.
</span>  <span class="nd">@triton.jit</span>
  <span class="k">def</span> <span class="nf">_attn_bwd_dkdv</span><span class="p">(</span><span class="n">dk</span><span class="p">,</span> <span class="n">dv</span><span class="p">,</span>  <span class="c1">#
</span>                     <span class="n">Q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">sm_scale</span><span class="p">,</span>  <span class="c1">#
</span>                     <span class="n">DO</span><span class="p">,</span>  <span class="c1">#
</span>                     <span class="n">M</span><span class="p">,</span> <span class="n">D</span><span class="p">,</span>  <span class="c1">#
</span>                     <span class="c1"># shared by Q/K/V/DO.
</span>                     <span class="n">stride_tok</span><span class="p">,</span> <span class="n">stride_d</span><span class="p">,</span>  <span class="c1">#
</span>                     <span class="n">H</span><span class="p">,</span> <span class="n">N_CTX</span><span class="p">,</span> <span class="n">BLOCK_M1</span><span class="p">:</span> <span class="n">tl</span><span class="p">.</span><span class="n">constexpr</span><span class="p">,</span>  <span class="c1">#
</span>                     <span class="n">BLOCK_N1</span><span class="p">:</span> <span class="n">tl</span><span class="p">.</span><span class="n">constexpr</span><span class="p">,</span>  <span class="c1">#
</span>                     <span class="n">HEAD_DIM</span><span class="p">:</span> <span class="n">tl</span><span class="p">.</span><span class="n">constexpr</span><span class="p">,</span>  <span class="c1">#
</span>                     <span class="c1"># Filled in by the wrapper.
</span>                     <span class="n">start_n</span><span class="p">,</span> <span class="n">start_m</span><span class="p">,</span> <span class="n">num_steps</span><span class="p">,</span>  <span class="c1">#
</span>                     <span class="n">MASK</span><span class="p">:</span> <span class="n">tl</span><span class="p">.</span><span class="n">constexpr</span><span class="p">):</span>
      <span class="n">offs_m</span> <span class="o">=</span> <span class="n">start_m</span> <span class="o">+</span> <span class="n">tl</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">BLOCK_M1</span><span class="p">)</span>
      <span class="n">offs_n</span> <span class="o">=</span> <span class="n">start_n</span> <span class="o">+</span> <span class="n">tl</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">BLOCK_N1</span><span class="p">)</span>
      <span class="n">offs_k</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">HEAD_DIM</span><span class="p">)</span>
      <span class="n">qT_ptrs</span> <span class="o">=</span> <span class="n">Q</span> <span class="o">+</span> <span class="n">offs_m</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="p">:]</span> <span class="o">*</span> <span class="n">stride_tok</span> <span class="o">+</span> <span class="n">offs_k</span><span class="p">[:,</span> <span class="bp">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">stride_d</span>
      <span class="n">do_ptrs</span> <span class="o">=</span> <span class="n">DO</span> <span class="o">+</span> <span class="n">offs_m</span><span class="p">[:,</span> <span class="bp">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">stride_tok</span> <span class="o">+</span> <span class="n">offs_k</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="p">:]</span> <span class="o">*</span> <span class="n">stride_d</span>
      <span class="c1"># BLOCK_N1 must be a multiple of BLOCK_M1, otherwise the code wouldn't work.
</span>      <span class="n">tl</span><span class="p">.</span><span class="nf">static_assert</span><span class="p">(</span><span class="n">BLOCK_N1</span> <span class="o">%</span> <span class="n">BLOCK_M1</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span>
      <span class="n">curr_m</span> <span class="o">=</span> <span class="n">start_m</span>
      <span class="n">step_m</span> <span class="o">=</span> <span class="n">BLOCK_M1</span>
      <span class="k">for</span> <span class="n">blk_idx</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_steps</span><span class="p">):</span>
          <span class="n">qT</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="n">qT_ptrs</span><span class="p">)</span>
          <span class="c1"># Load m before computing qk to reduce pipeline stall.
</span>          <span class="n">offs_m</span> <span class="o">=</span> <span class="n">curr_m</span> <span class="o">+</span> <span class="n">tl</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">BLOCK_M1</span><span class="p">)</span>
          <span class="n">m</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="n">M</span> <span class="o">+</span> <span class="n">offs_m</span><span class="p">)</span>
          <span class="n">sT</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">qT</span><span class="p">)</span>
          <span class="n">pT</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="n">math</span><span class="p">.</span><span class="nf">exp2</span><span class="p">(</span><span class="n">sT</span> <span class="o">-</span> <span class="n">m</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="p">:])</span>
          <span class="c1"># Autoregressive masking.
</span>          <span class="k">if</span> <span class="n">MASK</span><span class="p">:</span>
              <span class="n">mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">offs_m</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="p">:]</span> <span class="o">&gt;=</span> <span class="n">offs_n</span><span class="p">[:,</span> <span class="bp">None</span><span class="p">])</span>
              <span class="n">pT</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">where</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="n">pT</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span>
          <span class="n">do</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="n">do_ptrs</span><span class="p">)</span>
          <span class="c1"># Compute dV.
</span>          <span class="n">ppT</span> <span class="o">=</span> <span class="n">pT</span>
          <span class="n">ppT</span> <span class="o">=</span> <span class="n">ppT</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">tl</span><span class="p">.</span><span class="n">float16</span><span class="p">)</span>
          <span class="n">dv</span> <span class="o">+=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">ppT</span><span class="p">,</span> <span class="n">do</span><span class="p">)</span>
          <span class="c1"># D (= delta) is pre-divided by ds_scale.
</span>          <span class="n">Di</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="n">D</span> <span class="o">+</span> <span class="n">offs_m</span><span class="p">)</span>
          <span class="c1"># Compute dP and dS.
</span>          <span class="n">dpT</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">tl</span><span class="p">.</span><span class="nf">trans</span><span class="p">(</span><span class="n">do</span><span class="p">)).</span><span class="nf">to</span><span class="p">(</span><span class="n">tl</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
          <span class="n">dsT</span> <span class="o">=</span> <span class="n">pT</span> <span class="o">*</span> <span class="p">(</span><span class="n">dpT</span> <span class="o">-</span> <span class="n">Di</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="p">:])</span>
          <span class="n">dsT</span> <span class="o">=</span> <span class="n">dsT</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">tl</span><span class="p">.</span><span class="n">float16</span><span class="p">)</span>
          <span class="n">dk</span> <span class="o">+=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">dsT</span><span class="p">,</span> <span class="n">tl</span><span class="p">.</span><span class="nf">trans</span><span class="p">(</span><span class="n">qT</span><span class="p">))</span>
          <span class="c1"># Increment pointers.
</span>          <span class="n">curr_m</span> <span class="o">+=</span> <span class="n">step_m</span>
          <span class="n">qT_ptrs</span> <span class="o">+=</span> <span class="n">step_m</span> <span class="o">*</span> <span class="n">stride_tok</span>
          <span class="n">do_ptrs</span> <span class="o">+=</span> <span class="n">step_m</span> <span class="o">*</span> <span class="n">stride_tok</span>
      <span class="k">return</span> <span class="n">dk</span><span class="p">,</span> <span class="n">dv</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="n">qT</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="n">qT_ptrs</span><span class="p">)</span>
  <span class="c1"># Load m before computing qk to reduce pipeline stall.
</span>  <span class="n">offs_m</span> <span class="o">=</span> <span class="n">curr_m</span> <span class="o">+</span> <span class="n">tl</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">BLOCK_M1</span><span class="p">)</span>
  <span class="n">m</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="n">M</span> <span class="o">+</span> <span class="n">offs_m</span><span class="p">)</span>
  <span class="n">sT</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">qT</span><span class="p">)</span>
  <span class="n">pT</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="n">math</span><span class="p">.</span><span class="nf">exp2</span><span class="p">(</span><span class="n">sT</span> <span class="o">-</span> <span class="n">m</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="p">:])</span>
</code></pre></div></div>

<p>This part of code recomputes \(S = QK^T\) and \(P = \operatorname{softmax}(S)\) (actually its transposed version, and therefore it needs to pay attention to the broadcast rule <code class="language-plaintext highlighter-rouge">m[None, :]</code>. <code class="language-plaintext highlighter-rouge">m</code> is stored in the forward pass for calculating softmax in a numerical stable manner.).</p>

<p><code class="language-plaintext highlighter-rouge">dv += tl.dot(ppT, do)</code> implements the equation \(dV = P^T dO\). As the calculation \(dv_j = \sum_i P_{ij} do_i\), where \(dv_j, do_i\) denote the j-th and i-th row of \(V, O\) respectively, is chunked into multiple blocks, so do not forget the accumulation sum.</p>

<p><code class="language-plaintext highlighter-rouge">dpT = tl.dot(v, tl.trans(do)).to(tl.float32)</code> implements the equation \(dP = dO V^T\) (its transposed version).</p>

<p><code class="language-plaintext highlighter-rouge">dsT = pT * (dpT - Di[None, :])</code> implements the equation \(dS = \operatorname{dsoftmax}(dP) \in \mathbb{R}^{N\times N}\), which is further simplified to</p>

\[dS_{i,:} = \operatorname{dsoftmax}dP_{i,:} = (\text{diag}(P_{i,:}) - P_{i,:}P_{i,:}^T)dP_{i,:} = P_{i,:} \circ dP_{i,:} - \left( P_{i,:}^T dP_{i,:} \right) P_{i,:} = P_{i,:} \circ dP_{i,:} - D_i P_{i,:}\]

<p>as discussed above (its transposed version).</p>

<p><code class="language-plaintext highlighter-rouge">dk += tl.dot(dsT, tl.trans(qT))</code> implements the equation \(dK = dS^T Q\).</p>

<p>\(dQ\) is calculated similarly: a block of elements of <code class="language-plaintext highlighter-rouge">q</code> is first loaded (sequence parallelisation), and then carries out a loop over the length dimension of <code class="language-plaintext highlighter-rouge">k, v</code>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="n">start_m</span> <span class="o">=</span> <span class="n">pid</span> <span class="o">*</span> <span class="n">BLOCK_M2</span>
  <span class="n">offs_m</span> <span class="o">=</span> <span class="n">start_m</span> <span class="o">+</span> <span class="n">tl</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">BLOCK_M2</span><span class="p">)</span>
  <span class="c1"># load q, do, m and Di: they stay in SRAM throughout the inner loop.
</span>  <span class="n">q</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="n">Q</span> <span class="o">+</span> <span class="n">offs_m</span><span class="p">[:,</span> <span class="bp">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">stride_tok</span> <span class="o">+</span> <span class="n">offs_k</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="p">:]</span> <span class="o">*</span> <span class="n">stride_d</span><span class="p">)</span>
  <span class="n">do</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="n">DO</span> <span class="o">+</span> <span class="n">offs_m</span><span class="p">[:,</span> <span class="bp">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">stride_tok</span> <span class="o">+</span> <span class="n">offs_k</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="p">:]</span> <span class="o">*</span> <span class="n">stride_d</span><span class="p">)</span>

  <span class="n">m</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="n">M</span> <span class="o">+</span> <span class="n">offs_m</span><span class="p">)</span>
  <span class="n">m</span> <span class="o">=</span> <span class="n">m</span><span class="p">[:,</span> <span class="bp">None</span><span class="p">]</span>

  <span class="n">Di</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="n">D</span> <span class="o">+</span> <span class="n">offs_m</span><span class="p">)</span>
  <span class="n">Di</span> <span class="o">=</span> <span class="n">Di</span><span class="p">[:,</span> <span class="bp">None</span><span class="p">]</span>
</code></pre></div></div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0" id="figure-2">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/qk_dotprod_mat-480.webp 480w,/assets/img/qk_dotprod_mat-800.webp 800w,/assets/img/qk_dotprod_mat-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/qk_dotprod_mat.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
</div>
<div class="caption">
    Fig-2 An illustration of $ S = QK^T $.
</div>

<p>For the causal case, the procedure is split into two steps:</p>

<ol>
  <li>Calculate the non-masked blocks (yellow squares in the <a href="#figure-2">Fig-2</a>) by setting <code>end_n = start_m</code> <code>num_steps = end_n // BLOCK_N2</code> . So in the inner loop over <code>k, v</code>, the start and end indexes are <code>0</code> and <code>start_m</code>, respectively.</li>
  <li>
    <p>Calculate the diagonal block (the green square in the <a href="#figure-2">Fig-2</a>) by setting</p>

    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="n">MASK_BLOCK_N2</span><span class="p">:</span> <span class="n">tl</span><span class="p">.</span><span class="n">constexpr</span> <span class="o">=</span> <span class="n">BLOCK_N2</span> <span class="o">//</span> <span class="n">BLK_SLICE_FACTOR</span>
  <span class="n">num_steps</span> <span class="o">=</span> <span class="n">BLOCK_M2</span> <span class="o">//</span> <span class="n">MASK_BLOCK_N2</span>
</code></pre></div>    </div>

    <p>And the start and end indexes are <code>start_m</code> and <code>start_m + BLOCK_M2</code> respectively.</p>
  </li>
</ol>

<p>For the non-causal case, in the inner loop over <code class="language-plaintext highlighter-rouge">k, v</code>, the start and end indexes are simply <code class="language-plaintext highlighter-rouge">0</code> and <code class="language-plaintext highlighter-rouge">N_CTX</code>, respectively. However, in my implementation, it is also split into two steps: 1) from <code class="language-plaintext highlighter-rouge">0</code> to <code class="language-plaintext highlighter-rouge">start_m</code>, and 2) from <code class="language-plaintext highlighter-rouge">start_m</code> to <code class="language-plaintext highlighter-rouge">N_CTX</code>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="nd">@triton.jit</span>
  <span class="k">def</span> <span class="nf">_attn_bwd_dq</span><span class="p">(</span><span class="n">dq</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span>  <span class="c1">#
</span>                   <span class="n">do</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">Di</span><span class="p">,</span>
                   <span class="c1"># shared by Q/K/V/DO.
</span>                   <span class="n">stride_tok</span><span class="p">,</span> <span class="n">stride_d</span><span class="p">,</span>  <span class="c1">#
</span>                   <span class="n">H</span><span class="p">,</span> <span class="n">N_CTX</span><span class="p">,</span>  <span class="c1">#
</span>                   <span class="n">BLOCK_M2</span><span class="p">:</span> <span class="n">tl</span><span class="p">.</span><span class="n">constexpr</span><span class="p">,</span>  <span class="c1">#
</span>                   <span class="n">BLOCK_N2</span><span class="p">:</span> <span class="n">tl</span><span class="p">.</span><span class="n">constexpr</span><span class="p">,</span>  <span class="c1">#
</span>                   <span class="n">HEAD_DIM</span><span class="p">:</span> <span class="n">tl</span><span class="p">.</span><span class="n">constexpr</span><span class="p">,</span>  <span class="c1">#
</span>                   <span class="c1"># Filled in by the wrapper.
</span>                   <span class="n">start_m</span><span class="p">,</span> <span class="n">start_n</span><span class="p">,</span> <span class="n">num_steps</span><span class="p">,</span>  <span class="c1">#
</span>                   <span class="n">MASK</span><span class="p">:</span> <span class="n">tl</span><span class="p">.</span><span class="n">constexpr</span><span class="p">):</span>
      <span class="n">offs_m</span> <span class="o">=</span> <span class="n">start_m</span> <span class="o">+</span> <span class="n">tl</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">BLOCK_M2</span><span class="p">)</span>
      <span class="n">offs_n</span> <span class="o">=</span> <span class="n">start_n</span> <span class="o">+</span> <span class="n">tl</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">BLOCK_N2</span><span class="p">)</span>
      <span class="n">offs_k</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">HEAD_DIM</span><span class="p">)</span>
      <span class="n">kT_ptrs</span> <span class="o">=</span> <span class="n">K</span> <span class="o">+</span> <span class="n">offs_n</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="p">:]</span> <span class="o">*</span> <span class="n">stride_tok</span> <span class="o">+</span> <span class="n">offs_k</span><span class="p">[:,</span> <span class="bp">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">stride_d</span>
      <span class="n">vT_ptrs</span> <span class="o">=</span> <span class="n">V</span> <span class="o">+</span> <span class="n">offs_n</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="p">:]</span> <span class="o">*</span> <span class="n">stride_tok</span> <span class="o">+</span> <span class="n">offs_k</span><span class="p">[:,</span> <span class="bp">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">stride_d</span>
      <span class="c1"># BLOCK_M2 must be a multiple of BLOCK_N2, otherwise the code wouldn't work.
</span>      <span class="n">tl</span><span class="p">.</span><span class="nf">static_assert</span><span class="p">(</span><span class="n">BLOCK_M2</span> <span class="o">%</span> <span class="n">BLOCK_N2</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span>
      <span class="n">curr_n</span> <span class="o">=</span> <span class="n">start_n</span>
      <span class="n">step_n</span> <span class="o">=</span> <span class="n">BLOCK_N2</span>
      <span class="k">for</span> <span class="n">blk_idx</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_steps</span><span class="p">):</span>
          <span class="n">kT</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="n">kT_ptrs</span><span class="p">)</span>
          <span class="n">vT</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="n">vT_ptrs</span><span class="p">)</span>
          <span class="n">s</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">kT</span><span class="p">)</span>
          <span class="n">p</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="n">math</span><span class="p">.</span><span class="nf">exp2</span><span class="p">(</span><span class="n">s</span> <span class="o">-</span> <span class="n">m</span><span class="p">)</span>
          <span class="c1"># Autoregressive masking.
</span>          <span class="k">if</span> <span class="n">MASK</span><span class="p">:</span>
              <span class="n">offs_n</span> <span class="o">=</span> <span class="n">curr_n</span> <span class="o">+</span> <span class="n">tl</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">BLOCK_N2</span><span class="p">)</span>
              <span class="n">mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">offs_m</span><span class="p">[:,</span> <span class="bp">None</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="n">offs_n</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="p">:])</span>
              <span class="n">p</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">where</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span>
          <span class="c1"># Compute dP and dS.
</span>          <span class="n">dp</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">do</span><span class="p">,</span> <span class="n">vT</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">tl</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
          <span class="n">ds</span> <span class="o">=</span> <span class="n">p</span> <span class="o">*</span> <span class="p">(</span><span class="n">dp</span> <span class="o">-</span> <span class="n">Di</span><span class="p">)</span>
          <span class="n">ds</span> <span class="o">=</span> <span class="n">ds</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">tl</span><span class="p">.</span><span class="n">float16</span><span class="p">)</span>
          <span class="c1"># Compute dQ.
</span>          <span class="c1"># NOTE: We need to de-scale dq in the end, because kT was pre-scaled.
</span>          <span class="n">dq</span> <span class="o">+=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">ds</span><span class="p">,</span> <span class="n">tl</span><span class="p">.</span><span class="nf">trans</span><span class="p">(</span><span class="n">kT</span><span class="p">))</span>
          <span class="c1"># Increment pointers.
</span>          <span class="n">curr_n</span> <span class="o">+=</span> <span class="n">step_n</span>
          <span class="n">kT_ptrs</span> <span class="o">+=</span> <span class="n">step_n</span> <span class="o">*</span> <span class="n">stride_tok</span>
          <span class="n">vT_ptrs</span> <span class="o">+=</span> <span class="n">step_n</span> <span class="o">*</span> <span class="n">stride_tok</span>
      <span class="k">return</span> <span class="n">dq</span>

</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="n">kT</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="n">kT_ptrs</span><span class="p">)</span>
  <span class="n">vT</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="n">vT_ptrs</span><span class="p">)</span>
  <span class="n">s</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">kT</span><span class="p">)</span>
  <span class="n">p</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="n">math</span><span class="p">.</span><span class="nf">exp2</span><span class="p">(</span><span class="n">s</span> <span class="o">-</span> <span class="n">m</span><span class="p">)</span>
</code></pre></div></div>

<p>This part of code recomputes \(S = QK^T\) and \(P = \operatorname{softmax}(S)\).</p>

<p><code class="language-plaintext highlighter-rouge">dp = tl.dot(do, vT).to(tl.float32)</code> implements the equation \(dP = dO V^T\).</p>

<p><code class="language-plaintext highlighter-rouge">ds = p * (dp - Di)</code> implements the equation \(dS_{i,:} = P_{i,:} \circ dP_{i,:} - D_i P_{i,:}\).</p>

<p><code class="language-plaintext highlighter-rouge">dq += tl.dot(ds, tl.trans(kT))</code> implements the equation \(dQ = dS K\).</p>

<h2 id="concluding-remarks">Concluding Remarks</h2>

<p>Voila! We have walked through the core implementation of the backward pass of FlashAttention, where the Triton code shares a high similarity with matrix calculus equations. You can check out the Github repo <a href="https://github.com/TotalVariation/Learning-to-Learn-DL/tree/main/flash_attention">Learning-to-Learn-DL</a> containing an IPython notebook which is supposed to offer a more enhanced interactive experience and another notebook where a more flexible implementation of FlashAttention2 is given, which can handle both self-attention and cross-attention with arbitrary lengths. However, for practical usage, I recommend using the official <a href="https://github.com/Dao-AILab/flash-attention">FlashAttention Repo</a> written in CUDA. Furthermore, I believe this post will facilitate your understanding of the Triton implementation given in the official <a href="https://github.com/Dao-AILab/flash-attention">FlashAttention Repo</a>.</p>

<div class="language-html highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@misc{Cai2025flashattnbackward-2,
    author = {Xin Cai},
    title = {Learning the Backward Pass of FlashAttention: Part II Implementation in Triton},
    howpublished = {\url{https://totalvariation.github.io/blog/2025/intro-flashattention-backward-part2/},
    note = {Accessed: 2025-07-21},
    year = {2025}
}
</code></pre></div></div>]]></content><author><name>Xin Cai</name></author><summary type="html"><![CDATA[Part II Implementation in Triton]]></summary></entry><entry><title type="html">Michael Oher’s Story at Scale Why Industry-Led Academies May Be the Future</title><link href="https://totalvariation.github.io/blog/2025/rethink-education/" rel="alternate" type="text/html" title="Michael Oher’s Story at Scale Why Industry-Led Academies May Be the Future" /><published>2025-06-19T00:00:00+00:00</published><updated>2025-06-19T00:00:00+00:00</updated><id>https://totalvariation.github.io/blog/2025/rethink-education</id><content type="html" xml:base="https://totalvariation.github.io/blog/2025/rethink-education/"><![CDATA[<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/Quotefancy-2016666-3840x2160-480.webp 480w,/assets/img/Quotefancy-2016666-3840x2160-800.webp 800w,/assets/img/Quotefancy-2016666-3840x2160-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/Quotefancy-2016666-3840x2160.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
</div>
<div class="caption">
    Photo by <a href="https://quotefancy.com/quote/757737/Sydney-J-Harris-The-whole-purpose-of-education-is-to-turn-mirrors-into-windows">quotefancy</a>
</div>

<div class="jekyll-twitter-plugin"><blockquote class="twitter-tweet"><p lang="en" dir="ltr">As we progress along the Kardashev Scale, energy harnessed on Earth will increase a hundredfold and will mostly be solar aka fusion aka starlight. <br /><br />Then energy harnessed will increase perhaps a billionfold if we make it to Kardashev II, with space solar power, and another… <a href="https://t.co/0cHovopB9l">https://t.co/0cHovopB9l</a></p>&mdash; Elon Musk (@elonmusk) <a href="https://twitter.com/elonmusk/status/1921848768208453887?ref_src=twsrc%5Etfw">May 12, 2025</a></blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

</div>

<p>I stumbled across an X post from Elon Musk, envisioning the future of humans entering into Kardashev Type II civilization, which prompts me to rethink about today’s education system. Achieving technological feats like reaching a Kardashev Type II civilization would not have been possible without unlocking the potential of at least 80% individuals on this planet—a factor too often overlooked in human history, where the focus has largely been on exploiting various energy sources. Would we have reached today’s technological heights without the Renaissance, revealing what humanity is capable of when we recognize human potential and value?</p>

<p>This mission could hardly be fulfilled under today’s brittle education system. Industries already possess the resources—and, more importantly, a concentration of brilliant minds—to build their own academies and train 18-year-olds with high school diplomas through highly optimized curricula, bypassing traditional universities entirely. Would not it be a genuine waste if those brilliant minds played no role in nurturing the next generation? Imagine tech giants like Google or Tesla running academies tailored to their ambitions, taught by their top talent. Universities, after all, have long started to resemble businesses. Yet the privileges they enjoy often seem to adversely affect their ability to cultivate young minds. (It is not universal, but access to those remaining as exceptions is limited.) Self-oversight leads to complacency. What if universities operated like S&amp;P 500 companies—would it be a burden, or would it push them to become truly effective educational institutions? A profit-driven mindset might actually drive education providers to offer peak learning experiences to their customers. Aren’t those unprofitable but scientifically or socially fundamental programs the sources of competitive edge and the lasting value of traditional universities?</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/Hg30RBeTJ2nBvcd1ctWp0g-480.webp 480w,/assets/img/Hg30RBeTJ2nBvcd1ctWp0g-800.webp 800w,/assets/img/Hg30RBeTJ2nBvcd1ctWp0g-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/Hg30RBeTJ2nBvcd1ctWp0g.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
</div>
<div class="caption">
    Photo by <a href="https://medium.com/@marknutter/the-matrix-plot-that-should-have-been-54bcddc60e2b">The Matrix Plot That Should Have Been</a>
</div>

<p>In The Matrix, a chilling image shows humans encased as batteries to power AI. A more scientifically plausible scenario (temporarily omitting ethical concerns) is one where human minds become distributed simulating environments for AI’s continual learning. Once text and multimodal data are exhausted, the next phase of AI development may depend on mining human thinking traces—an inexhaustible source of training data, assuming human diversity and creativity survive oppressive forces. Until breakthroughs in brain–computer interface technologies allowing effective conversion of biochemical signals in human brains into tokens, these thinking traces could be harvested through human–LLM interactions. A centrally trained LLM serves as a baseline with reasonable cognitive capacity for meaningful human-AI interactions, but its future potential hinges on decentralized learning powered by billions of end users, each acting as a unique, parallel simulation environment.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/0b5fde96-c426-442e-b595-9cc954954314-480.webp 480w,/assets/img/0b5fde96-c426-442e-b595-9cc954954314-800.webp 800w,/assets/img/0b5fde96-c426-442e-b595-9cc954954314-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/0b5fde96-c426-442e-b595-9cc954954314.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
</div>
<div class="caption">
    Photo by <a href="https://casdbeavertales.org/28879/arts-entertainment/the-blind-side-an-inspiring-story-about-helping-others/">BEAVER TALES</a>
</div>

<p>To me, The Blind Side captures the true purpose of education: to uplift and empower individuals. Yet today’s educational institutions have gone astray, sinking into a swamp of reward hacking—where funding takes precedence over genuine learning. In contrast, high-tech companies dedicated to education services, combining AI tutors and human tutors and no longer hiding profit motives behind a glittering facade, hold the potential to revolutionize the system, unlocking each individual’s potential with personalized learning experiences at scale. Platforms like Coursera and edX have begun dipping toes, but so far only as supplements to traditional paths. Transformative stories like Michael Oher’s cannot be replicated by the current system—but with personalized AI tutors, they might have become scalable. More importantly, industry-led academies should emerge as a true parallel system, not just an auxiliary option.</p>

<h2 id="acknowledgement">Acknowledgement</h2>

<p>The completion of this blog post was assisted by OpenAI ChatGPT in refining phrasing, expression, and recommending titles and sayings.</p>

<div class="language-html highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@misc{Cai2025rethinked,
    author = {Xin Cai},
    title = {Michael Oher's Story at Scale: Why Industry-Led Academies May Be the Future},
    howpublished = {\url{https://totalvariation.github.io/blog/2025/rethink-education/}},
    note = {Accessed: 2025-06-19},
    year = {2025}
}
</code></pre></div></div>]]></content><author><name>Xin Cai</name></author><summary type="html"><![CDATA[A Perspective by Someone Who Couldn't Hack Academia]]></summary></entry><entry><title type="html">A Primer on Multimodal Large Language Models</title><link href="https://totalvariation.github.io/blog/2024/a-primer-on-mllms/" rel="alternate" type="text/html" title="A Primer on Multimodal Large Language Models" /><published>2024-05-20T00:00:00+00:00</published><updated>2024-05-20T00:00:00+00:00</updated><id>https://totalvariation.github.io/blog/2024/a-primer-on-mllms</id><content type="html" xml:base="https://totalvariation.github.io/blog/2024/a-primer-on-mllms/"><![CDATA[<blockquote>
    <p><b>The best material model of a cat is another, or preferably the same, cat.</b></p>
    <footer>- Norbert Wiener</footer>
</blockquote>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0" id="figure-1">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/mllm_primer_post_robot_cat_sdxl-480.webp 480w,/assets/img/mllm_primer_post_robot_cat_sdxl-800.webp 800w,/assets/img/mllm_primer_post_robot_cat_sdxl-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/mllm_primer_post_robot_cat_sdxl.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
</div>
<div class="caption">
    Fig-1 A sketch of general purpose AI assistants. Image Source: <a href="https://huggingface.co/docs/diffusers/en/using-diffusers/sdxl" target="_blank">Stable Diffusion XL</a>.
</div>

<h2 id="background">Background</h2>

<p>Large Language Models (LLMs) <d-cite key="zhao2023survey"></d-cite> have achieved remarkable milestones, especially the demonstrated emergent capabilities, e.g., In-Context Learning (ICL), Chain of Thought (CoT) reasoning, and instruction following. It is now an inevitable trend to integrate various modalities with LLMs to mimic the way humans interact with the open world, thus ushering in the new era of Multimodal Large Language Models (MLLMs <d-footnote>Please note that the term MLLMs hereafter is used interchangeably with Large Vision-Language Models (LVLMs) or Large Multimodal Models (LMMs) given that the transitive property of modality embeddings showcased in ImageBind. <d-cite key="girdhar2023imagebind"></d-cite> </d-footnote>) <d-cite key="yin2023survey"></d-cite>.</p>

<p>Recent work <d-cite key="li2023multimodal"></d-cite> has demonstrated that the development of MLLMs signifies a transition from specialist models to general-purpose visual assistants. This transition is regarded as a crucial step towards realizing the grand vision of a general-purpose multimodal AI agent, akin to bringing the beloved Japanese cartoon character Doraemon to life. It is further unveiled in <d-cite key="li2023multimodal"></d-cite> that a converging point for many development trajectories of vision-language/multimodal foundation models is <em>“the creation of general-purpose models and systems capable of following human intents and effortlessly executing a diverse array of vision and vision-language tasks in the wild.”</em></p>

<p>The key features that markedly differentiate MLLMs from deep learning models developed beforehand are condensed into the name itself: <mark>General Purpose Assistants</mark>, which include <strong>versatility</strong>, <strong>interactivity</strong>, and <strong>controllability</strong>. Specifically, the quest for a unified architecture capable of accomplishing a diverse range of discriminative and generative tasks has witnessed great success in Natural Language Processing (NLP), as exemplified by GPT-3, inspiring similar research endeavours in the field of computer vision. However, as pointed out in <d-cite key="li2023multimodal"></d-cite>, the development of unified vision systems significantly lags behind due to fragmented vision tasks and the difficulty of scaling up visual data. Furthermore, to enable seamless communication between machines and humans, natural language serves as a core component in building a general interactive interface, which can be further complemented by multimodal prompts such as clicks or scribbles provided by users. Additionally, researchers have made strides in steering the output behaviour of MLLMs by instruction tuning or aligning with human preferences, marking a hallmark in elevating machine intelligence to the next level as humans begin to positively intervene in the machine learning process. By leveraging human feedback and rich experience, machines can evolve more effectively and efficiently, leading to better outcomes for both machines and humans. Inspired by the interactions between Doraemon and Nobita, the ultimate goal of general-purpose AI assistants is to establish a <strong>symbiotic relationship</strong> between humans and AI systems, where AI systems not only enhance the overall well-being and quality of human life but also evolve by engaging in humans’ daily activities.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0" id="figure-2">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/mllm_primer_post_genaisys-480.webp 480w,/assets/img/mllm_primer_post_genaisys-800.webp 800w,/assets/img/mllm_primer_post_genaisys-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/mllm_primer_post_genaisys.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
</div>
<div class="caption">
    Fig-2 The general architecture of GenAISys. Image Source: <a href="https://jmtomczak.github.io/blog/21/21_genaisys.html" target="_blank">Jakub Tomczak</a>.
</div>

<p>Before delving into the technical aspects of MLLMs, I would like to introduce an intriguing and inspiring concept called Generative AI Systems (GenAISys), as shown in <a href="#figure-2">Fig-2</a>, recently introduced in a <a href="https://jmtomczak.github.io/blog/21/21_genaisys.html">blog post</a> by Jakub Tomczak, which offers a novel perspective on MLLMs. In essence, GenAISys can be considered as end-to-end trainable MLLMs enhanced by external tools or databases, resonating with a similar idea proposed in Chapter 6 of the work <d-cite key="li2023multimodal"></d-cite>, where the authors indicate the possibility of combining strengths from two existing modelling paradigms of MLLMs: training or chaining tools with LLMs. For more insights into GenAISys and its future directions, I encourage you to refer to the original post by Jakub Tomczak.</p>

<h2 id="technical-overview">Technical Overview</h2>

<p>The recent progress of MLLMs <d-footnote>Technically, the progress reviewed here excludes that related to Multimodal Agents, i.e., chaining tools with LLMs.</d-footnote> can be roughly divided into the following three aspects: 1) architectures, 2) training strategies and data, 3) evaluation benchmarks.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0" id="figure-3">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/mllm_primer_post_mllm_arch-480.webp 480w,/assets/img/mllm_primer_post_mllm_arch-800.webp 800w,/assets/img/mllm_primer_post_mllm_arch-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/mllm_primer_post_mllm_arch.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
</div>
<div class="caption">
    Fig-3 An overview of MLLM architecture.
</div>

<h3 id="architectures">Architectures</h3>

<p>A typical neural architecture of MLLMs, as illustrated in <a href="#figure-3">Fig-3</a>, consists of pre-trained modality encoders, pre-trained LLMs, learnable modality adapters/connectors, and optional modality decoders.</p>

<p><strong>Modality Encoders</strong> When compressing raw images, it is common practice to utilize pre-trained CLIP <d-cite key="radford2021learning"></d-cite> image encoders or other similar variants <d-cite key="sun2023eva"></d-cite>, leveraging a pre-aligned albeit coarse vision-semantic space. The underlying principle is to discretize/standardize modality embeddings with reference to language vocabulary, which can be extended to encoding other kinds of multimodal signals. For instance, the ImageBind <d-cite key="girdhar2023imagebind"></d-cite> encoder, used in ImageBind-LLM <d-cite key="han2023imagebind"></d-cite>, can construct a joint embedding space for six different modalities—images, text, audio, depth, thermal, and Inertial Measurement Unit (IMU) data—established by aligning a pair of modalities each time with InfoNCE <d-cite key="oord2018representation"></d-cite> loss.</p>

<p>Another critical factor in enriching visual embeddings in MLLMs and mitigating multimodal hallucination is increasing input resolution <d-cite key="liu2023improved"></d-cite> while not incurring significant computational overheads. Following this principle, dual vision encoders are employed in Mini-Gemini <d-cite key="li2024mini"></d-cite>, where the coarse embeddings from the pre-trained CLIP image encoder are complemented by dense visual features output from the LAION-pretrained <d-cite key="schuhmann2022laion"></d-cite> ConvNeXt <d-cite key="liu2022convnet"></d-cite>, which serves as the high-resolution vision encoder.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0" id="figure-4">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/mllm_primer_post_adc-480.webp 480w,/assets/img/mllm_primer_post_adc-800.webp 800w,/assets/img/mllm_primer_post_adc-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/mllm_primer_post_adc.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
</div>
<div class="caption">
    Fig-4 An illustration of workings of Analog-to-Digital Converters (ADC). Image Source: <a href="https://www.doeeet.com/content/eee-components/actives/analog-to-digital-converters-the-resolution-dilemma/" target="_blank">DOEEET</a>.
</div>

<p>Finally, I would like to highlight an intriguing perspective by drawing a parallel between the modality encoder in MLLMs and the Analog-to-Digital Converter (ADC), as shown in <a href="#figure-4">Fig-4</a>, which converts continuous analog signals into discrete digital signals through sampling and quantization. By establishing such a rough equivalence, we can gain deeper insights into the design rationale of the modality encoder. Specifically, bandwidth and Signal-to-Noise Ratio (SNR) play a crucial role in determining the overall performance and quality of an ADC. Analogously, the range of frequencies in input signals that can be captured by the modality encoder contributes to the overall performance of MLLMs, justifying the utilization of dual vision encoders, e.g., in Mini-Gemini <d-cite key="li2024mini"></d-cite>. As demonstrated in <d-cite key="park2022vision"></d-cite>, multi-head self-attention (MSAs) in Vision Transformers (ViT) <d-cite key="dosovitskiy2020image"></d-cite> function as a low-pass filter, while convolution operations act oppositely as a high-pass filter. Therefore, the convolutional branch supplements the ViT with high-frequency information, expanding the bandwidth of the modality encoder. Furthermore, image-text contrastive pre-training can be seen as a quantization process, where discrete codes (i.e., vocabulary indices) are allocated to encoded visual signals by filtering out noise and low-level information while maximally preserving mutual information. By analogy with a higher SNR leading to finer resolution of an ADC, the ability to discern the smallest semantic variation of the modality encoder can be improved by adopting fine-grained contrastive loss <d-cite key="li2022grounded"></d-cite> or detailed captioning <d-cite key="yu2022coca"></d-cite>.</p>

<p><strong>LLMs</strong> Regarding pre-trained LLMs, open-source models such as the LLaMA series <d-cite key="touvron2023llama"></d-cite> <d-cite key="touvron2023llama2"></d-cite> and Vicuna family <d-cite key="chiang2023vicuna"></d-cite> have gained widespread popularity in academic research. These models are often combined with Parameter-Efficient Fine-Tuning (PEFT) techniques <d-cite key="han2024parameter"></d-cite>, such as Low-Rank Adaptation (LoRA) <d-cite key="hu2021lora"></d-cite>, in the Supervised Fine-Tuning (SFT) phase to enhance instruction following and alignment with human preference. Furthermore, LLMs constructed with a Mixture of Experts (MoE) <d-cite key="shen2023mixture"></d-cite> <d-cite key="jiang2024mixtral"></d-cite> have attracted increasing attention due to the reduced computational cost of the sparse architecture.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0" id="figure-5">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/mllm_primer_post_microcontroller-480.webp 480w,/assets/img/mllm_primer_post_microcontroller-800.webp 800w,/assets/img/mllm_primer_post_microcontroller-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/mllm_primer_post_microcontroller.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
</div>
<div class="caption">
    Fig-5 A schematic diagram of microcontrollers, where the central unit CPU interacts with various components to process data, control and communicate with other devices. Image Source: <a href="https://www.hnhcart.com/blogs/microcontrollers/what-are-the-major-factors-on-which-microcontrollers-are-differentiated" target="_blank">The Major Factors On Which The Microcontrollers Are Differentiated</a>.
</div>

<p>Compared to human-like text generation, the emergent capabilities of LLMs, such as ICL and CoT reasoning, unlock the vast potential of machines in handling complex and flexible human instructions. When LLMs are extended with interfaces to sense and interact with the physical world, i.e., MLLMs, they can be analogized to microcontrollers, as shown in <a href="#figure-5">Fig-5</a>,  provided disregarding differences in energy consumption and application scenarios. A microcontroller is essentially a small computer, comprising a processor core, memory, and input/output (I/O) peripherals. Similarly, LLMs act as the processor core, executing human instructions rather than pre-programmed commands. Modality encoders and decoders enable LLMs to interact with the external world, analogous to ADCs or Digital-to-Analog Converters (DACs). The key distinction between microcontrollers and MLLMs lies in the latter’s capability for learning and adaptation, ushering in a new era where machines can continuously evolve through direct interaction with humans. Human language is distinguished by its complexity, generativity, and ability to convey abstract and displaced concepts, which supports the language-centric design pattern of MLLMs. However, this design is yet to be definitively validated, as discussed in Chapter 4 of <d-cite key="li2023multimodal"></d-cite>, particularly considering the distinct differences between vision and language.</p>

<p><strong>Modality Connectors</strong> Due to the noticeable gap between embeddings of natural languages and other modalities, particularly considering the difference in bandwidth, it is essential to establish reliable alignment with reference to textual embeddings to enable LLMs to understand sensory inputs. This alignment involves projecting embeddings from modality encoders into a space comprehensible to LLMs, ensuring unambiguous interpretation. This can be achieved using a trainable modality adapter or connector, bridging the frozen modality encoder and the LLM, which is computationally efficient and more accessible.</p>

<p>Currently, multimodal alignment can be achieved at either the token- or feature-level. At the token-level, modality-specific embeddings are converted into corresponding tokens, which are then concatenated with textual tokens and fed into LLMs for SFT. The modules commonly used for modality conversion include Q-Former <d-cite key="li2023blip"></d-cite> <d-cite key="zhang2023video"></d-cite>, MLP <d-cite key="liu2024visual"></d-cite> <d-cite key="liu2023improved"></d-cite>, and cross-attention layers <d-cite key="alayrac2022flamingo"></d-cite>. In contrast to lightweight modality adapters, a pre-trained LLaMA is utilized as the modality connector in InternVL <d-cite key="chen2023internvl"></d-cite>, acting as a language middleware. This approach may allow for the utilization of frozen LLMs in the SFT stage. Feature-level fusion involves deep interaction between modality-specific and language features, usually achieved by inserting extra trainable modules into a frozen LLM <d-cite key="alayrac2022flamingo"></d-cite> <d-cite key="zhang2023llama"></d-cite>, which may compromise the innate language capabilities.</p>

<p>The difficulty in multimodal alignment may stem from the next-token prediction loss used in optimizing LLMs, which may favor an over-reliance on the LLM’s parametric knowledge, as discussed in <d-cite key="zhai2023halle"></d-cite>, rather than faithfully grounding predictions in multimodal signals. Consequently, this can lead to multimodal hallucinations, where the model generates outputs that are inconsistent with the concrete content contained in sensory inputs. Another factor significantly influencing the generation of LLMs is the adopted decoding strategies. Recent work <d-cite key="leng2023mitigating"></d-cite> has shown that multimodal hallucination can be alleviated by adapting advanced decoding methods, such as contrastive decoding <d-cite key="li2022contrastive"></d-cite>, for MLLMs as a training-free corrective mechanism. Apart from using modality connectors, another viable solution is to textualize non-language signals using expert models, as exemplified by the VideoChat-Text model <d-cite key="li2023videochat"></d-cite>, though this approach comes with the downsides of information loss and a pipeline that is not end-to-end optimized.</p>

<p><strong>Modality Decoders</strong> In addition to the aforementioned three compulsory modules, MLLMs can be further extended with modality-specific decoders, particularly in the Any-to-Any workflow, to overcome the limitation of MLLMs only perceiving multimodal data.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0" id="figure-6">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/mllm_primer_post_nextgpt-480.webp 480w,/assets/img/mllm_primer_post_nextgpt-800.webp 800w,/assets/img/mllm_primer_post_nextgpt-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/mllm_primer_post_nextgpt.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
</div>
<div class="caption">
    Fig-6 An overview of an Any-to-Any multimodal system NExT-GPT that can perceive inputs and generate outputs in arbitrary combinations of text, images, videos, and audio. Image Source: <a href="https://arxiv.org/abs/2309.05519" target="_blank">NExT-GPT: Any-to-Any Multimodal LLM</a>.
</div>

<p>MLLMs with multimodal responsive capabilities have recently spurred a surge of interest in the area of Generative AI, regarded as a significant step towards Artificial General Intelligence (AGI). For image generation, recent works such as Emu <d-cite key="sun2023generative"></d-cite> and Mini-Gemini <d-cite key="li2024mini"></d-cite> default to using diffusion-based generative models <d-cite key="luo2022understanding"></d-cite> <d-cite key="ribeiro2024demystifying"></d-cite> <d-cite key="chan2024tutorial"></d-cite>, such as the Stable Diffusion family <d-cite key="rombach2022high"></d-cite>, due to their unparalleled text-to-image generation performance. The work NExT-GPT <d-cite key="wu2023next"></d-cite>, as illustrated in <a href="#figure-6">Fig-6</a>, further extends LLMs to include image, audio, and video diffusion models, enabling content creation in arbitrary combinations of image, text, audio, and video. Similar to the modality connectors used in the encoding phase, it is reasonable to bridge the gap between textual embeddings generated by LLMs and those used as conditional guidance in diffusion models, which can be achieved through decoder-side alignment.</p>

<h3 id="training-strategies-and-data">Training Strategies and Data</h3>

<p>The training process of MLLMs can be decomposed into three stages: 1) pre-alignment, 2) instruction tuning, 3) alignment with human preferences. The fulfilment of training objective in each phase heavily relies on specific data, necessitating cost-effective data scaling-up methods and high quality data.</p>

<p><strong>Pre-Alignment</strong> The pre-alignment stage often requires an enormous amount of text-paired data, such as images, videos, or audio files with associated textual descriptions, usually gathered from the Internet. However, the innate nature of LLMs as generative pre-trained models (next-token-prediction) casts doubt on the validity of using noisy and short text-paired data, which is originally used for contrastive alignment. As demonstrated in works such as ShareGPT4V <d-cite key="chen2023sharegpt4v"></d-cite>, both the pre-alignment and SFT stages significantly benefit from highly descriptive caption data. Additionally, the progressive alignment scheme adopted in InternVL <d-cite key="chen2023internvl"></d-cite>, transitioning from vision-language contrastive learning to generative pre-training, has proven to be more effective in accommodating the heterogeneous sources of pre-training data.</p>

<p><strong>Instruction Tuning</strong> Instruction tuning (IT) <d-cite key="zhang2023instruction"></d-cite> aims to steer LLMs to respond more faithfully to human instructions by fine-tuning on instruction following datasets, which consist of (Instruction, Input, Output) triplets. This technique has proven to be effective and computationally efficient in enhancing the controllability of LLMs’ output behavior, thereby eliciting knowledge from LLMs that is well-aligned with human intents. Furthermore, IT has been identified as a critical factor in unlocking the few-shot (ICL) or zero-shot generalization capability of LLMs on solving novel tasks.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0" id="figure-7">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/mllm_primer_post_self_instruct-480.webp 480w,/assets/img/mllm_primer_post_self_instruct-800.webp 800w,/assets/img/mllm_primer_post_self_instruct-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/mllm_primer_post_self_instruct.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
</div>
<div class="caption">
    Fig-7 An illustration of the overall process of Self-Instruct. Image Source: <a href="https://arxiv.org/abs/2212.10560" target="_blank">SELF-INSTRUCT: Aligning Language Models with Self-Generated Instructions</a>.
</div>

<p>The current trend in scaling up IT data involves leveraging semi-automatic data engines to minimize human intervention. Specifically, the majority of IT datasets are constructed using a paradigm known as Self-Instruct <d-cite key="wang2022self"></d-cite>, as depicted in <a href="#figure-7">Fig-7</a>, where a few manually-crafted seed examples serve as demonstrations for LLMs, such as GPT-4, to generate additional similar samples by harnessing the ICL capability. This approach can be extended to generate multimodal IT data. For instance, in the pioneering work LLaVA <d-cite key="liu2024visual"></d-cite>, images are converted into text descriptions and bounding box coordinates, thereby endowing the text-only GPT-4 with an imaginary vision capability, which are then combined with diverse task prompts to create visual instruction-following datasets. However, IT data built on top of GPT-4 (text-only LLMs) often suffer from multimodal hallucination, compromising the quality of the generated samples, which can be remedied with the advent of powerful MLLMs. For instance, in the work ALLaVA <d-cite key="chen2024allava"></d-cite>, GPT-4V, a multimodal variant of GPT-4, is utilized to create high-quality IT data by following a captioning-questioning-answering pipeline. Moreover, recent works, such as ALLaVA <d-cite key="chen2024allava"></d-cite> and Lynx <d-cite key="zeng2023matters"></d-cite>, emphasize that the quality of IT datasets—specifically the complexity and diversity of instructions and the detail of responses—is more crucial than their quantity.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0" id="figure-8">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/mllm_primer_post_data_engine-480.webp 480w,/assets/img/mllm_primer_post_data_engine-800.webp 800w,/assets/img/mllm_primer_post_data_engine-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/mllm_primer_post_data_engine.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
</div>
<div class="caption">
    Fig-8 A schematic diagram of semi-automatic data engine.
</div>

<p>Despite the promising performance gains achieved through IT, there have been criticisms regarding its efficacy. For instance, it has been shown that instruction-tuned LLMs merely mimic the style of proprietary models like ChatGPT and fall short in those critical dimensions such as factuality, coding, and problem solving <d-cite key="kung2023models"></d-cite> <d-cite key="gudibande2023false"></d-cite>, which has led to the conclusion that bridging the capabilities gap between open-source LLMs and proprietary ones through IT is considered a false promise <d-cite key="gudibande2023false"></d-cite>. Nevertheless, leveraging a scalable semi-automatic data engine (<a href="#figure-8">Fig-8</a>)-an iteratively refined pseudo data labelling/generation pipeline with human in the loop-is a key enabler for advancing MLLMs. For example, the All-Seeing Project <d-cite key="wang2023all"></d-cite>, a recent effort in building an open-world visual recognition and understanding model, created approximately 1 billion instance-level annotations of open-world visual concepts with detailed captions and question-answer pairs, which would have been prohibitive without the assistance of semi-automatic data engines.</p>

<p><strong>Preference Alignment</strong> Alignment with human preferences aims to further refine the output behaviour of MLLMs that have undergone SFT or instruction tuning by leveraging human/AI feedback on model responses. The two mainstream solutions currently in use are Reinforcement Learning with Human Feedback (RLHF) <d-cite key="ziegler2019fine"></d-cite> <d-cite key="ouyang2022training"></d-cite> and Direct Preference Optimization (DPO) <d-cite key="rafailov2024direct"></d-cite>.</p>

<h3 id="evaluation">Evaluation</h3>

<p>The evaluation of MLLMs is generally categorized into closed-ended and open-ended questions. For open-ended questions, the assessment criteria may involve human or GPT scoring. In contrast to evaluation methods developed before the era of LLMs, designing evaluation toolkits for MLLMs demands attention and efforts comparable to or even surpass those required for model development. As pointed out in <d-cite key="gudibande2023false"></d-cite>, human evaluators without domain expertise or significant time investment can be easily deceived by LLMs due to their fluent, confident, and well-structured responses. Therefore, acquiring quantitative measurements that can objectively and reliably reflect the multifaceted aspects of MLLMs is crucial, as these measurements facilitate pinpointing critical factors contributing to improvements along specified skill dimensions, identifying failure cases, and providing deeper insights into the inner workings of MLLMs.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0" id="figure-9">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/mllm_primer_post_ai_dev_cycle-480.webp 480w,/assets/img/mllm_primer_post_ai_dev_cycle-800.webp 800w,/assets/img/mllm_primer_post_ai_dev_cycle-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/mllm_primer_post_ai_dev_cycle.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
</div>
<div class="caption">
    Fig-9 An illustration of AI development cycle: a dynamic and iterative process where data creation, model development, and model assessment continuously inform and improve each other.
</div>

<p>We should be wary of increasingly potent MLLMs that can take shortcuts to inflate performance metrics and deliver delusive results, which requires researchers to act as adversaries, critically analyzing and probing the models’ responses for weaknesses. Consequently, evaluation frameworks that combine both human and AI efforts and can continuously evolve offer promising avenues to mitigate these issues, as exemplified in Dynabench <d-cite key="kiela2021dynabench"></d-cite>, where dataset creation, model development, and model evaluation co-evolve and reinforce each other, as shown in <a href="#figure-9">Fig-9</a>, serving as a counterbalance to the phenomenon where state-of-the-art AI systems quickly saturate benchmarks but fail on real-world challenges.</p>

<h2 id="concluding-remarks">Concluding Remarks</h2>

<p>The development of LLMs and their extension to multimodal understanding (MLLMs) reveals a promising pathway for machines to approach human-level intelligence: by actively interacting with humans and engaging in human daily activities, reminiscent of how we nurture and educate our offspring. In essence, the emergence of MLLMs marks a watershed moment in the evolution of machine learning systems, ushering in the era of Human-Centered AI (HCAI), which begins with the creation of a more natural and intuitive human-machine interaction interface. Reflecting on the saying of Richard Feynman that “What I cannot create, I do not understand,” we can anticipate significant breakthroughs in neuroscience when AI systems can faithfully emulate human cognitive capabilities by generating responses indistinguishable from those of humans. The development of AI assistants akin to Doraemon can provide profound insights into the neural and cognitive mechanisms underlying human intelligence and foster a closer synergy between neuroscience and AI research. While Norbert Wiener’s saying, “The best material model of a cat is another, or preferably the same, cat,” underscores the immense challenge of precisely simulating the complexity and intricacies inherent in biological systems with artificial models, it is exciting to contemplate the potential of an increasingly close collaborative relationship between humans and AI.</p>

<h2 id="acknowledgement">Acknowledgement</h2>

<p>This blog post draws significant inspiration from two outstanding works reviewing the latest advances in MLLMs:</p>

<ol>
  <li>
    <p>“A Survey on Multimodal Large Language Models” <d-cite key="yin2023survey"></d-cite>, which provides a timely and comprehensive overview of recent progress in MLLMs, supplemented by a continuously updated GitHub webpage: <a href="https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models">Awesome-Multimodal-Large-Language-Models</a>.</p>
  </li>
  <li>
    <p>“Multimodal Foundation Models: From Specialists to General-Purpose Assistants” <d-cite key="li2023multimodal"></d-cite>, where leading researchers in the field present an insightful dissection, offering both a historical perspective on the development of multimodal foundation models and an inspiring vision for their future.</p>
  </li>
</ol>

<p>Additionally, the completion of this blog post was assisted by OpenAI ChatGPT in refining phrasing, expression, and tone.</p>

<div class="language-html highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@misc{Cai2024mllm_primer,
    author = {Xin Cai},
    title = {A Primer on Multimodal Large Language Models: Towards Building Doraemon-like AI Assistants},
    howpublished = {\url{https://totalvariation.github.io/blog/2024/a-primer-on-mllms/}},
    note = {Accessed: 2024-05-20},
    year = {2024}
}
</code></pre></div></div>]]></content><author><name>Xin Cai</name></author><category term="MLLMs," /><category term="LVLMs," /><category term="General-Purpose-Assistants," /><category term="GenAI" /><summary type="html"><![CDATA[Towards Building Doraemon-like AI Assistants]]></summary></entry><entry><title type="html">Random Thoughts on Open World Vision Systems</title><link href="https://totalvariation.github.io/blog/2024/open-world-vision-systems/" rel="alternate" type="text/html" title="Random Thoughts on Open World Vision Systems" /><published>2024-02-01T00:00:00+00:00</published><updated>2024-02-01T00:00:00+00:00</updated><id>https://totalvariation.github.io/blog/2024/open-world-vision-systems</id><content type="html" xml:base="https://totalvariation.github.io/blog/2024/open-world-vision-systems/"><![CDATA[<blockquote>
    <p><b>It is not the strongest of the species that survives, nor the most intelligent that survives. It is the one that is most adaptable to change.</b></p> 
    <footer>- Charles Darwin</footer>
</blockquote>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/ante-hamersmit-qM8zX1celvc-unsplash-480.webp 480w,/assets/img/ante-hamersmit-qM8zX1celvc-unsplash-800.webp 800w,/assets/img/ante-hamersmit-qM8zX1celvc-unsplash-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/ante-hamersmit-qM8zX1celvc-unsplash.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
</div>
<div class="caption">
    Photo by <a href="https://unsplash.com/@ante_kante?utm_content=creditCopyText&amp;utm_medium=referral&amp;utm_source=unsplash">Ante Hamersmit</a> on <a href="https://unsplash.com/photos/person-holding-reptile-qM8zX1celvc?utm_content=creditCopyText&amp;utm_medium=referral&amp;utm_source=unsplash">Unsplash</a>
</div>

<p>Over the last decade, there has been remarkable progress in visual perception algorithms, driven by the development of layered differentiable models optimized in an end-to-end fashion. Despite these advancements, the deployment of such algorithms in open-world scenarios poses significant challenges. To effectively operate in open-world settings, these algorithms must incorporate capabilities for open-set recognition and open vocabulary learning, while also being robust to distribution shifts. Moreover, Open World Visual Understanding Systems (OWVUS) are expected to handle continuous streams of real-world data, adapting to non-stationary environments with minimal labelling requirements. This suggests the need for semi- or fully-automatic data engines that facilitate model learning alongside incremental data curation and labelling, potentially assisted with human intervention. The foundational pillars of building OWVUS lie in data and annotation efficiency, robustness, and generalization. These challenges can potentially be addressed through a unified framework known as Open-set Unsupervised Domain Adaptation (OUDA).</p>

<p>In general, visual signals can be decomposed into low- and high-frequency components. The former enables the unlocking of open-vocabulary capabilities in Vision-Language Models (VLMs) <d-cite key="radford2021learning"></d-cite> <d-cite key="li2021align"></d-cite> by establishing a well-aligned visual-semantic space. Conversely, the latter plagues the generalizability of deep neural networks on unseen or novel domains, as they may overfit to specific domain styles and therefore capture spurious correlations. OUDA serves as a nexus that connects a broad spectrum of research fields, including generative modelling <d-cite key="hoffman2018cycada"></d-cite> <d-cite key="ilse2020diva"></d-cite> <d-cite key="mahajan2020latent"></d-cite>, semi-supervised learning <d-cite key="berthelot2021adamatch"></d-cite> <d-cite key="zhang2020label"></d-cite>, meta-learning <d-cite key="shu2021open"></d-cite> <d-cite key="kim2022pin"></d-cite> <d-cite key="zhao2021learning"></d-cite>, open-set recognition <d-cite key="saito2021ovanet"></d-cite> <d-cite key="saito2020universal"></d-cite>, open-vocabulary learning <d-cite key="yu2023open"></d-cite> <d-cite key="zara2023autolabel"></d-cite> <d-cite key="wu2023towards"></d-cite>, out-of-distribution detection <d-cite key="shu2023clipood"></d-cite> <d-cite key="wang2023clipn"></d-cite>, few-/zero-shot learning <d-cite key="wu2022style"></d-cite> <d-cite key="fahes2023poda"></d-cite>, contrastive learning <d-cite key="da2022dual"></d-cite> <d-cite key="sahoo2021contrast"></d-cite> <d-cite key="zara2023simplifying"></d-cite>, unsupervised representation learning <d-cite key="hoyer2023mic"></d-cite> <d-cite key="vray2024distill"></d-cite>, active learning <d-cite key="zhang2022bamboo"></d-cite> <d-cite key="wu2022entropy"></d-cite>, continual learning <d-cite key="atanyan2024continuous"></d-cite> <d-cite key="lin2022prototype"></d-cite> <d-cite key="wang2302comprehensive"></d-cite>, and disentanglement of factors of variation <d-cite key="wu2022single"></d-cite> <d-cite key="wei2022unsupervised"></d-cite>. Its primary objective is to transfer knowledge from annotated source domains or pre-trained models (in the case of source-free UDA <d-cite key="liang2020we"></d-cite>) to unlabelled target domains. This transfer process requires addressing both covariate and semantic shifts, which correspond to variations in the high- and low-frequency components of visual signals, respectively. In essence, OUDA leverages methods developed within the aforementioned research areas to combat covariate or semantic shifts. On the other hand, addressing distribution shifts becomes unavoidable when extending any research problem from these areas to a more general and practical context.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0" id="figure-1">
        

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/open_world_vision_post_uda_methods-480.webp 480w,/assets/img/open_world_vision_post_uda_methods-800.webp 800w,/assets/img/open_world_vision_post_uda_methods-1400.webp 1400w," sizes="95vw" type="image/webp" />
    
    <img src="/assets/img/open_world_vision_post_uda_methods.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>

    </div>
</div>
<div class="caption">
    Fig-1 The taxonomy of UDA methods.
</div>

<p>OUDA integrates open-set recognition/open-vocabulary learning and domain adaptation/generalization within a unified framework, aiming to address both high-level semantic shift and low-level covariate shift simultaneously and therefore presenting compounded challenges that stem from both research domains. Recent advancements have demonstrated rapid progress in constructing sophisticated open-vocabulary detectors or segmentors <d-cite key="zhu2023survey"></d-cite>, facilitated by VLMs trained on web-scale image-text pairs which offer a comprehensive prior of the real-world. Central to this advancement is the endeavour to bridge the granularity gap between coarse-grained visual-semantic associations and fine-grained visual understanding objectives. Predominant solutions can fall into three main categories: (1) incorporating fine-grained awareness into pre-training recipes <d-cite key="li2022grounded"></d-cite> <d-cite key="zhong2022regionclip"></d-cite> <d-cite key="rao2022denseclip"></d-cite>; (2) transferring knowledge from VLMs to downstream fine-grained visual understanding tasks <d-cite key="gu2021open"></d-cite> <d-cite key="kuo2022f"></d-cite> <d-cite key="wu2023cora"></d-cite> <d-cite key="he2023clip"></d-cite>; (3) the amalgamation of Vision Foundation Models (VFMs) by leveraging their complementary expertise resulting from distinct pretraining objectives <d-cite key="han2023boosting"></d-cite> <d-cite key="wang2023sam"></d-cite>. Besides, the pursuit of handling diverse vision tasks with a unified architecture and a single suite of weights in the open-set scenario <d-cite key="zou2023generalized"></d-cite> <d-cite key="zhang2023simple"></d-cite> has garnered increasing attention as a step towards constructing general-purpose vision foundation models.</p>

<p>In the emerging task of Open-World Object Detection (OWOD) <d-cite key="joseph2021towards"></d-cite> <d-cite key="gupta2022ow"></d-cite> <d-cite key="wang2023detecting"></d-cite>, which combines open-set recognition with class incremental learning, the inherent open-vocabulary capability of VLMs offers convenience in identifying unknown classes. However, specialized components remain indispensable for the discovery of novel classes. Essentially, <strong>equipping a neural network with the ability to say NO when facing unfamiliar input</strong>, even with models like CLIP <d-cite key="wang2023clipn"></d-cite>, presents significant challenges. Particularly in vision tasks, developing a class-agnostic object localizer capable of generalizing to novel classes remains an open question <d-cite key="kim2022learning"></d-cite>. This challenge proves critical for two-stage open-world detectors or segmentors, as the generation of high-quality proposals for unknown classes is pivotal. Recently, there is a promising trend where class-agnostic object discovery is tackled without requiring any manual annotation by leveraging pre-trained features of self-supervised Vision Transformers (ViTs) <d-cite key="simeoni2023unsupervised"></d-cite>. However, these algorithms still struggle in complex scene-centric scenarios. OUDA introduces a more complicated pipeline compared to UDA with a close-set assumption, requiring the detection or rejection of unknown classes followed by cross-domain alignment with dominant solutions illustrated in <a href="#figure-1">Fig-1</a>. It has been demonstrated that overlooking unknown classes during domain alignment can lead to negative transfer or even catastrophic misalignment. As far as I know, existing methods for open-set recognition or novel class discovery largely rely on heuristic approaches, such as one-vs-all classifiers <d-cite key="saito2021ovanet"></d-cite>, entropy-based separation <d-cite key="saito2020universal"></d-cite>, inter-class distance or margin-based methods <d-cite key="miller2021class"></d-cite>, and leveraging zero-shot predictions from VLMs <d-cite key="yu2023open"></d-cite> <d-cite key="zara2023autolabel"></d-cite>. Furthermore, effectively separating (target-)private samples into semantically coherent clusters <d-cite key="zara2023autolabel"></d-cite>, rather than treating them indiscriminately as a generic unknown class, presents an even more formidable challenge, requiring the utilization of intrinsic structures within unseen or novel classes.</p>

<p>Scaling up pre-training data with minimal human intervention has proven to be critical to foundation models, such as GLIP <d-cite key="li2022grounded"></d-cite> and SAM <d-cite key="kirillov2023segment"></d-cite>. Particularly in scenarios where manual annotation is resource-intensive <d-cite key="delatolas2024learning"></d-cite>, there’s a pressing need for an automatic data annotation framework. Such a framework should not only generate reliable pseudo labels but also continually expand the concept pool, thereby necessitating resilience to domain shifts stemming from heterogeneous data sources and open-set recognition capability. In the context of video action recognition, this task is referred to as Open-set Video Domain Adaptation (OUVDA) <d-cite key="zara2023simplifying"></d-cite> <d-cite key="zara2023autolabel"></d-cite>, which remains largely unexplored. This emerging research direction is inherently more complex due to the additional temporal dimension and the scarcity of large-scale and diverse datasets, presenting unique challenges that warrant further investigation. The closed learning loop, which involves the simultaneous evolution of model updating and dataset expansion, lays the groundwork for OWVUS capable of continual self-development over time. From a data-centric standpoint, the challenge revolves around constructing a dynamic dataset capable of consistently absorbing novel semantic categories and selecting relevant samples continually <d-cite key="de2021continual"></d-cite> and actively <d-cite key="zhang2022bamboo"></d-cite> with human-machine synergy. Continual learning <d-cite key="wang2302comprehensive"></d-cite>, characterized by rapid adaptation to evolving data distributions and the potential encounter with unseen classes while avoiding catastrophic forgetting, can thus be integrated with OUDA to fulfil this objective.</p>

<p>To conclude, the prospect of unfolding possibilities and burgeoning potential in the field of OUDA and its synergies with other areas like continual learning and active learning fills me with anticipation and enthusiasm.</p>

<div class="language-html highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@misc{Cai2024OpenWorld,
    author = {Xin Cai},
    title = {Random Thoughts on Open World Vision Systems},
    howpublished = {\url{https://totalvariation.github.io/blog/2024/open-world-vision-systems/}},
    note = {Accessed: 2024-02-08},
    year = {2024}
}
</code></pre></div></div>]]></content><author><name>Xin Cai</name></author><category term="Open-set" /><category term="Open-vocabulary" /><category term="UDA" /><summary type="html"><![CDATA[It is not the strongest of the species that survives, nor the most intelligent that survives. It is the one that is most adaptable to change. - Charles Darwin]]></summary></entry></feed>