<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://totalvariation.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://totalvariation.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-02-07T01:00:08+00:00</updated><id>https://totalvariation.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Random Thoughts on Open World Vision Systems</title><link href="https://totalvariation.github.io/blog/2024/open-world-vision-systems/" rel="alternate" type="text/html" title="Random Thoughts on Open World Vision Systems"/><published>2024-02-01T00:00:00+00:00</published><updated>2024-02-01T00:00:00+00:00</updated><id>https://totalvariation.github.io/blog/2024/open-world-vision-systems</id><content type="html" xml:base="https://totalvariation.github.io/blog/2024/open-world-vision-systems/"><![CDATA[<p>Over the last decade, there has been remarkable progress in visual perception algorithms, driven by the development of layered differentiable models optimized in an end-to-end fashion. Despite these advancements, the deployment of such algorithms in open-world scenarios poses significant challenges. To effectively operate in open-world settings, these algorithms must incorporate capabilities for open-set recognition and open vocabulary learning, while also being robust to distribution shifts. Moreover, open-world visual understanding systems are expected to handle continuous streams of real-world data, adapting to non-stationary environments with minimal labelling requirements. This suggests the need for semi- or fully-automatic data engines that facilitate model learning alongside incremental data curation and labelling, potentially assisted with human intervention. The foundational pillars of building open-world visual understanding systems lie in data and annotation efficiency, robustness, and generalization. These challenges can potentially be addressed through a unified framework known as Open-set Unsupervised Domain Adaptation (OUDA).</p> <p>In general, visual signals can be decomposed into low- and high-frequency components. The former enables the unlocking of open-vocabulary capabilities in Vision-Language Models (VLMs) &lt;d-cite, key=”radford2021learning”&gt;&lt;/d-cite&gt; &lt;d-cite, key=”li2021align”&gt;&lt;/d-cite&gt; by establishing a well-aligned visual-semantic space. Conversely, the latter plagues the generalizability of deep neural networks on unseen or novel domains, as they may overfit to specific domain styles and therefore capture spurious correlations. OUDA serves as a nexus that connects a broad spectrum of research fields, including generative modelling &lt;d-cite, key=”hoffman2018cycada”&gt;<d-cite> &lt;d-cite, key="ilse2020diva"&gt;<d-cite> &lt;d-cite, key="mahajan2020latent"&gt;<d-cite>, semi-supervised learning &lt;d-cite, key="berthelot2021adamatch"&gt;<d-cite> &lt;d-cite, key="zhang2020label"&gt;<d-cite>, meta-learning &lt;d-cite, key="shu2021open"&gt;<d-cite> &lt;d-cite, key="kim2022pin"&gt;<d-cite> &lt;d-cite, key="zhao2021learning"&gt;<d-cite>, open-set recognition &lt;d-cite, key="saito2021ovanet"&gt;<d-cite> &lt;d-cite, key="saito2020universal"&gt;<d-cite>, open-vocabulary learning &lt;d-cite, key="yu2023open"&gt;<d-cite> &lt;d-cite, key="zara2023autolabel"&gt;<d-cite> &lt;d-cite, key="wu2023towards"&gt;<d-cite>, out-of-distribution detection &lt;d-cite, key="shu2023clipood"&gt;</d-cite> &lt;d-cite, key="wang2023clipn"&gt;</d-cite>, few-/zero-shot learning &lt;d-cite, key="wu2022style"&gt;</d-cite> &lt;d-cite, key="fahes2023poda"&gt;</d-cite>, contrastive learning &lt;d-cite, key="da2022dual"&gt;<d-cite> &lt;d-cite, key="sahoo2021contrast"&gt;<d-cite> &lt;d-cite, key="zara2023simplifying"&gt;<d-cite>, unsupervised representation learning &lt;d-cite, key="hoyer2023mic"&gt;<d-cite> &lt;d-cite, key="vray2024distill"&gt;<d-cite>, active learning &lt;d-cite, key="zhang2022bamboo"&gt;</d-cite> &lt;d-cite, key="wu2022entropy"&gt;</d-cite>, continual learning &lt;d-cite, key="atanyan2024continuous"&gt;</d-cite> &lt;d-cite, key="lin2022prototype"&gt;</d-cite> &lt;d-cite, key="wang2302comprehensive"&gt;</d-cite>, and disentanglement of factors of variation &lt;d-cite, key="wu2022single"&gt;</d-cite> &lt;d-cite, key="wei2022unsupervised"&gt;</d-cite>. Its primary objective is to transfer knowledge from annotated source domains or pre-trained models (in the case of source-free UDA &lt;d-cite, key="liang2020we"&gt;</d-cite>) to unlabelled target domains. This transfer process requires addressing both covariate and semantic shifts, which correspond to variations in the high- and low-frequency components of visual signals, respectively. In essence, OUDA leverages methods developed within the aforementioned research areas to combat covariate or semantic shifts. On the other hand, addressing distribution shifts becomes unavoidable when extending any research problem from these areas to a more general and practical context.</d-cite></d-cite></d-cite></d-cite></d-cite></d-cite></p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/open_world_vision_post_uda_methods-480.webp 480w,/assets/img/open_world_vision_post_uda_methods-800.webp 800w,/assets/img/open_world_vision_post_uda_methods-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/open_world_vision_post_uda_methods.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 1 The taxonomy of UDA methods. </div> <p>OUDA integrates open-set recognition/open-vocabulary learning and domain adaptation/generalization within a unified framework, aiming to address both high-level semantic shift and low-level covariate shift simultaneously and therefore presenting compounded challenges that stem from both research domains. Recent advancements have demonstrated rapid progress in constructing sophisticated open-vocabulary detectors or segmentors &lt;d-cite, key=”zhu2023survey”&gt;&lt;/d-cite&gt;, facilitated by VLMs trained on web-scale image-text pairs which offer a comprehensive prior of the real-world. Central to this advancement is the endeavour to bridge the granularity gap between coarse-grained visual-semantic associations and fine-grained visual understanding objectives. Predominant solutions can fall into three main categories: (1) incorporating fine-grained awareness into pre-training recipes &lt;d-cite, key=”li2022grounded”&gt;&lt;/d-cite&gt; &lt;d-cite, key=””&gt;&lt;/d-cite&gt; &lt;d-cite, key=”zhong2022regionclip”&gt;&lt;/d-cite&gt; &lt;d-cite, key=”rao2022denseclip”&gt;&lt;/d-cite&gt;; (2) transferring knowledge from VLMs to downstream fine-grained visual understanding tasks &lt;d-cite, key=”gu2021open”&gt;&lt;/d-cite&gt; &lt;d-cite, key=”kuo2022f”&gt;&lt;/d-cite&gt; &lt;d-cite, key=”wu2023cora”&gt;&lt;/d-cite&gt; &lt;d-cite, key=”he2023clip”&gt;&lt;/d-cite&gt;; (3) the amalgamation of Vision Foundation Models (VFMs) by leveraging their complementary expertise resulting from distinct pretraining objectives &lt;d-cite, key=”han2023boosting”&gt;&lt;/d-cite&gt; &lt;d-cite, key=”wang2023sam”&gt;&lt;/d-cite&gt;. Besides, the pursuit of handling diverse vision tasks with a unified architecture and a single suite of weights in the open-set scenario &lt;d-cite, key=”zou2023generalized”&gt;&lt;/d-cite&gt; &lt;d-cite, key=”zhang2023simple”&gt;&lt;/d-cite&gt; has garnered increasing attention as a step towards constructing general-purpose vision foundation models.</p> <p>In the emerging task of Open-World Object Detection (OWOD) &lt;d-cite, key=”joseph2021towards”&gt;&lt;/d-cite&gt; &lt;d-cite, key=”gupta2022ow”&gt;&lt;/d-cite&gt; &lt;d-cite, key=”wang2023detecting”&gt;&lt;/d-cite&gt;, which combines open-set recognition with class incremental learning, the inherent open-vocabulary capability of VLMs offers convenience in identifying unknown classes. However, specialized components remain indispensable for the discovery of novel classes. Essentially, equipping a neural network with the ability to say <strong>NO</strong> when facing unfamiliar input, even with models like CLIP &lt;d-cite, key=”wang2023clipn”&gt;&lt;/d-cite&gt;, presents significant challenges. Particularly in vision tasks, developing a class-agnostic object localizer capable of generalizing to novel classes remains an open question &lt;d-cite, key=”kim2022learning”&gt;&lt;/d-cite&gt;. This challenge proves critical for two-stage open-world detectors or segmentors, as the generation of high-quality proposals for unknown classes is pivotal. Recently, there is a promising trend where class-agnostic object discovery is tackled without requiring any manual annotation by leveraging pre-trained features of self-supervised Vision Transformers (ViTs) &lt;d-cite, key=”simeoni2023unsupervised”&gt;&lt;/d-cite&gt;. However, these algorithms still struggle in complex scene-centric scenarios. OUDA introduces a more complicated pipeline compared to UDA with a close-set assumption, requiring the detection or rejection of unknown classes followed by cross-domain alignment with dominant solutions illustrated in Fig.1. It has been demonstrated that overlooking unknown classes during domain alignment can lead to negative transfer or even catastrophic misalignment. As far as I know, existing methods for open-set recognition or novel class discovery largely rely on heuristic approaches, such as one-vs-all classifiers &lt;d-cite, key=”saito2021ovanet”&gt;&lt;/d-cite&gt;, entropy-based separation &lt;d-cite, key=”saito2020universal”&gt;&lt;/d-cite&gt;, inter-class distance or margin-based methods &lt;d-cite, key=”miller2021class”&gt;&lt;/d-cite&gt;, and leveraging zero-shot predictions from VLMs &lt;d-cite, key=”yu2023open”&gt;&lt;/d-cite&gt; &lt;d-cite, key=”zara2023autolabel”&gt;&lt;/d-cite&gt;. Furthermore, effectively separating (target-)private samples into semantically coherent clusters &lt;d-cite, key=”zara2023autolabel”&gt;&lt;/d-cite&gt;, rather than treating them indiscriminately as a generic unknown class, presents an even more formidable challenge, requiring the utilization of intrinsic structures within unseen or novel classes.</p> <p>Scaling up pre-training data with minimal human intervention has proven to be critical to foundation models, such as GLIP &lt;d-cite, key=”li2022grounded”&gt;&lt;/d-cite&gt; and SAM &lt;d-cite, key=”kirillov2023segment”&gt;&lt;/d-cite&gt;. Particularly in scenarios where manual annotation is resource-intensive &lt;d-cite, key=”delatolas2024learning”&gt;&lt;/d-cite&gt;, there’s a pressing need for an automatic data annotation framework. Such a framework should not only generate reliable pseudo labels but also continually expand the concept pool, thereby necessitating resilience to domain shifts stemming from heterogeneous data sources and open-set recognition capability. In the context of video action recognition, this task is referred to as Open-set Video Domain Adaptation (OUVDA) &lt;d-cite, key=”zara2023simplifying”&gt;&lt;/d-cite&gt; &lt;d-cite, key=”zara2023autolabel”&gt;&lt;/d-cite&gt;, which remains largely unexplored. This emerging research direction is inherently more complex due to the additional temporal dimension and the scarcity of large-scale and diverse datasets, presenting unique challenges that warrant further investigation. The closed learning loop, which involves the simultaneous evolution of model updating and dataset expansion, lays the groundwork for open-world visual understanding systems capable of continual self-development over time. From a data-centric standpoint, the challenge revolves around constructing a dynamic dataset capable of consistently absorbing novel semantic categories and selecting relevant samples continually &lt;d-cite, key=”de2021continual”&gt;&lt;/d-cite&gt; and actively &lt;d-cite, key=”zhang2022bamboo”&gt;<d-cite> with human-machine synergy. Continual learning &lt;d-cite, key="wang2302comprehensive"&gt;</d-cite>, characterized by rapid adaptation to evolving data distributions and the potential encounter with unseen classes while avoiding catastrophic forgetting, can thus be integrated with OUDA to fulfil this objective.</p> <p>To conclude, it can be anticipated that there will be a surge of research interest surrounding OUDA and its synergies with other areas such as continual learning and active learning, not only restricted to Open World Visual Understanding Systems (OWVUS). I am eager to delve deeper into this dynamic research field.</p>]]></content><author><name>Xin Cai</name></author><category term="Open"/><category term="World"/><category term="Visual"/><category term="Understanding"/><summary type="html"><![CDATA[Over the last decade, there has been remarkable progress in visual perception algorithms, driven by the development of layered differentiable models optimized in an end-to-end fashion. Despite these advancements, the deployment of such algorithms in open-world scenarios poses significant challenges. To effectively operate in open-world settings, these algorithms must incorporate capabilities for open-set recognition and open vocabulary learning, while also being robust to distribution shifts. Moreover, open-world visual understanding systems are expected to handle continuous streams of real-world data, adapting to non-stationary environments with minimal labelling requirements. This suggests the need for semi- or fully-automatic data engines that facilitate model learning alongside incremental data curation and labelling, potentially assisted with human intervention. The foundational pillars of building open-world visual understanding systems lie in data and annotation efficiency, robustness, and generalization. These challenges can potentially be addressed through a unified framework known as Open-set Unsupervised Domain Adaptation (OUDA). In general, visual signals can be decomposed into low- and high-frequency components. The former enables the unlocking of open-vocabulary capabilities in Vision-Language Models (VLMs) &lt;d-cite, key=”radford2021learning”&gt;&lt;/d-cite&gt; &lt;d-cite, key=”li2021align”&gt;&lt;/d-cite&gt; by establishing a well-aligned visual-semantic space. Conversely, the latter plagues the generalizability of deep neural networks on unseen or novel domains, as they may overfit to specific domain styles and therefore capture spurious correlations. OUDA serves as a nexus that connects a broad spectrum of research fields, including generative modelling &lt;d-cite, key=”hoffman2018cycada”&gt; &lt;d-cite, key="ilse2020diva"&gt; &lt;d-cite, key="mahajan2020latent"&gt;, semi-supervised learning &lt;d-cite, key="berthelot2021adamatch"&gt; &lt;d-cite, key="zhang2020label"&gt;, meta-learning &lt;d-cite, key="shu2021open"&gt; &lt;d-cite, key="kim2022pin"&gt; &lt;d-cite, key="zhao2021learning"&gt;, open-set recognition &lt;d-cite, key="saito2021ovanet"&gt; &lt;d-cite, key="saito2020universal"&gt;, open-vocabulary learning &lt;d-cite, key="yu2023open"&gt; &lt;d-cite, key="zara2023autolabel"&gt; &lt;d-cite, key="wu2023towards"&gt;, out-of-distribution detection &lt;d-cite, key="shu2023clipood"&gt; &lt;d-cite, key="wang2023clipn"&gt;, few-/zero-shot learning &lt;d-cite, key="wu2022style"&gt; &lt;d-cite, key="fahes2023poda"&gt;, contrastive learning &lt;d-cite, key="da2022dual"&gt; &lt;d-cite, key="sahoo2021contrast"&gt; &lt;d-cite, key="zara2023simplifying"&gt;, unsupervised representation learning &lt;d-cite, key="hoyer2023mic"&gt; &lt;d-cite, key="vray2024distill"&gt;, active learning &lt;d-cite, key="zhang2022bamboo"&gt; &lt;d-cite, key="wu2022entropy"&gt;, continual learning &lt;d-cite, key="atanyan2024continuous"&gt; &lt;d-cite, key="lin2022prototype"&gt; &lt;d-cite, key="wang2302comprehensive"&gt;, and disentanglement of factors of variation &lt;d-cite, key="wu2022single"&gt; &lt;d-cite, key="wei2022unsupervised"&gt;. Its primary objective is to transfer knowledge from annotated source domains or pre-trained models (in the case of source-free UDA &lt;d-cite, key="liang2020we"&gt;) to unlabelled target domains. This transfer process requires addressing both covariate and semantic shifts, which correspond to variations in the high- and low-frequency components of visual signals, respectively. In essence, OUDA leverages methods developed within the aforementioned research areas to combat covariate or semantic shifts. On the other hand, addressing distribution shifts becomes unavoidable when extending any research problem from these areas to a more general and practical context.]]></summary></entry><entry><title type="html">Displaying External Posts on Your al-folio Blog</title><link href="https://totalvariation.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog/" rel="alternate" type="text/html" title="Displaying External Posts on Your al-folio Blog"/><published>2022-04-23T23:20:09+00:00</published><updated>2022-04-23T23:20:09+00:00</updated><id>https://totalvariation.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog</id><content type="html" xml:base="https://totalvariation.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog/"><![CDATA[]]></content><author><name></name></author></entry></feed>