<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://totalvariation.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://totalvariation.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-05-20T11:22:39+00:00</updated><id>https://totalvariation.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">A Primer on Multimodal Large Language Models</title><link href="https://totalvariation.github.io/blog/2024/a-primer-on-mllms/" rel="alternate" type="text/html" title="A Primer on Multimodal Large Language Models"/><published>2024-05-20T00:00:00+00:00</published><updated>2024-05-20T00:00:00+00:00</updated><id>https://totalvariation.github.io/blog/2024/a-primer-on-mllms</id><content type="html" xml:base="https://totalvariation.github.io/blog/2024/a-primer-on-mllms/"><![CDATA[<blockquote> <p>The best material model of a cat is another, or preferably the same, cat.</p> <footer>- Norbert Wiener</footer> </blockquote> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0" id="figure-1"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mllm_primer_post_robot_cat_sdxl-480.webp 480w,/assets/img/mllm_primer_post_robot_cat_sdxl-800.webp 800w,/assets/img/mllm_primer_post_robot_cat_sdxl-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/mllm_primer_post_robot_cat_sdxl.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Fig-1 A sketch of general purpose AI assistants. Image Source: <a href="https://huggingface.co/docs/diffusers/en/using-diffusers/sdxl" target="_blank">Stable Diffusion XL</a>. </div> <h2 id="background">Background</h2> <p>Large Language Models (LLMs) <d-cite key="zhao2023survey"></d-cite> have achieved remarkable milestones, especially the demonstrated emergent capabilities, e.g., In-Context Learning (ICL), Chain of Thought (CoT) reasoning, and instruction following. It is now an inevitable trend to integrate various modalities with LLMs to mimic the way humans interact with the open world, thus ushering in the new era of Multimodal Large Language Models (MLLMs <d-footnote>Please note that the term MLLMs hereafter is used interchangeably with Large Vision-Language Models (LVLMs) or Large Multimodal Models (LMMs) given that the transitive property of modality embeddings showcased in ImageBind. <d-cite key="girdhar2023imagebind"></d-cite> </d-footnote>) <d-cite key="yin2023survey"></d-cite>.</p> <p>Recent work <d-cite key="li2023multimodal"></d-cite> has demonstrated that the development of MLLMs signifies a transition from specialist models to general-purpose visual assistants. This transition is regarded as a crucial step towards realizing the grand vision of a general-purpose multimodal AI agent, akin to bringing the beloved Japanese cartoon character Doraemon to life. It is further unveiled in <d-cite key="li2023multimodal"></d-cite> that a converging point for many development trajectories of vision-language/multimodal foundation models is <em>“the creation of general-purpose models and systems capable of following human intents and effortlessly executing a diverse array of vision and vision-language tasks in the wild.”</em></p> <p>The key features that markedly differentiate MLLMs from deep learning models developed beforehand are condensed into the name itself: <mark>General Purpose Assistants</mark>, which include <strong>versatility</strong>, <strong>interactivity</strong>, and <strong>controllability</strong>. Specifically, the quest for a unified architecture capable of accomplishing a diverse range of discriminative and generative tasks has witnessed great success in Natural Language Processing (NLP), as exemplified by GPT-3, inspiring similar research endeavours in the field of computer vision. However, as pointed out in <d-cite key="li2023multimodal"></d-cite>, the development of unified vision systems significantly lags behind due to fragmented vision tasks and the difficulty of scaling up visual data. Furthermore, to enable seamless communication between machines and humans, natural language serves as a core component in building a general interactive interface, which can be further complemented by multimodal prompts such as clicks or scribbles provided by users. Additionally, researchers have made strides in steering the output behaviour of MLLMs by instruction tuning or aligning with human preferences, marking a hallmark in elevating machine intelligence to the next level as humans begin to positively intervene in the machine learning process. By leveraging human feedback and rich experience, machines can evolve more effectively and efficiently, leading to better outcomes for both machines and humans. Inspired by the interactions between Doraemon and Nobita, the ultimate goal of general-purpose AI assistants is to establish a <strong>symbiotic relationship</strong> between humans and AI systems, where AI systems not only enhance the overall well-being and quality of human life but also evolve by engaging in humans’ daily activities.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0" id="figure-2"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mllm_primer_post_genaisys-480.webp 480w,/assets/img/mllm_primer_post_genaisys-800.webp 800w,/assets/img/mllm_primer_post_genaisys-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/mllm_primer_post_genaisys.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Fig-2 The general architecture of GenAISys. Image Source: <a href="https://jmtomczak.github.io/blog/21/21_genaisys.html" target="_blank">Jakub Tomczak</a>. </div> <p>Before delving into the technical aspects of MLLMs, I would like to introduce an intriguing and inspiring concept called Generative AI Systems (GenAISys), recently introduced in a <a href="https://jmtomczak.github.io/blog/21/21_genaisys.html">blog post</a> by Jakub Tomczak, which offers a novel perspective on MLLMs. In essence, GenAISys can be considered as end-to-end trainable MLLMs enhanced by external tools or databases, resonating with a similar idea proposed in Chapter 6 of the work <d-cite key="li2023multimodal"></d-cite>, where the authors indicate the possibility of combining strengths from two existing modelling paradigms of MLLMs: training or chaining tools with LLMs. For more insights into GenAISys and its future directions, I encourage you to refer to the original post by Jakub Tomczak.</p> <h2 id="technical-overview">Technical Overview</h2> <p>The recent progress of MLLMs <d-footnote>Technically, the progress reviewed here excludes that related to Multimodal Agents, i.e., chaining tools with LLMs.</d-footnote> can be roughly divided into the following three aspects: 1) architectures, 2) training strategies and data, 3) evaluation benchmarks.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0" id="figure-3"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mllm_primer_post_mllm_arch-480.webp 480w,/assets/img/mllm_primer_post_mllm_arch-800.webp 800w,/assets/img/mllm_primer_post_mllm_arch-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/mllm_primer_post_mllm_arch.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Fig-3 An overview of MLLM architecture. </div> <h3 id="architectures">Architectures</h3> <p>A typical neural architecture of MLLMs consists of pre-trained modality encoders, pre-trained LLMs, learnable modality adapters/connectors, and optional modality decoders.</p> <p><strong>Modality Encoders</strong> When compressing raw images, it is common practice to utilize pre-trained CLIP <d-cite key="radford2021learning"></d-cite> image encoders or other similar variants <d-cite key="sun2023eva"></d-cite>, leveraging a pre-aligned albeit coarse vision-semantic space. The underlying principle is to discretize/standardize modality embeddings with reference to language vocabulary, which can be extended to encoding other kinds of multimodal signals. For instance, the ImageBind <d-cite key="girdhar2023imagebind"></d-cite> encoder, used in ImageBind-LLM <d-cite key="han2023imagebind"></d-cite>, can construct a joint embedding space for six different modalities—images, text, audio, depth, thermal, and Inertial Measurement Unit (IMU) data—established by aligning a pair of modalities each time with InfoNCE <d-cite key="oord2018representation"></d-cite> loss.</p> <p>Another critical factor in enriching visual embeddings in MLLMs and mitigating multimodal hallucination is increasing input resolution <d-cite key="liu2023improved"></d-cite> while not incurring significant computational overheads. Following this principle, dual vision encoders are employed in Mini-Gemini <d-cite key="li2024mini"></d-cite>, where the coarse embeddings from the pre-trained CLIP image encoder are complemented by dense visual features output from the LAION-pretrained <d-cite key="schuhmann2022laion"></d-cite> ConvNeXt <d-cite key="liu2022convnet"></d-cite>, which serves as the high-resolution vision encoder.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0" id="figure-4"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mllm_primer_post_adc-480.webp 480w,/assets/img/mllm_primer_post_adc-800.webp 800w,/assets/img/mllm_primer_post_adc-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/mllm_primer_post_adc.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Fig-4 An illustration of workings of Analog-to-Digital Converters (ADC). Image Source: <a href="https://www.doeeet.com/content/eee-components/actives/analog-to-digital-converters-the-resolution-dilemma/" target="_blank">DOEEET</a>. </div> <p>Finally, I would like to highlight an intriguing perspective by drawing a parallel between the modality encoder in MLLMs and the Analog-to-Digital Converter (ADC), which converts continuous analog signals into discrete digital signals through sampling and quantization. By establishing such a rough equivalence, we can gain deeper insights into the design rationale of the modality encoder. Specifically, bandwidth and Signal-to-Noise Ratio (SNR) play a crucial role in determining the overall performance and quality of an ADC. Analogously, the range of frequencies in input signals that can be captured by the modality encoder contributes to the overall performance of MLLMs, justifying the utilization of dual vision encoders, e.g., in Mini-Gemini <d-cite key="li2024mini"></d-cite>. As demonstrated in <d-cite key="park2022vision"></d-cite>, multi-head self-attention (MSAs) in Vision Transformers (ViT) <d-cite key="dosovitskiy2020image"></d-cite> function as a low-pass filter, while convolution operations act oppositely as a high-pass filter. Therefore, the convolutional branch supplements the ViT with high-frequency information, expanding the bandwidth of the modality encoder. Furthermore, image-text contrastive pre-training can be seen as a quantization process, where discrete codes (i.e., vocabulary indices) are allocated to encoded visual signals by filtering out noise and low-level information while maximally preserving mutual information. By analogy with a higher SNR leading to finer resolution of an ADC, the ability to discern the smallest semantic variation of the modality encoder can be improved by adopting fine-grained contrastive loss <d-cite key="li2022grounded"></d-cite> or detailed captioning <d-cite key="yu2022coca"></d-cite>.</p> <p><strong>LLMs</strong> Regarding pre-trained LLMs, open-source models such as the LLaMA series <d-cite key="touvron2023llama"></d-cite> <d-cite key="touvron2023llama2"></d-cite> and Vicuna family <d-cite key="chiang2023vicuna"></d-cite> have gained widespread popularity in academic research. These models are often combined with Parameter-Efficient Fine-Tuning (PEFT) techniques <d-cite key="han2024parameter"></d-cite>, such as Low-Rank Adaptation (LoRA) <d-cite key="hu2021lora"></d-cite>, in the Supervised Fine-Tuning (SFT) phase to enhance instruction following and alignment with human preference. Furthermore, LLMs constructed with a Mixture of Experts (MoE) <d-cite key="shen2023mixture"></d-cite> <d-cite key="jiang2024mixtral"></d-cite> have attracted increasing attention due to the reduced computational cost of the sparse architecture.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0" id="figure-5"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mllm_primer_post_microcontroller-480.webp 480w,/assets/img/mllm_primer_post_microcontroller-800.webp 800w,/assets/img/mllm_primer_post_microcontroller-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/mllm_primer_post_microcontroller.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Fig-5 A schematic diagram of microcontrollers, where the central unit CPU interacts with various components to process data, control and communicate with other devices. Image Source: <a href="https://www.hnhcart.com/blogs/microcontrollers/what-are-the-major-factors-on-which-microcontrollers-are-differentiated" target="_blank">The Major Factors On Which The Microcontrollers Are Differentiated</a>. </div> <p>Compared to human-like text generation, the emergent capabilities of LLMs, such as ICL and CoT reasoning, unlock the vast potential of machines in handling complex and flexible human instructions. When LLMs are extended with interfaces to sense and interact with the physical world, i.e., MLLMs, they can be analogized to microcontrollers, provided disregarding differences in energy consumption and application scenarios. A microcontroller is essentially a small computer, comprising a processor core, memory, and input/output (I/O) peripherals. Similarly, LLMs act as the processor core, executing human instructions rather than pre-programmed commands. Modality encoders and decoders enable LLMs to interact with the external world, analogous to ADCs or Digital-to-Analog Converters (DACs). The key distinction between microcontrollers and MLLMs lies in the latter’s capability for learning and adaptation, ushering in a new era where machines can continuously evolve through direct interaction with humans. Human language is distinguished by its complexity, generativity, and ability to convey abstract and displaced concepts, which supports the language-centric design pattern of MLLMs. However, this design is yet to be definitively validated, as discussed in Chapter 4 of <d-cite key="li2023multimodal"></d-cite>, particularly considering the distinct differences between vision and language.</p> <p><strong>Modality Connectors</strong> Due to the noticeable gap between embeddings of natural languages and other modalities, particularly considering the difference in bandwidth, it is essential to establish reliable alignment with reference to textual embeddings to enable LLMs to understand sensory inputs. This alignment involves projecting embeddings from modality encoders into a space comprehensible to LLMs, ensuring unambiguous interpretation. This can be achieved using a trainable modality adapter or connector, bridging the frozen modality encoder and the LLM, which is computationally efficient and more accessible.</p> <p>Currently, multimodal alignment can be achieved at either the token- or feature-level. At the token-level, modality-specific embeddings are converted into corresponding tokens, which are then concatenated with textual tokens and fed into LLMs for SFT. The modules commonly used for modality conversion include Q-Former <d-cite key="li2023blip"></d-cite> <d-cite key="zhang2023video"></d-cite>, MLP <d-cite key="liu2024visual"></d-cite> <d-cite key="liu2023improved"></d-cite>, and cross-attention layers <d-cite key="alayrac2022flamingo"></d-cite>. In contrast to lightweight modality adapters, a pre-trained LLaMA is utilized as the modality connector in InternVL <d-cite key="chen2023internvl"></d-cite>, acting as a language middleware. This approach may allow for the utilization of frozen LLMs in the SFT stage. Feature-level fusion involves deep interaction between modality-specific and language features, usually achieved by inserting extra trainable modules into a frozen LLM <d-cite key="alayrac2022flamingo"></d-cite> <d-cite key="zhang2023llama"></d-cite>, which may compromise the innate language capabilities.</p> <p>The difficulty in multimodal alignment may stem from the next-token prediction loss used in optimizing LLMs, which may favor an over-reliance on the LLM’s parametric knowledge, as discussed in <d-cite key="zhai2023halle"></d-cite>, rather than faithfully grounding predictions in multimodal signals. Consequently, this can lead to multimodal hallucinations, where the model generates outputs that are inconsistent with the concrete content contained in sensory inputs. Another factor significantly influencing the generation of LLMs is the adopted decoding strategies. Recent work <d-cite key="leng2023mitigating"></d-cite> has shown that multimodal hallucination can be alleviated by adapting advanced decoding methods, such as contrastive decoding <d-cite key="li2022contrastive"></d-cite>, for MLLMs as a training-free corrective mechanism. Apart from using modality connectors, another viable solution is to textualize non-language signals using expert models, as exemplified by the VideoChat-Text model <d-cite key="li2023videochat"></d-cite>, though this approach comes with the downsides of information loss and a pipeline that is not end-to-end optimized.</p> <p><strong>Modality Decoders</strong> In addition to the aforementioned three compulsory modules, MLLMs can be further extended with modality-specific decoders, particularly in the Any-to-Any workflow, to overcome the limitation of MLLMs only perceiving multimodal data.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0" id="figure-6"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mllm_primer_post_nextgpt-480.webp 480w,/assets/img/mllm_primer_post_nextgpt-800.webp 800w,/assets/img/mllm_primer_post_nextgpt-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/mllm_primer_post_nextgpt.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Fig 6 An overview of an Any-to-Any multimodal system NExT-GPT that can perceive inputs and generate outputs in arbitrary combinations of text, images, videos, and audio. Image Source: <a href="https://arxiv.org/abs/2309.05519" target="_blank">NExT-GPT: Any-to-Any Multimodal LLM</a>. </div> <p>MLLMs with multimodal responsive capabilities have recently spurred a surge of interest in the area of Generative AI, regarded as a significant step towards Artificial General Intelligence (AGI). For image generation, recent works such as Emu <d-cite key="sun2023generative"></d-cite> and Mini-Gemini <d-cite key="li2024mini"></d-cite> default to using diffusion-based generative models <d-cite key="luo2022understanding"></d-cite> <d-cite key="ribeiro2024demystifying"></d-cite> <d-cite key="chan2024tutorial"></d-cite>, such as the Stable Diffusion family <d-cite key="rombach2022high"></d-cite>, due to their unparalleled text-to-image generation performance. The work NExT-GPT <d-cite key="wu2023next"></d-cite> further extends LLMs to include image, audio, and video diffusion models, enabling content creation in arbitrary combinations of image, text, audio, and video. Similar to the modality connectors used in the encoding phase, it is reasonable to bridge the gap between textual embeddings generated by LLMs and those used as conditional guidance in diffusion models, which can be achieved through decoder-side alignment.</p> <h3 id="training-strategies-and-data">Training Strategies and Data</h3> <p>The training process of MLLMs can be decomposed into three stages: 1) pre-alignment, 2) instruction tuning, 3) alignment with human preferences. The fulfilment of training objective in each phase heavily relies on specific data, necessitating cost-effective data scaling-up methods and high quality data.</p> <p><strong>Pre-Alignment</strong> The pre-alignment stage often requires an enormous amount of text-paired data, such as images, videos, or audio files with associated textual descriptions, usually gathered from the Internet. However, the innate nature of LLMs as generative pre-trained models (next-token-prediction) casts doubt on the validity of using noisy and short text-paired data, which is originally used for contrastive alignment. As demonstrated in works such as ShareGPT4V <d-cite key="chen2023sharegpt4v"></d-cite>, both the pre-alignment and SFT stages significantly benefit from highly descriptive caption data. Additionally, the progressive alignment scheme adopted in InternVL <d-cite key="chen2023internvl"></d-cite>, transitioning from vision-language contrastive learning to generative pre-training, has proven to be more effective in accommodating the heterogeneous sources of pre-training data.</p> <p><strong>Instruction Tuning</strong> Instruction tuning (IT) <d-cite key="zhang2023instruction"></d-cite> aims to steer LLMs to respond more faithfully to human instructions by fine-tuning on instruction following datasets, which consist of (Instruction, Input, Output) triplets. This technique has proven to be effective and computationally efficient in enhancing the controllability of LLMs’ output behavior, thereby eliciting knowledge from LLMs that is well-aligned with human intents. Furthermore, IT has been identified as a critical factor in unlocking the few-shot (ICL) or zero-shot generalization capability of LLMs on solving novel tasks.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0" id="figure-7"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mllm_primer_post_self_instruct-480.webp 480w,/assets/img/mllm_primer_post_self_instruct-800.webp 800w,/assets/img/mllm_primer_post_self_instruct-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/mllm_primer_post_self_instruct.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Fig 7 An illustration of the overall process of Self-Instruct. Image Source: <a href="https://arxiv.org/abs/2212.10560" target="_blank">SELF-INSTRUCT: Aligning Language Models with Self-Generated Instructions</a>. </div> <p>The current trend in scaling up IT data involves leveraging semi-automatic data engines to minimize human intervention. Specifically, the majority of IT datasets are constructed using a paradigm known as Self-Instruct <d-cite key="wang2022self"></d-cite>, where a few manually-crafted seed examples serve as demonstrations for LLMs, such as GPT-4, to generate additional similar samples by harnessing the ICL capability. This approach can be extended to generate multimodal IT data. For instance, in the pioneering work LLaVA <d-cite key="liu2024visual"></d-cite>, images are converted into text descriptions and bounding box coordinates, thereby endowing the text-only GPT-4 with an imaginary vision capability, which are then combined with diverse task prompts to create visual instruction-following datasets. However, IT data built on top of GPT-4 (text-only LLMs) often suffer from multimodal hallucination, compromising the quality of the generated samples, which can be remedied with the advent of powerful MLLMs. For instance, in the work ALLaVA <d-cite key="chen2024allava"></d-cite>, GPT-4V, a multimodal variant of GPT-4, is utilized to create high-quality IT data by following a captioning-questioning-answering pipeline. Moreover, recent works, such as ALLaVA <d-cite key="chen2024allava"></d-cite> and Lynx <d-cite key="zeng2023matters"></d-cite>, emphasize that the quality of IT datasets—specifically the complexity and diversity of instructions and the detail of responses—is more crucial than their quantity.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0" id="figure-8"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mllm_primer_post_data_engine-480.webp 480w,/assets/img/mllm_primer_post_data_engine-800.webp 800w,/assets/img/mllm_primer_post_data_engine-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/mllm_primer_post_data_engine.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Fig 8 A schematic diagram of semi-automatic data engine. </div> <p>Despite the promising performance gains achieved through IT, there have been criticisms regarding its efficacy. For instance, it has been shown that instruction-tuned LLMs merely mimic the style of proprietary models like ChatGPT and fall short in those critical dimensions such as factuality, coding, and problem solving <d-cite key="kung2023models"></d-cite> <d-cite key="gudibande2023false"></d-cite>, which has led to the conclusion that bridging the capabilities gap between open-source LLMs and proprietary ones through IT is considered a false promise <d-cite key="gudibande2023false"></d-cite>. Nevertheless, leveraging a scalable semi-automatic data engine-an iteratively refined pseudo data labelling/generation pipeline with human in the loop-is a key enabler for advancing MLLMs. For example, the All-Seeing Project <d-cite key="wang2023all"></d-cite>, a recent effort in building an open-world visual recognition and understanding model, created approximately 1 billion instance-level annotations of open-world visual concepts with detailed captions and question-answer pairs, which would have been prohibitive without the assistance of semi-automatic data engines.</p> <p><strong>Preference Alignment</strong> Alignment with human preferences aims to further refine the output behaviour of MLLMs that have undergone SFT or instruction tuning by leveraging human/AI feedback on model responses. The two mainstream solutions currently in use are Reinforcement Learning with Human Feedback (RLHF) <d-cite key="ziegler2019fine"></d-cite> <d-cite key="ouyang2022training"></d-cite> and Direct Preference Optimization (DPO) <d-cite key="rafailov2024direct"></d-cite>.</p> <h3 id="evaluation">Evaluation</h3> <p>The evaluation of MLLMs is generally categorized into closed-ended and open-ended questions. For open-ended questions, the assessment criteria may involve human or GPT scoring. In contrast to evaluation methods developed before the era of LLMs, designing evaluation toolkits for MLLMs demands attention and efforts comparable to or even surpass those required for model development. As pointed out in <d-cite key="gudibande2023false"></d-cite>, human evaluators without domain expertise or significant time investment can be easily deceived by LLMs due to their fluent, confident, and well-structured responses. Therefore, acquiring quantitative measurements that can objectively and reliably reflect the multifaceted aspects of MLLMs is crucial, as these measurements facilitate pinpointing critical factors contributing to improvements along specified skill dimensions, identifying failure cases, and providing deeper insights into the inner workings of MLLMs.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0" id="figure-9"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mllm_primer_post_ai_dev_cycle-480.webp 480w,/assets/img/mllm_primer_post_ai_dev_cycle-800.webp 800w,/assets/img/mllm_primer_post_ai_dev_cycle-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/mllm_primer_post_ai_dev_cycle.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Fig 9 An illustration of AI development cycle: a dynamic and iterative process where data creation, model development, and model assessment continuously inform and improve each other. </div> <p>We should be wary of increasingly potent MLLMs that can take shortcuts to inflate performance metrics and deliver delusive results, which requires researchers to act as adversaries, critically analyzing and probing the models’ responses for weaknesses. Consequently, evaluation frameworks that combine both human and AI efforts and can continuously evolve offer promising avenues to mitigate these issues, as exemplified in Dynabench <d-cite key="kiela2021dynabench"></d-cite>, where dataset creation, model development, and model evaluation co-evolve and reinforce each other, serving as a counterbalance to the phenomenon where state-of-the-art AI systems quickly saturate benchmarks but fail on real-world challenges.</p> <h2 id="concluding-remarks">Concluding Remarks</h2> <p>The development of LLMs and their extension to multimodal understanding (MLLMs) reveals a promising pathway for machines to approach human-level intelligence: by actively interacting with humans and engaging in human daily activities, reminiscent of how we nurture and educate our offspring. In essence, the emergence of MLLMs marks a watershed moment in the evolution of machine learning systems, ushering in the era of Human-Centered AI (HCAI), which begins with the creation of a more natural and intuitive human-machine interaction interface. Reflecting on the saying of Richard Feynman that “What I cannot create, I do not understand,” we can anticipate significant breakthroughs in neuroscience when AI systems can faithfully emulate human cognitive capabilities by generating responses indistinguishable from those of humans. The development of AI assistants akin to Doraemon can provide profound insights into the neural and cognitive mechanisms underlying human intelligence and foster a closer synergy between neuroscience and AI research. While Norbert Wiener’s saying, “The best material model of a cat is another, or preferably the same, cat,” underscores the immense challenge of precisely simulating the complexity and intricacies inherent in biological systems with artificial models, it is exciting to contemplate the potential of an increasingly close collaborative relationship between humans and AI.</p> <h2 id="acknowledgement">Acknowledgement</h2> <p>This blog post draws significant inspiration from two outstanding works reviewing the latest advances in MLLMs:</p> <ol> <li> <p>“A Survey on Multimodal Large Language Models” <d-cite key="yin2023survey"></d-cite>, which provides a timely and comprehensive overview of recent progress in MLLMs, supplemented by a continuously updated GitHub webpage: <a href="https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models">Awesome-Multimodal-Large-Language-Models</a>.</p> </li> <li> <p>“Multimodal Foundation Models: From Specialists to General-Purpose Assistants” <d-cite key="li2023multimodal"></d-cite>, where leading researchers in the field present an insightful dissection, offering both a historical perspective on the development of multimodal foundation models and an inspiring vision for their future.</p> </li> </ol> <p>Additionally, the completion of this blog post was assisted by OpenAI ChatGPT in refining phrasing, expression, and tone.</p> <div class="language-html highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@misc{Cai2024mllm_primer,
    author = {Xin Cai},
    title = {A Primer on Multimodal Large Language Models: Towards Building Doraemon-like AI Assistants},
    howpublished = {\url{https://totalvariation.github.io/blog/2024/a-primer-on-mllms/}},
    note = {Accessed: 2024-05-20},
    year = {2024}
}
</code></pre></div></div>]]></content><author><name>Xin Cai</name></author><category term="MLLMs,"/><category term="LVLMs,"/><category term="General"/><category term="Purpose"/><category term="Visual"/><category term="Assistants,"/><category term="GenAI"/><summary type="html"><![CDATA[Towards Building Doraemon-like AI Assistants]]></summary></entry><entry><title type="html">Random Thoughts on Open World Vision Systems</title><link href="https://totalvariation.github.io/blog/2024/open-world-vision-systems/" rel="alternate" type="text/html" title="Random Thoughts on Open World Vision Systems"/><published>2024-02-01T00:00:00+00:00</published><updated>2024-02-01T00:00:00+00:00</updated><id>https://totalvariation.github.io/blog/2024/open-world-vision-systems</id><content type="html" xml:base="https://totalvariation.github.io/blog/2024/open-world-vision-systems/"><![CDATA[<blockquote> "It is not the strongest of the species that survives, nor the most intelligent that survives. It is the one that is most adaptable to change." - Charles Darwin </blockquote> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ante-hamersmit-qM8zX1celvc-unsplash-480.webp 480w,/assets/img/ante-hamersmit-qM8zX1celvc-unsplash-800.webp 800w,/assets/img/ante-hamersmit-qM8zX1celvc-unsplash-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/ante-hamersmit-qM8zX1celvc-unsplash.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Photo by <a href="https://unsplash.com/@ante_kante?utm_content=creditCopyText&amp;utm_medium=referral&amp;utm_source=unsplash">Ante Hamersmit</a> on <a href="https://unsplash.com/photos/person-holding-reptile-qM8zX1celvc?utm_content=creditCopyText&amp;utm_medium=referral&amp;utm_source=unsplash">Unsplash</a> </div> <p>Over the last decade, there has been remarkable progress in visual perception algorithms, driven by the development of layered differentiable models optimized in an end-to-end fashion. Despite these advancements, the deployment of such algorithms in open-world scenarios poses significant challenges. To effectively operate in open-world settings, these algorithms must incorporate capabilities for open-set recognition and open vocabulary learning, while also being robust to distribution shifts. Moreover, Open World Visual Understanding Systems (OWVUS) are expected to handle continuous streams of real-world data, adapting to non-stationary environments with minimal labelling requirements. This suggests the need for semi- or fully-automatic data engines that facilitate model learning alongside incremental data curation and labelling, potentially assisted with human intervention. The foundational pillars of building OWVUS lie in data and annotation efficiency, robustness, and generalization. These challenges can potentially be addressed through a unified framework known as Open-set Unsupervised Domain Adaptation (OUDA).</p> <p>In general, visual signals can be decomposed into low- and high-frequency components. The former enables the unlocking of open-vocabulary capabilities in Vision-Language Models (VLMs) <d-cite key="radford2021learning"></d-cite> <d-cite key="li2021align"></d-cite> by establishing a well-aligned visual-semantic space. Conversely, the latter plagues the generalizability of deep neural networks on unseen or novel domains, as they may overfit to specific domain styles and therefore capture spurious correlations. OUDA serves as a nexus that connects a broad spectrum of research fields, including generative modelling <d-cite key="hoffman2018cycada"></d-cite> <d-cite key="ilse2020diva"></d-cite> <d-cite key="mahajan2020latent"></d-cite>, semi-supervised learning <d-cite key="berthelot2021adamatch"></d-cite> <d-cite key="zhang2020label"></d-cite>, meta-learning <d-cite key="shu2021open"></d-cite> <d-cite key="kim2022pin"></d-cite> <d-cite key="zhao2021learning"></d-cite>, open-set recognition <d-cite key="saito2021ovanet"></d-cite> <d-cite key="saito2020universal"></d-cite>, open-vocabulary learning <d-cite key="yu2023open"></d-cite> <d-cite key="zara2023autolabel"></d-cite> <d-cite key="wu2023towards"></d-cite>, out-of-distribution detection <d-cite key="shu2023clipood"></d-cite> <d-cite key="wang2023clipn"></d-cite>, few-/zero-shot learning <d-cite key="wu2022style"></d-cite> <d-cite key="fahes2023poda"></d-cite>, contrastive learning <d-cite key="da2022dual"></d-cite> <d-cite key="sahoo2021contrast"></d-cite> <d-cite key="zara2023simplifying"></d-cite>, unsupervised representation learning <d-cite key="hoyer2023mic"></d-cite> <d-cite key="vray2024distill"></d-cite>, active learning <d-cite key="zhang2022bamboo"></d-cite> <d-cite key="wu2022entropy"></d-cite>, continual learning <d-cite key="atanyan2024continuous"></d-cite> <d-cite key="lin2022prototype"></d-cite> <d-cite key="wang2302comprehensive"></d-cite>, and disentanglement of factors of variation <d-cite key="wu2022single"></d-cite> <d-cite key="wei2022unsupervised"></d-cite>. Its primary objective is to transfer knowledge from annotated source domains or pre-trained models (in the case of source-free UDA <d-cite key="liang2020we"></d-cite>) to unlabelled target domains. This transfer process requires addressing both covariate and semantic shifts, which correspond to variations in the high- and low-frequency components of visual signals, respectively. In essence, OUDA leverages methods developed within the aforementioned research areas to combat covariate or semantic shifts. On the other hand, addressing distribution shifts becomes unavoidable when extending any research problem from these areas to a more general and practical context.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0" id="figure-1"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/open_world_vision_post_uda_methods-480.webp 480w,/assets/img/open_world_vision_post_uda_methods-800.webp 800w,/assets/img/open_world_vision_post_uda_methods-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/open_world_vision_post_uda_methods.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Fig-1 The taxonomy of UDA methods. </div> <p>OUDA integrates open-set recognition/open-vocabulary learning and domain adaptation/generalization within a unified framework, aiming to address both high-level semantic shift and low-level covariate shift simultaneously and therefore presenting compounded challenges that stem from both research domains. Recent advancements have demonstrated rapid progress in constructing sophisticated open-vocabulary detectors or segmentors <d-cite key="zhu2023survey"></d-cite>, facilitated by VLMs trained on web-scale image-text pairs which offer a comprehensive prior of the real-world. Central to this advancement is the endeavour to bridge the granularity gap between coarse-grained visual-semantic associations and fine-grained visual understanding objectives. Predominant solutions can fall into three main categories: (1) incorporating fine-grained awareness into pre-training recipes <d-cite key="li2022grounded"></d-cite> <d-cite key="zhong2022regionclip"></d-cite> <d-cite key="rao2022denseclip"></d-cite>; (2) transferring knowledge from VLMs to downstream fine-grained visual understanding tasks <d-cite key="gu2021open"></d-cite> <d-cite key="kuo2022f"></d-cite> <d-cite key="wu2023cora"></d-cite> <d-cite key="he2023clip"></d-cite>; (3) the amalgamation of Vision Foundation Models (VFMs) by leveraging their complementary expertise resulting from distinct pretraining objectives <d-cite key="han2023boosting"></d-cite> <d-cite key="wang2023sam"></d-cite>. Besides, the pursuit of handling diverse vision tasks with a unified architecture and a single suite of weights in the open-set scenario <d-cite key="zou2023generalized"></d-cite> <d-cite key="zhang2023simple"></d-cite> has garnered increasing attention as a step towards constructing general-purpose vision foundation models.</p> <p>In the emerging task of Open-World Object Detection (OWOD) <d-cite key="joseph2021towards"></d-cite> <d-cite key="gupta2022ow"></d-cite> <d-cite key="wang2023detecting"></d-cite>, which combines open-set recognition with class incremental learning, the inherent open-vocabulary capability of VLMs offers convenience in identifying unknown classes. However, specialized components remain indispensable for the discovery of novel classes. Essentially, <strong>equipping a neural network with the ability to say NO when facing unfamiliar input</strong>, even with models like CLIP <d-cite key="wang2023clipn"></d-cite>, presents significant challenges. Particularly in vision tasks, developing a class-agnostic object localizer capable of generalizing to novel classes remains an open question <d-cite key="kim2022learning"></d-cite>. This challenge proves critical for two-stage open-world detectors or segmentors, as the generation of high-quality proposals for unknown classes is pivotal. Recently, there is a promising trend where class-agnostic object discovery is tackled without requiring any manual annotation by leveraging pre-trained features of self-supervised Vision Transformers (ViTs) <d-cite key="simeoni2023unsupervised"></d-cite>. However, these algorithms still struggle in complex scene-centric scenarios. OUDA introduces a more complicated pipeline compared to UDA with a close-set assumption, requiring the detection or rejection of unknown classes followed by cross-domain alignment with dominant solutions illustrated in <a href="#figure-1">Fig-1</a>. It has been demonstrated that overlooking unknown classes during domain alignment can lead to negative transfer or even catastrophic misalignment. As far as I know, existing methods for open-set recognition or novel class discovery largely rely on heuristic approaches, such as one-vs-all classifiers <d-cite key="saito2021ovanet"></d-cite>, entropy-based separation <d-cite key="saito2020universal"></d-cite>, inter-class distance or margin-based methods <d-cite key="miller2021class"></d-cite>, and leveraging zero-shot predictions from VLMs <d-cite key="yu2023open"></d-cite> <d-cite key="zara2023autolabel"></d-cite>. Furthermore, effectively separating (target-)private samples into semantically coherent clusters <d-cite key="zara2023autolabel"></d-cite>, rather than treating them indiscriminately as a generic unknown class, presents an even more formidable challenge, requiring the utilization of intrinsic structures within unseen or novel classes.</p> <p>Scaling up pre-training data with minimal human intervention has proven to be critical to foundation models, such as GLIP <d-cite key="li2022grounded"></d-cite> and SAM <d-cite key="kirillov2023segment"></d-cite>. Particularly in scenarios where manual annotation is resource-intensive <d-cite key="delatolas2024learning"></d-cite>, there’s a pressing need for an automatic data annotation framework. Such a framework should not only generate reliable pseudo labels but also continually expand the concept pool, thereby necessitating resilience to domain shifts stemming from heterogeneous data sources and open-set recognition capability. In the context of video action recognition, this task is referred to as Open-set Video Domain Adaptation (OUVDA) <d-cite key="zara2023simplifying"></d-cite> <d-cite key="zara2023autolabel"></d-cite>, which remains largely unexplored. This emerging research direction is inherently more complex due to the additional temporal dimension and the scarcity of large-scale and diverse datasets, presenting unique challenges that warrant further investigation. The closed learning loop, which involves the simultaneous evolution of model updating and dataset expansion, lays the groundwork for OWVUS capable of continual self-development over time. From a data-centric standpoint, the challenge revolves around constructing a dynamic dataset capable of consistently absorbing novel semantic categories and selecting relevant samples continually <d-cite key="de2021continual"></d-cite> and actively <d-cite key="zhang2022bamboo"></d-cite> with human-machine synergy. Continual learning <d-cite key="wang2302comprehensive"></d-cite>, characterized by rapid adaptation to evolving data distributions and the potential encounter with unseen classes while avoiding catastrophic forgetting, can thus be integrated with OUDA to fulfil this objective.</p> <p>To conclude, the prospect of unfolding possibilities and burgeoning potential in the field of OUDA and its synergies with other areas like continual learning and active learning fills me with anticipation and enthusiasm.</p> <div class="language-html highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@misc{Cai2024OpenWorld,
    author = {Xin Cai},
    title = {Random Thoughts on Open World Vision Systems},
    howpublished = {\url{https://totalvariation.github.io/blog/2024/open-world-vision-systems/}},
    note = {Accessed: 2024-02-08},
    year = {2024}
}
</code></pre></div></div>]]></content><author><name>Xin Cai</name></author><category term="Open-set"/><category term="Open-vocabulary"/><category term="UDA"/><summary type="html"><![CDATA["It is not the strongest of the species that survives, nor the most intelligent that survives. It is the one that is most adaptable to change." - Charles Darwin]]></summary></entry></feed>