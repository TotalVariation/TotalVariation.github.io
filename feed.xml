<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://totalvariation.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://totalvariation.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-10-29T09:10:09+00:00</updated><id>https://totalvariation.github.io/feed.xml</id><title type="html">Xin’s Homepage</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Reconstructing the Scaffolding A Learner’s Notes on Ordinary Differential Equations</title><link href="https://totalvariation.github.io/blog/2025/concise-intro-odes-part1/" rel="alternate" type="text/html" title="Reconstructing the Scaffolding A Learner’s Notes on Ordinary Differential Equations"/><published>2025-10-28T00:00:00+00:00</published><updated>2025-10-28T00:00:00+00:00</updated><id>https://totalvariation.github.io/blog/2025/concise-intro-odes-part1</id><content type="html" xml:base="https://totalvariation.github.io/blog/2025/concise-intro-odes-part1/"><![CDATA[<div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/phase_portrait_predator_prey-480.webp 480w,/assets/img/phase_portrait_predator_prey-800.webp 800w,/assets/img/phase_portrait_predator_prey-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/phase_portrait_predator_prey.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> The phase portrait of Lotka Volterra equations. </div> <p> In this blog, I will present detailed mathematical proofs of the Picard–Lindelo&#776;f theorem and the Cauchy–Peano existence theorem. This post is part of my ongoing series of learning notes on Ordinary Differential Equations (ODEs), written with the primary goal of strengthening my own understanding and practising the analytical tools I have learned in real analysis and matrix analysis. It would be my pleasure if anyone finds this blog even slightly helpful in their own study of ODEs. </p> <p> In contrast to more general treatments <d-footnote>We warmly recommend the <a href="https://sites.math.washington.edu//~burke/crs/555/555_notes/index.html">lecture notes</a> provided by Prof. James Burke to readers seeking a more mathematically rigorous treatment of ODE theories.</d-footnote>, where an ordinary differential equation is expressed as an $ m^{th} $-order $ n \times n $ system $ f(t, \mathbf{x}, \mathbf{x}', \ldots, \mathbf{x}^{(m)}) = 0 $ with $ f $ mapping a subset of $ \mathbb{R} \times (\mathbb{F}^n)^{m+1} $ into $ \mathbb{F}^n $. I will begin with the single-variable case, then move on to vector-valued first-order ODEs, and finally showcase its usage in proving the existence and uniqueness theorem of the general n-th order ODEs and their linear counterparts as supplementary exercises. </p> <h2> Picard-Lindelo&#776;f Theorem </h2> <p> Consider the initial value problem (IVP) of the form : $$ \frac{dy}{dt} = f(t,y), y(t_0) = y_0 $$ where $ f(t, y) $ is a function defined in some region of the $ (t, y) $-plane containing the initial point $ (t_0, y_0) $. If $ f(t, y) $ is continuous in $ t $ and Lipschitz continuous in $ y $ in a rectangular region $ R = \{ (t, y) : |t - t_0| \le a, |y - y_0| \le b \} $, where $ a $ and $ b $ are positive constants. Then, there exists a unique solution $ y(t) $ to the IVP, defined on some interval $ [t_0 - h, t_0 + h] $ for some $ h &gt; 0 $. </p> <p> <strong>Lipschitz Continuity</strong> A function $ f(t, y) $ is Lipschitz continuous in $ y $ if there exists a constant $ L_f &gt; 0 $ (called the Lipschitz constant) such that: $$ |f(t, y_1) - f(t, y_2)| \leq L_f |y_1 - y_2| $$ for all $ (t, y_1), (t, y_2) \in R $. It’s stronger than continuity but weaker than differentiability—most differentiable functions with bounded derivatives are Lipschitz continuous. </p> <p><strong>Proof:</strong></p> <p> <em>Step 1:</em> The IVP $ \frac{dy}{dt} = f(t, y) $, $ y(t_0) = y_0 $ is equivalent to the integral equation by the Fundamental Theorem of Calculus $ y(t) = y_0 + \int_{t_0}^t f(s, y(s)) \, ds $. </p> <p> <em>Step 2:</em> Picard's Iteration: Define a sequence of functions $ \{ y_n(t) \} $ to approximate the solution: Start with $ y_0(t) = y_0 $ (the constant initial condition). For $ n \geq 0 $, define: $$ y_{n+1}(t) = y_0 + \int_{t_0}^t f(s, y_n(s)) \, ds $$ </p> <p> <em>Step 3:</em> Pick $ h \leq \min\left\{a, \frac{b}{M} \right\} $ such that the sequence of functions $ \{ y_n(t) \} $ is well-defined on the interval $ [t_0 - h, t_0 + h] $, where $$ M = \sup_{(t, y) \in R} |f(t, y)| &lt; \infty $$ since $ f $ is continuous and $ R $ is bounded (compact). Specifically, $ |y(t) - y_0| \leq \int_{t_0}^t |f(s, y(s))| \, ds \leq Mh &lt; b $. </p> <p> <em>Step 4:</em> Use Banach Fixed-Point Theorem to prove convergence. Define the operator $ T $ on the space of continuous functions $ \mathcal{C}[t_0 - h, t_0 + h] $ with the supremum norm $ \|y\|_{\infty} = \sup_{t} |y(t)| $, guaranteeing a complete metric space (i.e., every Cauchy sequence is convergent): $$ T \left[ y \right]\left(t \right) = y_0 + \int_{t_0}^t f(s, y(s)) \, ds. $$ We’ll show $ T $ is a contraction mapping, meaning there exists $ 0 &lt; \lambda &lt; 1 $ such that: $ \| T[y] - T[z] \|_{\infty} \le \lambda \| y - z \|_{\infty} $. The Banach fixed-point theorem then guarantees the sequence $ y_{n+1} = T[y_n] $ converges to a unique fixed point $ y = T[y] $, satisfying the integral equation and hence the IVP. For any $ y, z \in C[t_0 - h, t_0 + h] $: $$ |T[y](t) - T[z](t)| = \left| \int_{t_0}^t [f(s, y(s)) - f(s, z(s))] \, ds \right| \leq \int_{t_0}^t |f(s, y(s)) - f(s, z(s))| \, ds $$ Using the Lipschitz condition: $$ |f(s, y(s)) - f(s, z(s))| \leq L_f |y(s) - z(s)| \leq L_f \| y - z \|_{\infty} $$ Thus, $$ \begin{align*} |T[y]\left(t \right) - T[z]\left(t \right)| &amp;\le \int_{t_0}^t L_f \| y - z \|_{\infty} \, ds = L_f \| y - z \|_{\infty} |t - t_0| \\ &amp;\le L_f h \| y - z \|_{\infty} \; \text{since} \: |t - t_0| \le h. \end{align*} $$ Taking the supremum: $ \| T[y] - T[z] \|_{\infty} \leq L_f h \| y - z \|_{\infty} $. If $ L_f h &lt; 1 $ (e.g., choosing $ h &lt; \min\{ a, \frac{b}{M}, \frac{1}{L_f} \} $), then $ \lambda = L_f h &lt; 1 $, and $ T $ is a contraction mapping. </p> <p> The single variable case of Picard-Lindelo&#776;f Theorem can be easily generalised to vector-valued ODEs: $$ \frac{d\mathbf{x}}{dt} = f(t, \mathbf{x}), \quad \mathbf{x}(0) = \mathbf{x}_0, $$ where $ t \in [0, T] $ for some fixed $ T &gt; 0 $, $ \mathbf{x}(t) \in \mathbb{R}^d $, $ f = (f_1, \ldots, f_d) : D \subset [0, T] \times \mathbb{R}^d \to \mathbb{R}^d $ is continuous and Lipschitz continuous with respect to $ \mathbf{x} $, i.e., $$ \| f(t, \mathbf{x}) - f(t, \mathbf{y}) \| \leq L_f \| \mathbf{x} - \mathbf{y} \|, \quad \forall (t, \mathbf{x}), (t, \mathbf{y}) \in D, $$ $ \mathbf{x}_0 \in \mathbb{R}^d $ is the initial condition at $ t = 0 $.<d-footnote>Technically, this is a forward time version. A backward-in-time version (on $ [t_0 - a, t_0] $) can be proved in a similar fashion by simply using $ \widetilde{x}(t) = x(c - t) $, and correspondingly $ \widetilde{f}(t) = -f(c-t, x) $.</d-footnote> A solution can be viewed as a parametric curve in an n-dimensional space while $ t $ changing in the interval $ [0, T] $. It can also be viewed as a trajectory of a particle moving in an n-dimensional space with its velocity (i.e., the tangent vector of the path) specified as per $ f(t, \mathbf{x}) $. </p> <p> The mathematical proof is almost identical to that of the single variable case, provided the vector norm $ \| \cdot \| $ is used instead to replace the absolute value $ | \cdot | $. </p> <p> <em>Step 1:</em> Construct a compact set $ R = [0, h] \cap [0, T] \times \overline{B}(\mathbf{x}_0, r) $ (hypercylinder) and a contraction operator $$ \Phi[\mathbf{x}]\left(t \right) = \mathbf{x}_0 + \int_{t_0}^t f(s, \mathbf{x}(s))ds, $$ where $ h \leq \frac{r}{M} $ and $ M = \sup_{ (t, \mathbf{x}) \in R} \| f(t, \mathbf{x}) \| &lt; \infty $, ensuring $ \| \Phi[\mathbf{x}]\left(t \right) - \mathbf{x}_0 \| \leq r $ for all $ t \in [0, h] $. </p> <p> <em>Step 2:</em> $$ \| \Phi[\mathbf{x}](t) - \Phi[\mathbf{y}](t)\| = \| \int_0^t \left[ f(s, \mathbf{x}(s)) - f(s, \mathbf{y}(s))ds \right] \| \leq \int_0^t L_f \| \mathbf{x}(s) - \mathbf{y}(s) \| ds $$ Define the supremum norm on $ \mathcal{C}([0, T];\mathbb{R}^d) $ as $ \| \mathbf{x} \|_{\infty} = \sup_t \| \mathbf{x}(t) \| $, and then take $ \sup $ gives $$ \| \Phi[\mathbf{x}] - \Phi[\mathbf{y}] \|_{\infty} \le L_f h \| \mathbf{x} - \mathbf{y} \|_{\infty}. $$ Thus if we choose $ h &lt; \frac{1}{L_f} $, then $ \Phi[\cdot] $ is a contraction mapping. </p> <p> <em>Step 3:</em> By Banach Fixed-Point Theorem, $ \Phi $ has a unique fixed point $ \mathbf{x} $ such that: $$ \mathbf{x}(t) = \mathbf{x}_0 + \int_0^t f(s, \mathbf{x}(s))ds, \quad t \in [0, h], \; \text{where} \; h = \min \{ \frac{r}{M}, \frac{1}{L_f} \} $$ </p> <p>In the following, we give another proof where the property of uniform convergence is used.</p> <p> In the Picard's Iteration $ \mathbf{x}_0(t) \equiv \mathbf{x}_0, \mathbf{x}_{n+1}(t) = \mathbf{x}_0 + \int_0^t f(s, \mathbf{x}_n(s)) ds $ $$ \begin{align*} \| \mathbf{x}_1(t) - \mathbf{x}_0(t) \| &amp;= \| \int_0^t f(s, \mathbf{x}_0(s)) ds \| \leq Mt \\ \| \mathbf{x}_2(t) - \mathbf{x}_1(t) \| &amp;= \| \int_0^t \left[ f(s, \mathbf{x}_1(s)) - f(s, \mathbf{x}_0(s)) ds \right] \| \leq \int_0^t L_f \| \mathbf{x}_1(s) - \mathbf{x}_0(s) \| ds \\ &amp;\leq M L_f \int_0^t s ds = M L_f \frac{t^2}{2}\\ \| \mathbf{x}_3(t) - \mathbf{x}_2(t) \| &amp;= \| \int_0^t \left[ f(s, \mathbf{x}_2(s)) - f(s, \mathbf{x}_1(s)) ds \right] \| \leq \int_0^t L_f \| \mathbf{x}_2(s) - \mathbf{x}_1(s) \| ds \\ &amp;\leq ML_f^2 \int_0^t \frac{s^2}{2} ds = ML_f^2 \frac{t^3}{3!}\\ &amp; \vdots \\ \| \mathbf{x}_{n}(t) - \mathbf{x}_{n-1}(t) \| &amp; \leq M L_f^{n-1} \frac{t^{n}}{n!} \leq M L_f^{n-1} \frac{h^{n}}{n!} \end{align*} $$ $$ \sum_{i=1}^{n} \| \mathbf{x}_{i}(t) - \mathbf{x}_{i-1}(t) \| \leq \frac{M}{L_f} \underbrace{\sum_{i=1}^n \frac{(KL_f)^i}{i !}}_{e^{L_f h} - 1} $$ </p> <p> By Weierstrass M-Test, series $ \sum_{i=1}^{n} \| \mathbf{x}_{i}(t) - \mathbf{x}_{i-1}(t) \| $ converges uniformly on $ [0, h] $, implying $ \| \mathbf{x}_{n}(t) \| \le \| \mathbf{x}_0(t) \| + \sum_{i=1}^{n} \| \mathbf{x}_{i}(t) - \mathbf{x}_{i-1}(t) \| $ converges uniformly to $ \mathbf{x}(t) $ on $ [0, h] $. As $ R $ is compact, $ f $ is continuous and thus uniform continuous, implying that $ f(t, \mathbf{x}_{n}(t)) $ converges uniformly to $ f(t, \mathbf{x}(t)) $. Furthermore, the uniform convergence guarantees $ \lim_{n} \int_a^b f_n(t) dt = \int_a^b f(t) dt $, where $ f_n \to f $ uniformly on $ R = [0, h] \cap [0, T] \times \overline{B}(\mathbf{x}_0, r) $ and $ f_n $ is integrable. </p> <p> Therefore, we can conclude that $$ \lim_n \mathbf{x}_{n+1}(t) = \mathbf{x}_0 + \lim_n \int_0^t f(s, \mathbf{x}_n(s)) ds = \mathbf{x}_0 + \int_0^t \lim_n f(s, \mathbf{x}_n(s)) ds $$ $$ \mathbf{x}(t) = \mathbf{x}_0 + \int_0^t f(s, \mathbf{x}(s)) ds $$ </p> <p> The uniqueness of the solution can be proved by Gro&#776;nwall's Lemma. Let $ u \in \mathcal{C}([0, T]) $. If there exist two constants $ \alpha, \beta \in \mathbb{R}, \beta \geq 0 $, such that $ u(t) \leq \alpha + \beta \int_0^t u(s) ds, \; t \in [0, T] $, then $ u(t) \leq \alpha e^{\beta t} $ for all $ t \in [0, T] $. </p> <p><strong>Proof:</strong></p> <p> Let $ v(t) := \alpha + \beta \int_0^t u(s) ds $. Then $ u \le v $ and $ v^{\prime}(t) = \beta u(t) \leq \beta v(t) $. Solve $ v^{\prime}(t) \le \beta v(t) $ to get $ u(t) \le v(t) \leq \alpha e^{\beta t} $. If there exist two solutions $ \mathbf{x}_1(t), \mathbf{x}_2(t) $ that both satisfy $ \mathbf{x}^{\prime}(t) = f(t, \mathbf{x}(t)), \mathbf{x}(0) = \mathbf{x}_0 $, then $$ \| \mathbf{x}_1(t) - \mathbf{x}_2(t) \| = \| \int_0^t \left[ f(s, \mathbf{x}_1(s)) - f(s, \mathbf{x}_2(s)) \right] ds \| \leq L_f \int_0^t \| \mathbf{x}_1(s) - \mathbf{x}_2(s) \| ds $$ Set $ u(t) = \| \mathbf{x}_1(t) - \mathbf{x}_2(t) \| $, then by Gro&#776;nwall's Lemma, $ \| \mathbf{x}_1(t) - \mathbf{x}_2(t) \| = 0 $. </p> <p> <strong>Maximal Interval of Existence and Continuation</strong> Given $ (t_0, \mathbf{x}_0) $, there exists a unique maximal solution: $ (t_0, \tau_{+}) \to \mathbb{R}^d $ with $ t_0 &lt; \tau_{+} \le T $, obtained by extending the local solution as long as it remains in $ [t_0, T] \times \mathbb{R}^d $ and within regions where $ f $ is continuous and locally Lipschitz in $ \mathbf{x} $. If $ \tau_{+} \le T $, then necessarily $ \lim_{t \to \tau_{+}} \| x(t) \| \to \infty $ or $ x(t) $ approaches a point where $ f $ ceases to be continuous or locally Lipschitz in $ \mathbf{x} $. In particular, if the solution remains bounded (not blow-up in finite time) and $ f $ is continuous and locally Lipschitz in $ \mathbf{x} $ on all of $ [t_0, T] \times \mathbb{R}^d $, the solution can be extended to the right point $ T $. </p> <p> Next, let us take a quick look at a simple example <d-footnote>The example is borrowed from the book Elementary Differential Equations and Boundary Value Problems.<d-cite key="boyce2021elementary"></d-cite></d-footnote> to gain an intuitive understanding of how Picard's iteration works: $$ y' = 2t(1 + y), \quad y(0) = 0 $$ </p> <p> Applying the Picard's iteration $ \Phi_{n+1}(t) = y_0 + \int_0^t f(s, \Phi_n(s)) ds $ to $ f(t, y(t)) = 2t(1 + y) $, we obtain: $$ \Phi_n(t) = \sum_{k=1}^n \frac{t^{2k}}{k!} $$ </p> <p> <em>Proof by induction.</em> For $ n = 1 $, $ \Phi_1(t) = \int_0^t 2s\left(1 + \Phi_0(s) \right) ds = t^2 $, where $ \Phi_0(t) = 0 $. We then need to show that if it is true for $ n = k $, then it also holds true for $ n = k + 1 $. $$ \begin{align*} \Phi_{k+1}(t) &amp;= \int_0^t 2s \left( 1 + \Phi_k(s) \right) ds \\ &amp;= \int_0^t 2s \left( 1 + s^2 + \frac{s^4}{2!} + \cdots + \frac{s^{2k}}{k!} \right) ds\\ &amp;= \int_0^t 2s + 2s^3 + \frac{2s^5}{2!} + \cdots + \frac{2s^{2k+1}}{k!} ds \\ &amp;= t^2 + \frac{t^4}{2!} + \frac{t^6}{3!} + \cdots + \frac{t^{2k+2}}{(k+1)!} \end{align*} $$ The infinite series $ \sum_{k=1}^{\infty} \frac{t^{2k}}{k!} $ converges to $ \Phi(t) = e^{t^2} - 1 $. </p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/picard_iter-480.webp 480w,/assets/img/picard_iter-800.webp 800w,/assets/img/picard_iter-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/picard_iter.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2> Cauchy-Peano Existence Theorem </h2> <p> For the initial value problem $$ \begin{cases} \mathbf{x}^{\prime}(t) = f(t, \mathbf{x}(t)), \quad t \in [0, T], \\ \mathbf{x}(t_0) = \mathbf{x}_0 \in \mathbb{R}^d \end{cases} $$ where $ f : [0, T] \times \mathbb{R}^d \to \mathbb{R}^d $ is continuous. Then there exists a solution $ \mathbf{x}(t) \in \mathcal{C}^1([t_0, t_0 + h]; \mathbb{R}^d) $ that satisfies both the differential equation and the initial condition. </p> <p> <em>Step 1:</em> Define a compact set $ R = [t_0, t_0 + h] \cap [0, T] \times \overline{B}(\mathbf{x}_0, r) $, where $ h \le \frac{r}{M} $ and $ M = \sup_{ (t, \mathbf{x}) \in \mathbb{R}^d} \| f(t, \mathbf{x}) \| &lt; \infty $, to ensure $ \| \mathbf{x}(t) - \mathbf{x}_0 \| \le \| \int_0^t f(s, \mathbf{x}(s)) ds \| \le r $. </p> <p> <em>Step 2:</em> Construct a sequence of piecewise linear functions $ \mathbf{x}_n(t) $, called Euler polygons, to approximate the solution over $ [t_0, t_0 + h] $. Specifically, for each positive integer $ n $: <ul> <li> Divide $ [t_0, t_0 + h] $ into $ n $ equal subintervals of length $ \Delta_n = \frac{h}{n} $, with points $ t_k^n = t_0 + k \Delta_n $ for $ k = 0, 1, \ldots, n $.</li> <li> Set $ \mathbf{x}_n(t_0) = \mathbf{x}_0 $.</li> <li> On each subinterval $ [t_k^n, t_{k+1}^n] $, define $ \mathbf{x}_n(t) $ linearly with slope $ f(t_k^n, \mathbf{x}_n(t_k^n)) $: $ \mathbf{x}_n(t) = \mathbf{x}_n(t_k^n) + f(t_k^n, \mathbf{x}_n(t_k^n))(t - t_k^n) \quad \text{for} \quad t \in [t_k^n, t_{k+1}^n] $.</li> </ul> </p> <p> <em>Step 3:</em> To prove local existence of a solution, we then apply <a href="https://users.math.msu.edu/users/shapiro/Pubvit/Downloads/ArzNotes/ArzNotes.pdf">Arzela&#768;–Ascoli theorem</a>, which states that that a sequence of functions $ \{ f_n \}_1^{\infty} \in \mathcal{C}(X) $, where $ X $ denotes a compact metric space with the supremum norm $ \| f \|_{\infty} = \sup_{x \in X} | f(x) | $ has a uniformly convergent subsequence if it is bounded and equicontinuous. Specifically, $ \{ f_n \}_1^{\infty} \in \mathcal{C}(X) $ is bounded means that there exists a positive constant $ M &lt; \infty $ such that $ | f_n(x) | &lt; M $ for each $ x \in X $ and each $ f_n $. $ \{ f_n \}_1^{\infty} \in \mathcal{C}(X) $ is equicontinuous at a point $ a \in X $ means that, for each $ \epsilon &gt; 0 $, there exists $ \delta &gt; 0 $ such that $ |f_n(x) - f_n(a) | &lt; \epsilon $ for all $ x \in X $ with $ \text{dist}(x, a) &lt; \delta $ and for all $ f_n $, which means $ \delta $ is independent of functions. Furthermore, If $ X $ is compact and a family of functions $ \mathcal{F} \in \mathcal{C}(X) $ is equicontinuous on $ X $, then $ \mathcal{F} $ is uniformly equicontinuous, which means that, for each $ \epsilon &gt; 0 $, there exists $ \delta &gt; 0 $ such that $ | f(x) - f(y) | &lt; \epsilon $ for all $ x, y \in X $ with $ \text{dist}(x, y) &lt; \delta $ and for all $ f \in \mathcal{F} $. </p> <p> In our case $ \mathbf{x}_n(t) \in \mathcal{C}([0, T]; \mathbb{R}^d) $, we need to prove the (uniform) boundedness and equicontinuity of $ \mathbf{x}_n $. Then there exists a subsequence $ \{ \mathbf{x}_{n_k} \} $ converging uniformly to some $ \mathbf{x} \in \mathcal{C}([0, T]; \mathbb{R}^d) $. <ul> <li>Uniform boundedness. For each $ n $ and $ t \in [t^n_k, t^n_{k+1}] $, $$ \| \mathbf{x}_n(t) - \mathbf{x}_0 \| \le \sum_{j=0}^{k-1} \| \mathbf{x}_n(t_{j+1}^n) - \mathbf{x}_n(t_{j}^n) \| + \| \mathbf{x}_n(t) - \mathbf{x}_n(t_k^n) \| \leq \sum_{j=0}^{k-1} M \Delta_n + M | t - t_k^n | \leq M | t - t_0 | \leq Mh $$ </li> <li>Equicontinuity. For $ s, t \in [t^n_k, t^n_{k+1}] $ with $ s &lt; t $, $$ \| \mathbf{x}_n(t) - \mathbf{x}_n(s) \| \leq \int_s^t \| f(t_k^n, \mathbf{x}_n(t_k^n)) \| du \leq M | t - s | $$ </li> </ul> </p> <p> Unlike Picard–Lindelo&#776;f, the Cauchy-Piano Existence theorem does not assume Lipschitz continuity in $ \mathbf{x} $, so the uniqueness is not guaranteed. A classical example in $ d = 1 $ is $ x'(t) = \sqrt{|x(t)|}, \; x(0) = 0 $. Here $ f(x) = \sqrt{|x|} $ is continuous but not Lipschitz at $ 0 $ (the derivative of $ \sqrt{|x|} $ is unbounded near zero). For any $ \tau \ge 0 $, $$ x_{\tau}(t) = \begin{cases} 0, \; 0 \le t \le \tau, \\ \frac{(t - \tau)^2}{4}, \; t \ge \tau, \end{cases} $$ is a $ C^1 $ solution. Thus there are infinitely many solutions passing through $ (0, 0) $. </p> <h2> The General n-th Order ODE </h2> <p> Next, we consider the IVP of the general n-th order ODE: $$ \begin{cases} y^{(n)}(t) = F(t, y(t), y'(t), \ldots, y^{(n-1)}(t)), \quad t \in [0, T], \\ y(t_0) = y_0, y'(t_0) = y_1, \ldots, y^{(n-1)}(t_0) = y_{n-1}, \end{cases} $$ where $ F : [0, T] \times \mathbb{R}^n \to \mathbb{R} $ is continuous in $ t $ and Lipschitz continuous w.r.t. the other variables, i.e., $ | F(t, \mathbf{x}) - F(t, \mathbf{z}) | \leq L \| \mathbf{x} - \mathbf{z} \| $. Then, there exists a unique function $ y \in \mathcal{C}^n(J), \; J := [t_0 - h, t_0 + h] \cap [0, T] $ solving the IVP. </p> <p>We will show that the n-th order ODE can be transformed to a system of n first-order ODEs, then apply the Picard-Lindelo&#776;f theorem derived above regarding systems of first-order ODEs to conclude the proof.</p> <p> Define $ x_1(t) := y(t), x_2(t) := y'(t), x_3(t) := y''(t), \ldots, x_n(t) := y^{(n-1)}(t) $. Then $$ \begin{align*} x'_1 &amp;= x_2 \\ x'_2 &amp;= x_3 \\ &amp;\vdots \\ x'_{n-1} &amp;= x_n \\ x'_n &amp;= F(t, x_1, x_2, \ldots, x_n) \end{align*} $$ Write this compactly as the first-order system: $$ \begin{cases} \mathbf{x}'(t) = f(t, \mathbf{x}(t)) \\ \mathbf{x}(t_0) = \mathbf{x}_0 : = (y_0, y_1, \ldots, y_{n-1})^T \end{cases} $$ where $ f : [0, T] \times \mathbb{R}^n \to \mathbb{R}^n $ is defined by $ f(t, \mathbf{x}) := (x_2, x_3, \ldots, x_n, F(t, \mathbf{x}))^T, \; \mathbf{x} = (x_1, \ldots, x_n)^T \in \mathbb{R}^n $. </p> <p> It is trivial to check that $ f $ is continuous as its components are continuous. We just need to verify the Lipschitz continuity in $ \mathbf{x} $. For $$ (t, \mathbf{x}), (t, \mathbf{z}) \in \widetilde{\mathcal{R}} := ([t_0 - h, t_0 + h] \cap [0, T]) \times \overline{B}(\mathbf{x}_0, r) \subset [0, T] \times \mathbb{R}^n, $$ $$ \begin{align*} \| f(t, \mathbf{x}) - f(t, \mathbf{z}) \| &amp;= \| (x_2 - z_2, \ldots, x_n - z_n, F(t, \mathbf{x}) - F(t, \mathbf{z})) \| \\ &amp;\le \| \mathbf{x} - \mathbf{z} \| + | F(t, \mathbf{x}) - F(t, \mathbf{z}) | \leq (1 + L) \| \mathbf{x} - \mathbf{z} \| \end{align*} $$ </p> <p> By the Picard–Lindelo&#776;f theorem proved previously for a system of first-order ODEs, there exists a unique solution $ \mathbf{x}(t) \in \mathcal{C}^1(J;\mathbb{R}^n) $ of the system $ \mathbf{x}'(t) = f(t, \mathbf{x}(t)) $ with the initial condition $ \mathbf{x}(t_0) = \mathbf{x}_0 : = (y_0, y_1, \ldots, y_{n-1})^T $ on $ J := [t_0 - h, t_0 + h] \cap [0, T] $, where $ h \le \{ \frac{r}{\widetilde{M}}, \frac{1}{L} \} $ and $ \widetilde{M} = \sup | f(t, \mathbf{x}) | &lt; \infty $. As $ \mathbf{x} \in \mathcal{C}^1(J) $, $ x_n = y^{(n-1)} \in \mathcal{C}^1(J) $, implying $ y^{(n)} \in \mathcal{C}^n(J) $. </p> <h3> N-th Order Linear ODEs </h3> <p> For n-th order linear ODEs $ y^{(n)} + p_1(t)y^{(n-1)} + \cdots + p_{n-1}(t)y' + p_n(t)y = g(t) $ with initial conditions $ y(t_0) = y_0, y'(t_0) = y_1, \ldots, y^{(n-1)}(t_0) = y_{n-1} $, where $ t_0 \in I := (a, b) $ and $ p_1(t), \ldots, p_n(t), g(t) $ are continuous on the open interval $ I $, then there exists a unique solution $ y = \phi(t) $ that satisfies the IVP and exists throughout the interval $ I $. </p> <p> Similarly, we can convert n-th order linear ODEs to a system of first-order ODEs $ \mathbf{x}' = \mathbf{A}(t)\mathbf{x} + \mathbf{g}(t) $, where $$ \mathbf{A}(t) = \begin{pmatrix} 0 &amp; 1 &amp; 0 &amp; \cdots &amp; 0 \\ 0 &amp; 0 &amp; 1 &amp; \cdots &amp; 0 \\ \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 0 &amp; 0 &amp; 0 &amp; \cdots &amp; 1 \\ -p_n(t) &amp; -p_{n-1}(t) &amp; -p_{n-2}(t) &amp; \cdots &amp; -p_1(t) \end{pmatrix}. $$ and $ \mathbf{g}(t) = (0, 0, \ldots, 0, g(t))^T $, with initial conditions $ \mathbf{x}(t_0) = \mathbf{x}_0 := (y_0, y_1, \ldots, y_{n-1})^T $. </p> <p> To apply the Picard–Lindelo&#776;f theorem, we need to check two conditions of $ f(t, \mathbf{x}) = \mathbf{A}(t)\mathbf{x} + \mathbf{g}(t) $: <ul> <li> <em>Continuity.</em> The entries of $ \mathbf{A}(t) $ are continuous functions $ p_i(t) $ and thus $ \mathbf{A}(t) $ is continuous ($ \| \mathbf{A}(t_1) - \mathbf{A}(t_2) \| \le \sum_{i,j} \left[a_{ij}(t_1) - a_{ij}(t_2) \right]^2 $, where the matrix norm is defined by $ \| \mathbf{A} \| = \max_{\| \mathbf{x} \|=1} \| \mathbf{Ax} \|, \;\text{where}\; \mathbf{x} \in \mathbb{R}^n $). $ \mathbf{g}(t) $ is continuous as the only contained entry $ g(t) $ is continuous as specified.</li> <li> <em>Lipschitz continuity.</em> $$ \| \mathbf{f}(t, \mathbf{x}) - \mathbf{f}(t, \mathbf{y}) \| = \| \mathbf{A}(t) \mathbf{x} + \mathbf{g}(t) - (\mathbf{A}(t) \mathbf{y} + \mathbf{g}(t)) \| = \| \mathbf{A}(t) (\mathbf{x} - \mathbf{y}) \| \le \| \mathbf{A}(t) \| \| \mathbf{x} - \mathbf{y} \|. $$ The matrix norm is bounded (e.g., $ \| \mathbf{A} \|_{\infty} = \max_i \sum_j | a_{ij} | $, and matrix norms are equivalent.), say by $ K $, since each entry $ p_i(t) $ is a continuous function on a compact interval ($ | p_i(t) | &lt; M_i $). Thus, $ \| f(t, \mathbf{x}) - f(t, \mathbf{y}) \| \leq K \| \mathbf{x} - \mathbf{y} \| $.</li> </ul> </p> <p> The Picard-Lindelo&#776;f theorem guarantees a local solution $ \mathbf{x}(t) $ existing on some interval $ [t_0 - h, t_0 + h] \subset I $. Since the ODE is linear, we can extend the solution to the entire interval $ I $. $$ \| \mathbf{x}(t) \| \leq \| \mathbf{x}_0 \| + \int_{t_0}^t [ \| \mathbf{A}(s) \| \| \mathbf{x}(s) \| + \| \mathbf{g}(s) \| ] \, ds. $$ Since $ \| \mathbf{A}(s) \| \le K $ and $ \| \mathbf{g}(s) \| \le M_g $ on $ I $, apply Gro&#776;nwall’s inequality. $$ \| \mathbf{x}(t) \| \leq \left( \| \mathbf{x}_0 \| + M_g| t - t_0 | \right)e^{K| t - t_0 |} $$ </p> <p> Therefore, the solution remains bounded in the interval. Suppose the maximal interval of existence has a right endpoint $ \beta &lt; b $. Since $ p_i(t), g(t) $ are continuous on $ I $, they are continuous on any compact subinterval $ [t_0, \beta + \epsilon] \subset I $. Reapply Picard-Lindelo&#776;f at $ t = \beta $, using $ \mathbf{x}(\beta) $ as the new initial condition. This extends the solution beyond $ \beta $, contradicting the maximality of $ \beta $. Thus, the solution extends to $ b $. Similarly, it extends to the left endpoint $ a $. Furthermore, any two solutions defined on overlapping intervals must coincide as their difference satisfies the homogeneous equation $ \mathbf{A}(t)\mathbf{x} = \mathbf{0} $, which has a unique solution $ \mathbf{x}(t) = \mathbf{0} $. Thus, the solution is unique across $ I $. </p> <p> Actually, $ f(t, \mathbf{x}) = \mathbf{A}(t)\mathbf{x} + \mathbf{g}(t) $ is a special case of a common sufficient condition for global existence on the full interval $ [0, T] $, i.e., a linear growth bound: $$ \| f(t, \mathbf{x}) \| \le a(t) + b \| \mathbf{x} \|, \; t \in [0, T], \mathbf{x} \in \mathbb{R}^d, $$ with $ a \in L^1([0, T]) $ and $ b\ge 0 $. By Gro&#776;nwall's inequality, $ \| \mathbf{x}(t) \| \le \left(\|\mathbf{x}_{0}\| + \int_{t_0}^t a(s)ds \right)e^{b(t - t_0)}, \; t \ge t_0 $. This bound prevents blow-up in finite time, so the solution can be extended to the end point $ T $. </p> <h2> Concluding Remarks </h2> <p> It is not uncommon for mathematics textbooks to omit certain steps in their proofs—either because the authors consider them non-essential or because they are intentionally left as exercises for the reader. However, such deliberate gaps can easily discourage learners like me, who have not received systematic training in mathematics at the college level or beyond. Fortunately, the remarkable mathematical reasoning capabilities of large language models (LLMs) come to the rescue. This blog was developed from a self-contained, step-by-step tutorial generated by ChatGPT. Throughout the learning process, I played the role of a verifier, which greatly flattened the learning curve. </p> <p> As an aside, I noticed that ChatGPT first employs a kind of verbalised reinforcement learning as a high-level planner, decomposing the overall objective into manageable steps with concrete, verifiable targets (e.g., boxed key equations). This process is further refined by reinforcement learning with verifiable rewards (RLVR) techniques, which help ensure that the generated content stays on course. Even so, I was completely astonished by ChatGPT’s ability to produce LaTeX equations with such precision—almost entirely error-free. It had never occurred to me that intricate and often lengthy mathematical expressions in LaTeX format could be compressed within an LLM and then elicited so accurately. Whether or not this achievement can be considered a “jewel in the crown” of intelligence, it undeniably showcases the transformative potential of LLMs in reshaping education—for instance, through specialized textbooks collaboratively compiled by communities of learners. Moreover, LLMs enhanced with RLVR techniques have unexpectedly evolved into sophisticated sifters for navigating the overwhelming volume of knowledge and the low signal-to-noise ratio in the internet era. </p> <h2>Acknowledgement</h2> <p>This blog was developed from a self-contained, step-by-step tutorial generated by OpenAI ChatGPT, which also assisted in refining the phrasing and expressions, as well as in suggesting suitable titles.</p> <div class="language-html highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@misc{Cai2025odelearningnotes-1,
    author = {Xin Cai},
    title = {Reconstructing the Scaffolding A Learner’s Notes on Ordinary Differential Equations: Part 1 Exploring the Picard–Lindelöf and Cauchy–Peano Theorems},
    howpublished = {\url{https://totalvariation.github.io/blog/2025/concise-intro-odes-part1/}},
    note = {Accessed: 2025-10-28},
    year = {2025}
}
</code></pre></div></div>]]></content><author><name>Xin Cai</name></author><summary type="html"><![CDATA[Part &#8544 Exploring the Picard–Lindelo&#776;f and Cauchy–Peano Theorems]]></summary></entry><entry><title type="html">Reconstructing the Scaffolding A Learner’s Notes on Ordinary Differential Equations</title><link href="https://totalvariation.github.io/blog/2025/concise-intro-odes-part2/" rel="alternate" type="text/html" title="Reconstructing the Scaffolding A Learner’s Notes on Ordinary Differential Equations"/><published>2025-10-28T00:00:00+00:00</published><updated>2025-10-28T00:00:00+00:00</updated><id>https://totalvariation.github.io/blog/2025/concise-intro-odes-part2</id><content type="html" xml:base="https://totalvariation.github.io/blog/2025/concise-intro-odes-part2/"><![CDATA[<div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <video src="/assets/video/traj-one-step-methods.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" autoplay="" controls=""/> </figure> </div> </div> <div class="caption"> Animated Trajectories of One-Step Methods on Lotka-Volterra Equations </div> <p> This post is part of my ongoing series of learning notes on Ordinary Differential Equations (ODEs). In this instalment, I will first present a fundamental theorem on the convergence of general one-step methods, followed by a concise introduction to Runge–Kutta (RK) methods, including detailed derivations of the order conditions up to order four. </p> <p> Throughout this blog, we focus on the initial value problems (IVPs) for one-dimensional ODEs for notational simplicity $$ y'(t) = f(t, y(t)), \; y(t_0) = y_0, \; t \in [t_0, T] $$ with $ f $ a sufficiently smooth (as specified when needed) scalar function. In the more general setting, where $ f : \mathbb{R} \times \mathbb{R}^d \to \mathbb{R}^d $, the corresponding proofs remain essentially identical, with vector norms used in place of absolute values where appropriate. </p> <h2> Convergence of General One Step Methods </h2> <p> A general (explicit) one-step method can be written in the form: $$ y_{n+1} = y_n + h \Phi(t_n, y_n, h), \; n=0,1,\ldots, N-1, \; h = \frac{T - t_0}{N}, \; t_n = t_0 + nh, \; y_0 = y(t_0) $$ </p> <p> <strong>Definition Local Truncation Error (LTE):</strong> The local truncation error $ \tau_{n+1} $ at step $ t_{n+1} $ is defined as by assuming $ y_n = y(t_n) $ is exact: $$ \tau_{n+1} = \left[ y(t_{n+1}) - y(t_n) - h \Phi(t_n, y(t_n), h) \right] $$ </p> <p> The method is consistent if $ \max_{0 \leq n \leq N-1} \frac{1}{h} |\tau_{n+1}| \to 0 \; \text{as} \; h \to 0 $, which is equivalent to $ \Phi(t, y, 0) = f(t, y), \; \text{for all} \; t \in [0, T] $ as $ \lim_{h \to 0} \frac{1}{h} \tau_{n+1} = y'(t_n) - \Phi(t_n, y(t_n), 0) $. </p> <p> Yet, the convergence of a numerical scheme concerns the global error $ e_n = y(t_n) - y_n $, which accumulates over steps. The method is convergent if: $ \max_{0 \leq n \leq N} |e_n| \to 0 \; \text{as} \; h \to 0 $, with $ t \in [0, T] $. It is convergent of order $ p $ if $ \max |e_n| = O(h^p) $. The consistent condition alone does not guarantee convergence, we need an additional stability condition on $ \Phi $: $ |\Phi(t, y_1, h) - \Phi(t, y_2, h)| \leq L_{\Phi} |y_1 - y_2|, t \in [0, T], h \in [0, h_0] $, i.e., $ \Phi $ satisfying the Lipschitz condition in $ y $ and independent of $ h $. This ensures errors do not amplify excessively. </p> <p> <strong>Theorem Convergence of One-Step Method</strong> If the one-step method is consistent and $ \Phi $ is Lipschitz in $ y $ with constant $ L_{\Phi} $ independent of $ h $, then it is convergent with the global error expressed as: $ |e_n| \le \frac{\tau}{hL_{\Phi}} (e^{L_{\Phi}(T - t_0)} - 1) $, where $ \tau = \max_{0 \leq n \leq N} |\tau_n| $. </p> <p><strong>Proof:</strong></p> <p> By the definition of the LTE, the exact solution satisfies: $$ y(t_{n+1}) = y(t_n) + h \Phi(t_n, y(t_n), h) + \tau_{n+1} $$ Subtracting the numerical scheme $$ y_{n+1} = y_n + h \Phi(t_n, y_n, h) $$, we obtain a recursive formula for error function: $$ \begin{align*} e_{n+1} &amp;= y(t_{n+1}) - y_{n+1} = [y(t_n) - y_n] + h [\Phi(t_n, y(t_n), h) - \Phi(t_n, y_n, h)] + \tau_{n+1} \\ e_{n+1} &amp;= e_n + h [\Phi(t_n, y(t_n), h) - \Phi(t_n, y_n, h)] + \tau_{n+1} \\ |e_{n+1}| &amp;\le |e_n| + h |\Phi(t_n, y(t_n), h) - \Phi(t_n, y_n, h)| + |\tau| \end{align*} $$ </p> <p> By the Lipschitz condition on $ \Phi $, i.e., $ |\Phi(t_n, y(t_n), h) - \Phi(t_n, y_n, h)| \leq L_{\Phi} |y(t_n) - y_n| = L_{\Phi} |e_n| $, we get, $$ \begin{align*} |e_{n+1}| &amp;\le (1 + hL_{\Phi} ) |e_n| + |\tau| \\ |e_{n}| &amp;\le (1 + h L_{\Phi} )^n |e_0| + |\tau| \sum_{k=0}^{n-1} (1 + hL_{\Phi})^k \end{align*} $$ </p> <p> If $ |e_0| = y_0 - y(t_0) = 0 $, we get $$ |e_n| \leq |\tau| \cdot \frac{(1 + h L_{\Phi})^n - 1}{h L_{\Phi}} = \frac{|\tau|}{hL_{\Phi}} \left[ (1 + h L_{\Phi})^n - 1 \right] $$ </p> <p> Since $ 1 + x \leq e^x $ and $ n h = t_n - t_0 \le T - t_0 $, we have $ (1 + h L_{\Phi})^n \leq e^{n h L_{\Phi}} \leq e^{L_{\Phi} (T - t_0)} $, thus yielding the final expression: $$ |e_n| \leq \frac{|\tau|}{hL_{\Phi}} (e^{L_{\Phi} (T - t_0)} - 1) $$ As $ h \to 0 $, consistency implies $ \frac{1}{h}|\tau| \to 0 $, so $ |e_n| \to 0 $, proving convergence. If the method is of order $ p $, then $ |\tau| = O(h^{p+1}) $, so $ |e_n| = O(h^p) $, proving order $ p $ convergence. </p> <p> The main takeaway is, for one-step methods, <strong>consistency + one-step stability (Lipschitz continuity) $ \implies $ convergence</strong>. Furthermore, the LTE is $ \mathcal{O}(h^{p+1}) $ $ \implies $ the global error is $ \mathcal{O}(h^p) $. </p> <p> Next, we will examine the LTE of some common one-step methods: <ol> <li> Explicit Euler $ y_{n+1} = y_n + hf(t_n, y_n) $. Use Taylor expansion of the exact solution: $$ y(t_{n+1}) = y(t_n) + h y'(t_n) + \mathcal{O}(h^2) $$ Given $ y'(t_n) = f(t_n, y(t_n)) $, so $$ \tau_{n+1} = y(t_{n+1}) - \left(y(t_n) + hf(t_n, y(t_n)) \right) = \mathcal{O}(h^2) $$ hence $ |\tau| \le Ch^2 $ (with $ C = \max_{t \in [t_0, T]} |y''(t)| $). Thus, explicit Euler has order $ p = 1 $. </li> <li> Improved Euler $ y_{n+1} = y_n + \frac{h}{2} \left[ f(t_n, y_n) + f(t_n + h, y_n + h f(t_n, y_n)) \right] $. Use Taylor expansion of the exact solution: $$ y(t_{n+1}) = y(t_n) + h y'(t_n) + \frac{h^2}{2} y''(t_n) + \mathcal{O}(h^3) $$ Given $ y'(t_n) = f(t_n, y(t_n)) $ and $$ y''(t_n) = f_t(t_n, y(t_n)) + f_y(t_n, y(t_n)) f(t_n, y(t_n)) $$ we have $$ y(t_{n+1}) = y(t_n) + h f(t_n, y(t_n)) + \frac{h^2}{2} \left[ f_t(t_n, y(t_n)) + f_y(t_n, y(t_n)) f(t_n, y(t_n)) \right] + \mathcal{O}(h^3) $$ Use multivariate Taylor expansion: $$ f(t_n + h, y(t_n) + h f(t_n, y(t_n))) = f(t_n, y(t_n)) + h f_t(t_n, y(t_n)) + h f_y(t_n, y(t_n)) f(t_n, y(t_n)) + \mathcal{O}(h^2) $$ so $$ \tau_{n+1} = y(t_{n+1}) - \left( y(t_n) + \frac{h}{2} \left( f(t_n, y(t_n)) + f(t_n + h, y(t_n) + h f(t_n, y(t_n))) \right) \right) = \mathcal{O}(h^3) $$ hence $ |\tau| \le Ch^3 $. Thus, improved Euler has order $ p = 2 $. </li> </ol> </p> <p>It turns out that the methods we discussed above, i.e., explicit Euler and improved Euler, are special cases of Runge-Kutta methods, which we will inspect shortly.</p> <h2> General Runge-Kutta Methods </h2> <p> An $ s $-stage Runge-Kutta (RK) method computes stage derivatives $$ k_i = f \left(t_n + c_i h, y_n + h \sum_{j=1}^s a_{ij}k_j \right), i=1,\ldots,s, $$ and updates as per the following formula: $$ y_{n+1} = y_n + h \sum_{i=1}^s b_i k_i. $$ </p> <p> The coefficients $ \mathbf{A} = [a_{ij}]_{s \times s}, \mathbf{b} = (b_1, \cdots, b_s), \mathbf{c}=(c_1, \cdots, c_s)^T $ are real numbers arranged in the Butcher tableau $$ \begin{array}{c|ccc} c_1 &amp; a_{11} &amp; \cdots &amp; a_{1s} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ c_s &amp; a_{s1} &amp; \cdots &amp; a_{ss} \\ \hline &amp; b_1 &amp; \cdots &amp; b_s \end{array} $$ The $ k_i $ are slopes evaluated at intermediate points. If $ \mathbf{A} $ is strictly lower triangular ($ a_{ij} = 0 $ for $ j \ge i $), the method is explicit. Otherwise, it is implicit. </p> <p> To derive RK methods of order $ p $, the numerical solution must match the exact Taylor expansion up to $ O(h^{p+1}) $ (i.e., ensuring the LTE is $ O(h^{p+1}) $). Expand $ y_{n+1} $ and each $ k_i $ in Taylor series, then equate coefficients. This yields order conditions on $ \mathbf{b}, \mathbf{c}, A $. </p> <p> We will use one-dimensional ODEs for simplicity, but the methods extend naturally to systems of ODEs, i.e., $ y(t) \in \mathbb{R}^d, f : \mathbb{R} \times \mathbb{R}^d \to \mathbb{R}^d $ with higher-order derivatives generalised to multilinear maps (e.g., second order derivatives correspond to bilinear maps). </p> <p><strong>Taylor expansion of the exact solution</strong></p> <p> Fix $ t = t_n $. Denote $ f = f(t, y(t)), f_t = \frac{\partial f}{\partial t}, f_y = \frac{\partial f}{\partial y} $. Differentiating the ODE $ y' = f $ yields $$ \begin{align*} y'' &amp;= \frac{d}{dt} f(t,y) = f_t + f_y y' = f_t + f_y f \\ y^{(3)} &amp;= \frac{d}{dt} (f_t + f_y f) = f_{tt} + f_{ty}y' + \frac{d}{dt}(f_y f) \\ \frac{d}{dt}(f_y f) &amp;= (f_{yt} + f_{yy}y')f + f_y(f_t + f_y y') \end{align*} $$ Substitute $ y' = f $ to obtain $$ y^{(3)} = f_{tt} + 2f_{ty}f + f_{yy}ff + f_y f_t + f_y f_y f $$ Thus the Taylor expansion of the exact solution is $$ y(t + h) = y + hf + \frac{h^2}{2}(f_t + f_y f) + \frac{h^3}{6} (f_{tt} + 2f_{ty}f + f_{yy}ff + f_y f_t + f_y f_y f) + \mathcal{O}(h^4) $$ </p> <p> If $ f : \mathbb{R} \times \mathbb{R}^d \to \mathbb{R}^d $, then $ f_y \in \mathbb{R}^{d \times d} $ is the Jacobian matrix. Correspondingly, $ f_y f $ denotes matrix-vector multiplication, and $ f_{yy} f f := f_{yy}[f, f] $ denotes the bilinear map <d-footnote>We encourage readers seeking a deep understanding of this concept to consult the chapter 12 of the book Matrix Calculus (for Machine Learning and Beyond). <d-cite key="bright2025matrix"></d-cite> </d-footnote> acting on vectors, where $ f_{yy} \in \mathcal{L}(\mathbb{R}^d, \mathcal{L}(\mathbb{R}^d, \mathbb{R}^d)) $, $ f_{yy} \cdot e_i = H_{f_i} $ with $ H_{f_i} $ defined as per the Hessian matrix of a scalar-valued function. </p> <p><strong>Stage expansions</strong></p> <p> For small $ h $, expand each stage: $$ k_i = f \left(t + c_i h, y + h \sum_j a_{ij}k_j \right) $$ </p> <p><em>Zeroth order:</em> $ k_i = f + \mathcal{O}(h) $.</p> <p><em>First order:</em> $ k_i = f + c_i h f_t + h f_y \sum_j a_{ij} k_j + \mathcal{O}(h^2) $.</p> <p> Replace $ k_j = f + \mathcal{O}(h) $ inside the sum to get $$ k_i = f + c_i h f_t + h f_y \sum_j a_{ij} f + \mathcal{O}(h^2) $$ Assume the row sum condition $ c_i := \sum_j a_{ij} $, which holds true for all classic RK methods. Then $$ k_i = f + h(c_i f_t + c_i f_y f) + \mathcal{O}(h^2) $$ Insert the expansion of $ k_i $ into the numerical update $ y_{n+1} = y_n + h \sum_i b_i k_i $: $$ y_{n+1} = y_n + h \sum_i b_i f + h^2 \sum_i b_i (c_i f_t + \alpha_i f_y f) + \mathcal{O}(h^3) $$ Group terms: $$ y_{n+1} = y_n + h \left( \sum_i b_i \right) f + \frac{h^2}{2} \left(2 \sum_i b_i c_i \right) f_t + \frac{h^2}{2}\left( 2 \sum_i b_i c_i \right)f_y f + \mathcal{O}(h^3) $$ Matching with the exact Taylor series to order 2 $$ y(t + h) = y + hf + \frac{h^2}{2} (f_t + f_y f) + \mathcal{O}(h^3) $$ </p> <p>Equate coefficients of $ h $ and $ h^2 $:</p> <p> <strong>Order 1 condition:</strong> $$ \sum_{i=1}^s b_i = 1 $$ </p> <p> <strong>Order 2 condition:</strong> $$ \sum_{i=1}^s b_i c_i = \frac{1}{2} $$ </p> <p> <strong>Order 3 conditions:</strong> We need one higher order in the stage expansion. $$ k_i = f + c_i h f_t + h f_y \sum_j a_{ij} k_j + \frac{1}{2}(c_i h)^2 f_{tt} + c_i h f_{ty} \left( h \sum_j a_{ij} k_j \right) + \frac{1}{2} \left( h \sum_j a_{ij} k_j \right) f_{yy} \left( h \sum_{\ell} a_{i \ell} k_{\ell} \right) + \mathcal{O}(h^3) $$ Now substitute $ k_j = f + h \left( c_j f_t + c_j f_y f \right) + \mathcal{O}(h^2) $ into those sums, $$ \sum_j a_{ij} k_j = c_i f + h \sum_j a_{ij}\left( c_j f_t + c_j f_y f \right) + \mathcal{O}(h^2) $$ $$ \begin{align*} k_i &amp;= f + c_i h f_t + h f_y (c_i f) + \frac{h^2}{2}c^2_i f_{tt} + c_i h f_{ty} \left( h c_i f \right) + \frac{h^2}{2} (c_i f) f_{yy} (c_i f) \\ &amp; + h^2 \left( \sum_j a_{ij} c_j \right) f_y f_t + h^2 \left( \sum_j a_{ij} c_j \right) f_y f_y f + \mathcal{O}(h^3) \end{align*} $$ $$ \begin{align*} k_i &amp;= f + h (c_i f_t + c_i f_y f) \\ &amp;+ \frac{h^2}{2} c^2_i f_{tt} + h^2 c_i^2 f_{ty}f + \frac{h^2}{2} c^2_i f_{yy} f f \\ &amp;+ h^2 \left( \sum_j a_{ij} c_j \right) f_y f_t + h^2 \left( \sum_j a_{ij} c_j \right) f_y f_y f + \mathcal{O}(h^3) \end{align*} $$ Now substitute into the update $ y_{n+1} = y + h \sum_i b_i k_i $ to obtain, $$ \begin{align*} y_{n+1} &amp;= y + h \sum_i b_i f \\ &amp; + h^2 \sum_i b_i \left( c_i f_t + c_i f_y f \right) \\ &amp; + h^3 \left[ \frac{1}{2} \sum_i b_i c_i^2 f_{tt} + \sum_i b_i c_i^2 f_{ty}f + \frac{1}{2}\sum_i b_i c_i^2 f_{yy} f f \right. \\ &amp; \left. + \left(\sum_{i, j}b_i a_{ij} c_j \right) f_y f_t + \left(\sum_{i, j}b_i a_{ij} c_j \right) f_y f_y f \right] + \mathcal{O}(h^4). \end{align*} $$ Compare with the exact Taylor series to order 3: $$ y(t + h) = y + hf + \frac{h^2}{2} (f_t + f_y f) + \frac{h^3}{3} \left( f_{tt} + 2 f_{ty} f + f_{yy} f f + f_y f_t + f_y f_y f \right) + \mathcal{O}(h^4) $$ we get: $$ \begin{align*} \sum_i b_i &amp;= 1 \\ \sum_i b_i c_i &amp;= \frac{1}{2} \\ \sum_i b_i c_i^2 &amp;= \frac{1}{3} \\ \sum_{i,j} b_i a_{ij} c_j &amp;= \frac{1}{6} \end{align*} $$ So up to order 3, the independent order conditions are $$ \begin{align*} \sum_i b_i = 1, \sum_i b_i c_i = \frac{1}{2}, \sum_i b_i c_i^2 = \frac{1}{3}, \sum_{i,j} b_i a_{ij} c_j = \frac{1}{6} \end{align*} $$ </p> <p> <strong>Order 4 conditions:</strong> $$ \begin{align*} \sum_i b_i c_i^3 = \frac{1}{4} \\ \sum_{i, j} b_i c_i a_{ij} c_j = \frac{1}{8} \\ \sum_{i, j} b_i a_{ij} c_j^2 = \frac{1}{12} \\ \sum_{i, j, \ell} b_i a_{ij} a_{j \ell} c_{\ell} = \frac{1}{24} \end{align*} $$ </p> <p> <strong>Sketch of derivation</strong><d-footnote>In the following derivations, we assume the general setting, where $ y(t) \in \mathbb{R}^d, f : \mathbb{R} \times \mathbb{R}^d \to \mathbb{R}^d. $</d-footnote> </p> <p> Extend the stage expansion by one more order to order 4: $$ \begin{align*} k_i &amp;= f + h(c_i f_t + c_i f_y f) \\ &amp;+ h^2\left( (\sum_j a_{ij}c_j)f_y f_t + (\sum_j a_{ij}c_j)f_y f_y f + \frac{1}{2} c_i^2 f_{tt} + c_i^2 f_{ty}f + \frac{1}{2} c_i^2 f_{yy}(f, f) \right) \\ &amp;+ h^3 \left( \frac{1}{2} (\sum_j a_{ij}c_j^2) f_y f_{tt} + (\sum_j a_{ij}c_j^2) f_y f_{ty} f + \frac{1}{2}(\sum_j a_{ij}c_j^2) f_y f_{yy}(f, f) \right. \\ &amp;\left. + (\sum_{j \ell} a_{ij} a_{j \ell} c_{\ell}) f_y f_y f_t + (\sum_{j \ell} a_{ij} a_{j \ell} c_{\ell}) f_y f_y f_y f + c_i(\sum_j a_{ij}c_j)f_{ty}f_t + c_i(\sum_j a_{ij}c_j) f_{ty} f_y f \right. \\ &amp;\left. + c_i(\sum_j a_{ij}c_j) f_{yy} (f_t, f) + c_i(\sum_j a_{ij}c_j) f_{yy}(f, f_y f) \right. \\ &amp;\left. + \frac{1}{6} c_i^3 f_{ttt} + \frac{1}{6} c_i^3 f_{yyy}(f, f, f) + \frac{1}{2} c_i^3 f_{tyy}(f, f) + \frac{1}{2} c_i^3 f_{tty} f \right) + \mathcal{O}(h^4) \end{align*} $$ </p> <p> Substitute into the update: $$ \begin{align*} y_{n+1} &amp;= y_n + h (\sum_i b_i) f + h^2 \left( (\sum_i b_i c_i)f_t + (\sum_i b_i c_i)f_y f \right) \\ &amp; + h^3 \left( (\sum_{ij} b_i a_{ij} c_j )f_y f_t + (\sum_{ij} b_i a_{ij} c_j ) f_y f_y f + \right. \\ &amp;\left. + \frac{1}{2} (\sum_i b_i c_i^2) f_{tt} + (\sum_i b_i c_i^2) f_{ty} f + \frac{1}{2} (\sum_i b_i c_i^2) f_{yy}(f, f) \right) \\ &amp;+ h^4 \left( \frac{1}{2}(\sum_{ij} b_i a_{ij} c_j^2) f_y f_{tt} + (\sum_{ij} b_i a_{ij} c_j^2) f_y f_{ty} f + \frac{1}{2} (\sum_i b_i a_{ij} c_j^2) f_y f_{yy}(f, f) \right. \\ &amp;\left. + (\sum_{i j \ell} b_i a_{ij} a_{j\ell} c_{\ell})f_y f_y f_t + (\sum_{i j \ell} b_i a_{ij} a_{j\ell} c_{\ell}) f_y f_y f_y f + (\sum_{ij} b_i c_i a_{ij} c_j) f_{ty} f_t + (\sum_{ij} b_i c_i a_{ij} c_j)f_{ty} f_y f \right. \\ &amp; \left. + (\sum_{ij} b_i c_i a_{ij} c_j) f_{yy}(f_t, f) + (\sum_{ij} b_i c_i a_{ij} c_j) f_{yy}(f, f_y f) + \frac{1}{6} (\sum_i b_i c_i^3) f_{ttt} \right. \\ &amp;\left. + \frac{1}{6} (\sum_i b_i c_i^3) f_{yyy}(f, f, f) + \frac{1}{2} (\sum_i b_i c_i^3) f_{tyy}(f, f) + \frac{1}{2} (\sum_i b_i c_i^3) f_{tty}f \right) + \mathcal{O}(h^5) \end{align*} $$ </p> <p> Match with the exact Taylor series $$ \begin{align*} y(t + h) &amp;= y + hf + \frac{h^2}{2} \left(f_t + f_y f \right) + \frac{h^3}{6} \left(f_{tt} + 2f_{ty}f + f_{yy}(f, f) + f_y f_t + f_y f_y f \right) \\ &amp; + \frac{h^4}{24} \left( f_{ttt} + 3 f_{tty}f + 3 f_{tyy}(f, f) + 3 f_{ty}f_t + 3 f_{ty} f_y f + f_{yyy}(f, f, f) + 3f_{yy}(f_t, f) \right. \\ &amp;\left. + 3f_{yy} (f_y f, f) + f_y f_{tt} + 2 f_y f_{ty} f + f_y f_{yy}(f, f) + f_y f_y f_t + f_y f_y f_y f \right) + \mathcal{O}(h^5) \end{align*} $$ </p> <p>Next, let us take a quick look at the classical RK4 method</p> <p> Butcher tableau: $$ \begin{array}{c|cccc} 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\ \frac{1}{2} &amp; \frac{1}{2} &amp; 0 &amp; 0 &amp; 0 \\ \frac{1}{2} &amp; 0 &amp; \frac{1}{2} &amp; 0 &amp; 0 \\ 1 &amp; 0 &amp; 0 &amp; 1 &amp; 0 \\ \hline &amp; \frac{1}{6} &amp; \frac{1}{3} &amp; \frac{1}{3} &amp; \frac{1}{6} \end{array} $$ </p> <p> Stage derivatives: $$ \begin{align*} k_1 &amp;= f(t_n ,y_n) \\ k_2 &amp;= f(t_n + \frac{1}{2}h, y_n + \frac{1}{2}h k_1)\\ k_3 &amp;= f(t_n + \frac{1}{2}h, y_n + \frac{1}{2}h k_2)\\ k_4 &amp;= f(t_n + h ,y_n + h k_3) \end{align*} $$ </p> <p> The numerical update: $$ y_{n+1} = y_n + \frac{h}{6} \left(k_1 + 2 k_2 + 2 k_3 + k_4 \right) $$ </p> <p>Next, we will construct a general 2-stage, order-2 explicit RK by using order conditions derived above.</p> <p> Let $$ \begin{array}{c|cc} 0 &amp; 0 &amp; 0 \\ c_2 &amp; a_{21} &amp; 0 \\ \hline &amp; b_1 &amp; b_2 \end{array} \quad (c_2 = a_{21}). $$ </p> <p> Order conditions: $$ b_1 + b_2 = 1, \; b_2 c_2 = \frac{1}{2} $$ </p> <p> Choose a free parameter $ c_2 = \alpha \neq 0 $. Then $$ b_2 = \frac{1}{2 \alpha}, \; b_1 = 1 - \frac{1}{2 \alpha} $$ Thus the Butcher tableau of the 2-stage explicit RK is $$ \begin{array}{c|cc} 0 &amp; 0 &amp; 0 \\ \alpha &amp; \alpha &amp; 0 \\ \hline &amp; 1 - \frac{1}{2 \alpha} &amp; \frac{1}{2 \alpha} \end{array} \quad (\alpha \neq 0) $$ </p> <p> Stage derivatives: $$ \begin{align*} k_1 &amp;= f(t_n, y_n) \\ k_2 &amp;= f(t_n + \alpha h, y_n + \alpha h k_1) \end{align*} $$ when $ \alpha = \frac{1}{2} $, we recover the midpoint method: $$ y_{n+1} = y_n + h f(t_n + \frac{1}{2}h, y_n + \frac{1}{2}h f(t_n, y_n)) $$ when $ \alpha = 1 $, we recover Heun's method (improved Euler): $$ y_{n+1} = y_n + \frac{h}{2}\left(f(t_n, y_n) + f(t_n +h, y_n + hf(t_n, y_n)) \right) $$ </p> <p>Last, let us take a look at an implicit RK2 method (i.e., implicit trapezoidal method or Crank-Nicolson method):</p> <p> Butcher tableau: $$ \begin{array}{c|cc} 0 &amp; 0 &amp; 0\\ 1 &amp; \frac{1}{2} &amp; \frac{1}{2}\\ \hline &amp; \frac{1}{2} &amp; \frac{1}{2} \end{array} $$ </p> <p> Update equations: $$ \begin{align*} k_1 &amp;= f(t_n, y_n) \\ k_2 &amp;= f(t_n + h, y_n +h(\frac{1}{2}k_1 + \frac{1}{2}k_2))\\ y_{n+1} &amp;= y_n + \frac{h}{2}(k_1 + k_2) \end{align*} $$ </p> <p> We can verify order conditions $$ \begin{align*} \sum_i b_i &amp;= \frac{1}{2} + \frac{1}{2} = 1 \\ \sum_i b_i c_i &amp;= \frac{1}{2} \cdot 0 + \frac{1}{2} \cdot 1 = \frac{1}{2} \end{align*} $$ </p> <h2> Concluding Remarks </h2> <p> Voila&#768;! This concludes a concise yet mathematically rigorous introduction to general one-step methods for numerically solving IVPs of ODEs. We have carefully derived the key convergence theorem—that <strong>consistency + stability (Lipschitz continuity) $ \implies $ convergence</strong>—and established the order conditions (up to order four) for general Runge–Kutta (RK) methods. </p> <p> As an aside, current large language models (LLMs) still tend to struggle with the detailed derivation of higher-order RK conditions, particularly beyond third order. For instance, expanding the stage derivatives $ k_i = f \left(t_n + c_i h, y_n + h \sum_{j=1}^s a_{ij}k_j \right) $ to order four using multivariate Taylor series, as we did above, is challenging for LLMs. I suspect this limitation arises from the relative scarcity of third-order differential terms (i.e., $ f_{ttt}, f_{tty}f, f_{tyy}(f, f), f_{yyy}(f, f, f), f_y f_{tt}, f_y f_{ty} f,f_y f_{yy}(f, f), f_y f_y f_t, f_y f_y f_y f $) in their pre-training corpora. In such cases, symbolic computation systems, also known as computer algebra systems (CAS), remain more suitable tools for handling the intricate algebraic manipulations required. Nevertheless, I believe that as long as LLMs can function as faithful interpolating functions of existing knowledge, they hold immense potential to transform the educational landscape, particularly by democratising access to advanced mathematical and scientific knowledge. </p> <h2> Acknowledgement </h2> <p>This blog was developed from a self-contained, step-by-step tutorial generated by OpenAI ChatGPT, which also assisted in refining the phrasing and expressions, as well as in suggesting suitable titles.</p> <div class="language-html highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@misc{Cai2025odelearningnotes-2,
    author = {Xin Cai},
    title = {Reconstructing the Scaffolding A Learner’s Notes on Ordinary Differential Equations: Part 2 Convergence and Order in One-Step (Runge–Kutta) Methods},
    howpublished = {\url{https://totalvariation.github.io/blog/2025/concise-intro-odes-part2/}},
    note = {Accessed: 2025-10-28},
    year = {2025}
}
</code></pre></div></div>]]></content><author><name>Xin Cai</name></author><summary type="html"><![CDATA[Part &#8545 Convergence and Order in One-Step (Runge–Kutta) Methods]]></summary></entry><entry><title type="html">Reconstructing the Scaffolding A Learner’s Notes on Ordinary Differential Equations</title><link href="https://totalvariation.github.io/blog/2025/concise-intro-odes-part3/" rel="alternate" type="text/html" title="Reconstructing the Scaffolding A Learner’s Notes on Ordinary Differential Equations"/><published>2025-10-28T00:00:00+00:00</published><updated>2025-10-28T00:00:00+00:00</updated><id>https://totalvariation.github.io/blog/2025/concise-intro-odes-part3</id><content type="html" xml:base="https://totalvariation.github.io/blog/2025/concise-intro-odes-part3/"><![CDATA[<div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <video src="/assets/video/traj-lmm.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" autoplay="" controls=""/> </figure> </div> </div> <div class="caption"> Animated Trajectories of Linear Multistep Methods on Lotka-Volterra Equations </div> <p> This post is part of my ongoing series of learning notes on Ordinary Differential Equations (ODEs). In this instalment, I will present detailed mathematical proofs of the Dahlquist Equivalence Theorem for Linear Multistep Methods (LMMs), which establishes that Convergence $ \iff $ Consistency + Zero-Stability. </p> <p> Throughout this blog, we focus on the initial value problems (IVPs) for one-dimensional ODEs for notational simplicity $$ y'(t) = f(t, y(t)), \; y(t_0) = y_0, \; t \in [t_0, T] $$ with $ f $ a sufficiently smooth (as specified when needed) scalar function. </p> <h2> Consistency and Zero-Stability </h2> <p> A $ k $-step linear multistep method (LMM) has the form $$ \sum_{j=0}^k \alpha_j y_{n+j} = h \sum_{j=0}^k \beta_j f_{n+j}, \alpha_k = 1, n \geq 0. $$ </p> <p>Define the generating polynomials $ \rho(\zeta) = \sum_{j=0}^k \alpha_j \zeta^j $, $ \sigma(\zeta) = \sum_{j=0}^k \beta_j \zeta^j $.</p> <h3>Consistency</h3> <p> Define the local truncation error (LTE) of an LMM by substituting the exact solution $ y(t) $ into the scheme: $$ \tau_{n+k} := \sum_{j=0}^k \alpha_j y(t_{n+j}) - h \sum_{j=0}^k \beta_j y'(t_{n+j}) $$ The method has order $ p $ if $ \tau_{n+k} = \mathcal{O}(h^{p+1}) $ for smooth $ y(t) $. </p> <p> Taylor expand about $ t_n $: $$ \begin{align*} y(t_{n+j}) &amp;= y(t_n) + jh \cdot y'(t_n) + \frac{(jh)^2}{2!} y''(t_n) + \cdots + \frac{(jh)^p}{p!}y^{(p)}(t_n) + \mathcal{O} (h^{p+1}) \\ y'(t_{n+j}) &amp;= y'(t_n) + jh \cdot y''(t_n) + \frac{(jh)^2}{2!} y'''(t_n) + \cdots + \frac{(jh)^{p-1}}{(p-1)!}y^{(p)}(t_n) + \mathcal{O} (h^{p+1}) \end{align*} $$ </p> <p> Insert into $ \tau_{n+k} $, collect powers of $ h $, and require the coefficients of $ y^{(\ell)}(t_n) $ to vanish for $ \ell = 0, 1, \ldots, p $. $$ \tau_{n+k} = C_0 y(t_n) + C_1 h y'(t_n) + \cdots + C_p h^p y^{(p)}(t_n) + \mathcal{O}(h^{p+1}) $$ $$ \begin{align*} C_0 &amp;: \sum_{j=0}^k \alpha_j = \rho(1) = 0. \\ C_1 &amp;: \sum_{j=0}^k j \alpha_j = \sum_{j=0}^k \beta_j, \iff \rho'(1) = \sigma(1). \end{align*} $$ </p> <p> In general, the coefficient of $ h^{\ell}y^{(\ell)}(t_n) $ for $ \ell \geq 1 $ is $$ \frac{1}{\ell !} \sum_{j=0}^k \alpha_j j^{\ell} - \frac{1}{(\ell - 1) !} \sum_{j=0}^k \beta_j j^{\ell - 1}. $$ For order $ p $, these must vanish for $ \ell = 1, 2, \ldots, p $: $$ \sum_{j=0}^k \alpha_j j^{\ell} = \ell \sum_{j=0}^k \beta_j j^{\ell - 1}, \; \ell = 0, 1, \ldots, p, $$ An LMM is consistent if $ C_0 = C_1 = 0 $, i.e., $ \rho(1) = 0 $ and $ \rho'(1) = \sigma(1) $, which means $ \frac{1}{h} \tau_{n+k} = \mathcal{O}(h) $. </p> <h3>Zero-Stability</h3> <p> Consider the homogeneous recurrence with a perturbation: $$ \sum_{j=0}^k \alpha_j w_{n+j} = \delta_{n+k}, \; n \geq 0, $$ with given starting values $ w_0, w_1, \ldots, w_{k-1} $. </p> <p> The LMM is zero-stable if there exists a constant $ C $ independent of $ h $ such that $$ \max_{0 \leq n \leq N} |w_n| \leq C \left(\max_{0 \leq j \leq k-1} |w_j| + \max_{k \leq m \leq N} |\delta_m| \right) $$ </p> <p><strong>The root condition</strong></p> <p> Let $ \rho(\zeta) = \sum_{j=0}^k \alpha_j \zeta^j $. The root condition is: All roots of $ \rho $ satisfy $ |\zeta| \le 1 $, and any root with $ |\zeta| = 1 $ is simple (i.e., algebraic multiplicity is equal to 1). </p> <p><strong>Theorem</strong> An LMM is zero-stable $ \iff $ its $ \rho(\zeta) $ satisfies the root condition.</p> <p>Assume the root condition holds. Consider the homogeneous recurrence $ \sum_{j=0}^k \alpha_j w_{n+j} = 0, \; \alpha_k = 1 $.</p> <p> Introduce the companion matrix $$ \mathbf{C} = \begin{pmatrix} 0 &amp; 1 &amp; \cdots &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 1 &amp; \cdots &amp; 0 \\ \vdots &amp; \vdots &amp; \ddots &amp; \ddots &amp;\vdots \\ 0 &amp; 0 &amp; \cdots &amp; 0 &amp; 1 \\ - \alpha_0 &amp; - \alpha_1 &amp; \cdots &amp; - \alpha_{k-2} &amp; -\alpha_{k-1} \end{pmatrix}_{k \times k} $$ and $$ \mathbf{w}_n = \begin{pmatrix} w_{n} \\ \vdots \\ w_{n+k-2} \\ w_{n+k-1} \end{pmatrix} $$ $$ \mathbf{w}_{n+1} = \mathbf{C} \mathbf{w}_n $$ The eigenvalues of $ \mathbf{C} $ are exactly the roots of $ \rho $. </p> <p><strong>Companion Matrix of a Polynomial</strong></p> <p> For each monic polynomial $$ p(x) = x^n + \alpha_{n-1}x^{n-1} + \cdots + \alpha_1 x + \alpha_0, $$ the companion matrix $ p(x) $ is defined to be $$ \mathbf{C} = \begin{pmatrix} 0 &amp; 0 &amp; \cdots &amp; 0 &amp; -\alpha_0 \\ 1 &amp; 0 &amp; \cdots &amp; 0 &amp; -\alpha_1 \\ \vdots &amp; \ddots &amp; \ddots &amp; &amp;\vdots \\ 0 &amp; \cdots &amp; 1 &amp; 0 &amp; -\alpha_{n-2} \\ 0 &amp; 0 &amp; \cdots &amp; 1 &amp; -\alpha_{n-1} \end{pmatrix}_{n \times n} $$ </p> <p> The polynomial $ p(x) $ is both the characteristic and minimum polynomial for $ \mathbf{C} $ (i.e., $ \mathbf{C} $ is nonderogatory), which implies $ \text{geo multi}(\lambda_j) = 1 $ for each $ \lambda_j $, or, equivalently, $ \text{alg mult}(\lambda_j) = \text{index}(\lambda_j) $ for each $ \lambda_j \in \sigma(\mathbf{C}) $. </p> <details><summary>Click here to know more</summary> <p><strong>Proof</strong><d-footnote>The proof is borrowed from the book Matrix Analysis and Applied Linear Algebra <d-cite key="meyer2023matrix"></d-cite>. Please refer to the 7.11 for more details.</d-footnote>:</p> <p> Write $ \mathbf{C} = \mathbf{N} - \mathbf{c}\mathbf{e}^T_n $, where $$ \mathbf{N} = \begin{pmatrix} 0 &amp; &amp; &amp; \\ 1 &amp; \ddots &amp; &amp; \\ &amp; \ddots &amp; \ddots &amp; \\ &amp; &amp; 1 &amp; 0 \end{pmatrix} $$ and $$ \mathbf{c} = \begin{pmatrix} \alpha_0 \\ \alpha_1 \\ \vdots \\ \alpha_{n-1} \end{pmatrix} $$ </p> <p> $$ \begin{align*} \det (x\mathbf{I} - \mathbf{C}) &amp;= \det(x\mathbf{I} - \mathbf{N} + \mathbf{c}\mathbf{e}^T_n) \\ &amp;= \det \left((x\mathbf{I} - \mathbf{N})(\mathbf{I} - (x\mathbf{I} - \mathbf{N})^{-1}\mathbf{c}\mathbf{e}^T_n) \right) \\ &amp;= \det(x\mathbf{I} - \mathbf{N}) \left(1 + \mathbf{e}_n^T(x\mathbf{I} - \mathbf{N})^{-1}\mathbf{c} \right)\\ &amp;= x^n \left( 1 + \mathbf{e}^T_n\left(\frac{\mathbf{I}}{x} + \frac{\mathbf{N}}{x^2} + \frac{\mathbf{N}^2}{x^3} + \cdots + \frac{\mathbf{N}^{n-1}}{x^n} \right)\mathbf{c} \right) \\ &amp;= x^n + \alpha_{n-1} x^{n-1} + \alpha_{n-2} x^{n-2} + \cdots + \alpha_0 = p(x) \end{align*} $$ </p> <p> Set $ \mathcal{B} = \{ \mathbf{e}_1, \mathbf{e}_2, \ldots, \mathbf{e}_n \} $, and let $ v_i(x) = x^k - \sum_{j=0}^{k-1} \alpha_j x^j $ be the minimum polynomial of $ \mathbf{e}_i $ w.r.t. $ \mathbf{C} $, i.e., $ v(\mathbf{A})\mathbf{e}_i = \mathbf{0} $. Observe that $ \mathbf{C}\mathbf{e}_j = \mathbf{e}_{j+1} $ for $ j = 1, \ldots, n-1 $, so $$ \{ \mathbf{e}_1, \mathbf{C}\mathbf{e}_1, \mathbf{C}^2 \mathbf{e}_1, \ldots, \mathbf{C}^{n-1} \mathbf{e}_1 \} = \{ \mathbf{e}_1, \mathbf{e}_2, \mathbf{e}_3, \ldots, \mathbf{e}_n \} $$ and $$ \mathbf{C}^n \mathbf{e}_1 = \mathbf{C} \mathbf{e}_n = \mathbf{C}_{\star n} = - \sum_{j=0}^{n-1} \alpha_j \mathbf{e}_{j+1} = - \sum_{j=0}^{n-1} \alpha_j \mathbf{C}^j \mathbf{e}_j \implies v_1(x) = p(x) $$ </p> <p> Since $ v_1(x) $ divides the least common multiple polynomial of all $ v_i(x) $'s, which is known to be the minimum polynomial $ m(x) $ of $ \mathbf{C} $, we conclude that $ p(x) $ divides $ m(x) $. Given $ m(x) $ always divides $ p(x) $, so $ m(x) = p(x) $. </p> </details> <p><strong>Proof of the sufficiency half:</strong></p> <p> By the root condition, every eigenvalue $ \lambda $ satisfies $ |\lambda| \le 1 $, with the Jordan block corresponding to $ | \lambda | = 1 $ being of size $ 1 \times 1 $ as $ \text{alg mult}(\lambda) = \text{index}(\lambda) $ for each $ \lambda \in \mathbf{C} $. Therefore, $ \mathbf{C} $ is power-bounded: $$ \exists M \; \text{s.t.}\; \| \mathbf{C} \| \leq M \; \forall n \geq 0 $$ </p> <details><summary>Click here to know more</summary> <p><strong>Sketch of proof:</strong></p> <p> Converting $ \mathbf{C} = \mathbf{S} \mathbf{J} \mathbf{S}^{-1} $ by similarity transformation to its Jordan form. Blocks with $ | \lambda | &lt; 1 $ decay to zero as $ n \to \infty $; $$ \mathbf{J}_{\star}^k = \begin{pmatrix} \lambda &amp; 1 &amp; &amp; \\ &amp; \ddots &amp; \ddots &amp;\\ &amp; &amp; \ddots &amp; 1\\ &amp; &amp; &amp; \lambda \end{pmatrix}_{m \times m}^k = \begin{pmatrix} \lambda^k &amp; \begin{pmatrix} k \\ 1 \end{pmatrix} \lambda^{k-1} &amp; \begin{pmatrix} k \\ 2 \end{pmatrix} \lambda^{k-2} &amp; \cdots &amp; \begin{pmatrix} k \\ m-1 \end{pmatrix} \lambda^{k-m+1} \\ &amp; \lambda^k &amp; \begin{pmatrix} k \\ 1 \end{pmatrix} \lambda^{k-1} &amp; \ddots &amp; \vdots \\ &amp; &amp; \ddots &amp; \ddots &amp; \begin{pmatrix} k \\ 2 \end{pmatrix} \lambda^{k-2} \\ &amp; &amp; &amp; \lambda^k &amp; \begin{pmatrix} k \\ 1 \end{pmatrix} \lambda^{k-1} \\ &amp; &amp; &amp; &amp; \lambda^k \end{pmatrix}_{m \times m} $$ </p> <p> It is clear that if $ |\lambda| &lt; 1 $, $ \lim_{k \to \infty} \lambda^k = 0 $ and $ \lim_{k \to \infty} \begin{pmatrix} k\\ j \end{pmatrix} \lambda^{k-j} = 0 $ for each fixed value of $ j $. </p> <p> The block with $ |\lambda| = 1 $ is $ \left[ 1 \right]_{1 \times 1} $. Thus, $ \| \mathbf{C}^n \| \le \| \mathbf{S} \| \| \mathbf{S}^{-1} \| $ (e.g., $ \| \mathbf{C}^n \|_2 \le \frac{\sigma_1}{\sigma_n} $, where $ \sigma_1 $ and $ \sigma_n $ are the largest and smallest singular values of $ \mathbf{S} $ respectively, and matrix norms are equivalent) uniformly in $ n $. </p> </details> <p> For the perturbed recurrence $ \sum_{j=0}^k \alpha_j w_{n+j} = \delta_{n+k}, \; n \geq 0 $, reformulate it as the matrix-vector form: $$ \mathbf{w}_{n+1} = \mathbf{C} \mathbf{w}_n + \mathbf{e}_k \delta_{n+k}, \; \text{with} \; \mathbf{w}_{n} = \left(w_{n}, \cdots , w_{n+k-2} , w_{n+k-1} \right)^T, \; \mathbf{e}_k = \left(0, 0, \ldots, 1 \right)^T. $$ </p> <p> Unroll: $$ \mathbf{w}_n = \mathbf{C}^n \mathbf{w}_0 + \sum_{m=0}^{n-1} \mathbf{C}^{n-1-m}\mathbf{e}_k \delta_{m+k} $$ $$ \| \mathbf{w}_n \|_{\infty} \leq M \| \mathbf{w}_0 \|_{\infty} + M \sum_{m=0}^{n-1} | \delta_{m+k} | \leq M \left( \|\mathbf{w}_0\|_{\infty} + \sum_{j=k}^{n-1+k} | \delta_j | \right) \le M \left( \max_{0 \le j \le k-1} |w_j| + \max_{k \le m \le N} |\delta_m| \right). $$ </p> <p><strong>Proof of the necessity half:</strong></p> <p> The general solution to the homogeneous linear difference equation $ \sum_{j=0}^k \alpha_j w_{n+j} = 0 $ can be expressed as $ w_n = \sum_{j=1}^s \sum_{\ell = 0}^{m_j - 1} c_{j, \ell}n^{\ell}\zeta_j^n $, where $ \zeta_j $ are distinct complex roots of the characteristic polynomial $ p(\zeta) = a_k \prod_{j=1}^s (\zeta - \zeta_j)^{m_j} $, and $ m_j $ are their respective multiplicities satisfying $ \sum_{j=1}^s m_j = k $. </p> <p><em>Proof by contradiction</em></p> <p> <ul> <li>If there is a root $ | \zeta | &gt; 1 $, then the homogeneous solution contains terms $ | \zeta |^n $ growing exponentially.</li> <li>If there is a root $ | \zeta | = 1 $ with multiplicity $ \ge 2 $, the homogenous solution contains $ n \zeta $, which grows linearly in $ n $, which is also unbounded since $ n = \frac{T - t_0}{h} \to \infty \; \text{as} \; h \to 0 $. Thus, zero-stability implies the root condition.</li> </ul> </p> <h2>Convergence Theorem (Consistency + Zero-Stability $ \iff $ Convergence)</h2> <p> <strong>Convergence of linear multistep methods</strong> A $ k $-step linear multi-step method is convergent on $ [t_0, T] $ if for sufficiently smooth IVP $ y' = f(t, y) $ with the exact solution $ y(t) $, and for any family of starting values $ y_j(h), j = 0, \ldots, k-1 $ satisfying $ \lim_{h \to 0} | y(t_j) - y_j(h) | = 0, \; j = 0, \ldots, k-1 $ and the numerical solution $ \{ y_n(h) \}_{n=0}^{N(h)} $ produced by the LMM satisfies $ \lim_{h \to 0} \max_{0 \le n \le N(h)} | e_n | = 0 $ with $ e_n := y(t_n) - y_n(h) $. </p> <p><strong>Dahlquist Equivalence Theorem for Linear Multistep Methods</strong> If a linear multistep method for solving ODE IVPs is convergent, i.e., the global error \(e_n := y(t_n) - y_n\) satisfies \(\max_{0 \leq n \leq N} | e_n | \to 0 \; \text{as} \; h \to 0\), if and only if it is consistent and zero-stable. Moreover, if \(\max_{0 \leq n \leq N} | e_n | \leq C h^p\), then the linear multistep method is of order \(p\).</p> <p>Proof of sufficiency (Consistency + Zero-Stability \(\implies\) Convergence):</p> <p>Subtracting the numerical scheme from the equation of the local truncation error \(\tau_{n+k} := \sum_{j=0}^k \alpha_j y(t_{n+j}) - h \sum_{j=0}^k \beta_j f(t_{n+j}, y(t_{n+j}))\):</p> \[\sum_{j=0}^k \alpha_j e_{n+j} = h \sum_{j=0}^k \beta_j \left(f(t_{n+j}, y(t_{n+j})) - f(t_{n+j}, y_{n+j}) \right) + \tau_{n+k}\] <p>Let \(r_{n+k} = \sum_{j=0}^k \beta_j \left(f(t_{n+j}, y(t_{n+j})) - f(t_{n+j}, y_{n+j}) \right)\), then the error recurrence can be viewed as a perturbed linear recurrence \(\sum_{j=0}^k \alpha_j e_{n+j} = \delta_{n+k}, \; n \geq 0\) with \(\delta_{n+k} = r_{n+k} + \tau_{n+k}\). By zero-stability, there exists \(C_0\) (independent of \(h\)) such that</p> \[\max_{0 \leq n \leq N} | e_n | \leq C_0 \left( \max_{0 \leq j \leq k-1} | e_j | + \sum_{m=k}^N | r_m | + \sum_{m=k}^N | \tau_m | \right)\] <table> <tbody> <tr> <td>The starting errors \(e_0, \ldots, e_{k-1}\) are assumed \(\mathcal{O}(h^p)\), which are usually obtained by a one-step method of order \(p\), so $$ \max_{0 \leq j \leq k-1}</td> <td>e_j</td> <td>\leq C_{s}h^p $$.</td> </tr> </tbody> </table> <table> <tbody> <tr> <td>The LTE has order \(p\) by assumption: $$</td> <td>\tau_m</td> <td>\leq C_{\tau} h^{p+1} \(. Hence\) \sum_{m=k}^N</td> <td>\tau_m</td> <td>\leq C_{\tau} (Nh) h^p = C_{\tau}(T - t_0) h^p $$.</td> </tr> </tbody> </table> <table> <tbody> <tr> <td>Using Lipschitz continuity of \(f\), i.e., $$</td> <td>f(t, y_1) - f(t, y_2)</td> <td>\leq L</td> <td>y_1 - y_2</td> <td>\(to obtain an error bound of terms\) r_m $$,</td> </tr> </tbody> </table> \[| \sum_{j=0}^k \beta_j \left( f(t_{n+j}, y(t_{n+j})) - f(t_{n+j}, y_{n+j}) \right) | \le L \sum_{j=0}^k | \beta_j | | e_{n+j} | \leq \widetilde{L} \max_{0 \leq j \leq k} | e_{n+j} |, \; \text{where} \; \widetilde{L} = L\sum_{j=0}^k | \beta_j |\] \[h \sum_{m=k}^{N}|r_m|\le h \widetilde{L} \sum_{m=k}^{N} \max_{0\le j\le k}|e_{m-k+j}| \le \widetilde{L} h \sum_{m=0}^{N} \, \max_{0\le j\le k}|e_{m-k+j}|\] <table> <tbody> <tr> <td>Let $$ E := \max_{0\le n\le N}</td> <td>e_n</td> <td>\(,\) h\sum_{m=k}^N</td> <td>r_m</td> <td>\leq \widetilde{L}(T - t_0)E $$.</td> </tr> </tbody> </table> <p>Collecting terms,</p> \[E \leq C_0\left(C_s h^p + C_{\tau}(T - t_0)h^p + \widetilde{L}(T - t_0)E \right)\] \[E \le \frac{C_0}{1 - C_0\widetilde{L}(T - t_0)}\left(C_s + C_{\tau}(T - t_0) \right) h^p = \mathcal{O}(h^p)\] <table> <tbody> <tr> <td>If \(1 - C_0\widetilde{L}(T - t_0) &gt; 0\), we are done. If not, we use a standard patching trick: splitting the time interval \([t_0, T]\) into \(M\) short subintervals of length \(\Delta &lt; T - t_0\), chosen such that \(C_0\widetilde{L}\Delta \le \frac{1}{2}\). One the first subinterval, the error bound derived above holds true, implying the local maximum error is of order \(\mathcal{O}(h^p)\). Therefore, the endpoints of the previous subinterval can be used as starting values of the next subinterval, leading to the same error estimate by applying the above derivation. Iterating over a finite number of subintervals yields an overall bound $$ \max_{0 \le n \le N}</td> <td>e_n</td> <td>\le Ch^p $$.</td> </tr> </tbody> </table> <p>Furthermore, it is evident that the source of global error of LMMs can be decomposed into two terms:</p> \[\max_{0\le n\le N} |e_n| \le C_1 \cdot \underbrace{\max_{0 \le j \le k-1} | e_j |}_{\text{initial error}} + C_2 \cdot \underbrace{\max_{ k \le m \le N} |\tau_m |}_{\substack{\text{the sum of LTE} \\ \text{controlled by} \\ \text{zero-stability}}}\] <p>Proof of necessity (Convergence \(\implies\) Consistency + Zero-Stability):</p> <ol> <li>Convergence \(\implies\) Consistency</li> </ol> <p>Subtracting the numerical scheme from the formula defining the LTE \(\tau_{n+k}\):</p> \[\tau_{n+k} = \sum_{j=0}^k \alpha_j \left( y(t_{n+j}) - y_{n+j}(h) \right) + h \sum_{j=0}^k \beta_j \left( f(t_{n+j}, y_{n+j}(h)) - f(t_{n+j}, y(t_{n+j})) \right)\] <p>By the Lipschitz continuity of \(f\),</p> \[|\tau_{n+k}| \le \left( \sum_{j=0}^k |\alpha_j| \right) \max_{0 \le j \le k} | e_{n+j}(h) | + hL\left( \sum_{j=0}^k |\beta_j| \right) \max_{0 \le j \le k} | e_{n+j}(h) |\] <p>Hence</p> \[|\tau_{n+k}| \le C(h) \max_{0 \le j \le k} | e_{n+j}(h) |\] <table> <tbody> <tr> <td>Since \(e_n(h) \to 0\) (by convergence), $$ \lim_{h \to 0}</td> <td>\tau_{n+k}(h)</td> <td>= 0 \; \forall n \ge 0 $$.</td> </tr> </tbody> </table> <p>Finally, applying this to the test function \(y(t) = 1\) and \(y(t) = t\) gives the usual algebraic conditions. Specifically,</p> <p>If \(y(t) = 1\), then \(y'(t) = 0\) and \(\tau = \sum_{j=0}^k \alpha_j = \rho(1)\). Since \(\tau \to 0\), we get \(\rho(1) = 0\).</p> <p>If \(y(t) = t\), then \(y(t_{n+j}) = t_n + jh, y'(t_{n+j}) = 1\). Then</p> \[\tau = t_n \sum_j \alpha_j + h\left( \sum_j j \alpha_j - \sum_j \beta_j \right)\] <p>Using \(\sum_j \alpha_j = 0\), we conclude that \(\sum_j j \alpha_j = \sum_j \beta_j\), i.e., \(\rho'(1) = \sigma(1)\).</p> <ol> <li>Convergence \(\implies\) Zero-Stability</li> </ol> <p>Proof by contradiction. Assume the method is convergent as defined above, but not zero-stable. Then \(\rho\) violates the root condition:</p> <ul> <li> Case A: $ \rho $ has a root $ \zeta_0 $ with $ | \zeta_0 | \ge 1 $;</li> <li> Case B: $ \rho $ has a root $ \zeta_0 $ on the unit circle but with algebraic multiplicity $ m \ge 2 $.</li> </ul> <p>To arrive at the contradiction, in each case, we only need to find a family of starting perturbations whose magnitude tends to zero as \(h \to 0\), but for which the propagated solutions as per the numerical scheme \(\sum_{j=0}^k \alpha_j y_{n+j} = h \sum_{j=0}^k \beta_j f_{n+j}\) do not tend to zero.</p> <p>Recall that the homogeneous recurrence \(\sum_{j=0}^k \alpha_j w_{n+j} = 0\) can be written in the matrix-vector form \(\mathbf{w}_{n+1} = \mathbf{C} \mathbf{w}_n\), where \(\mathbf{w}_n = ( w_{n}, \ldots, w_{n+k-2}, w_{n+k-1})^T\) and \(\mathbf{C}\) is the companion matrix</p> \[\mathbf{C} = \begin{pmatrix} 0 &amp; 1 &amp; \cdots &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 1 &amp; \cdots &amp; 0 \\ \vdots &amp; \vdots &amp; \ddots &amp; \ddots &amp;\vdots \\ 0 &amp; 0 &amp; \cdots &amp; 0 &amp; 1 \\ - \alpha_0 &amp; - \alpha_1 &amp; \cdots &amp; - \alpha_{k-2} &amp; -\alpha_{k-1} \end{pmatrix}_{k \times k}\] <p>The polynomial \(p(\zeta) = \sum_{j=0}^k \alpha_j \zeta^j, \alpha_k = 1\) is both the characteristic and minimum polynomial for \(\mathbf{C}\), which means the eigenvalues of \(\mathbf{C}\) are exactly the roots of \(\rho\) and \(\text{alg mult}(\lambda_j) = \text{index}(\lambda_j)\) for each eigenvalue \(\lambda_j \in \sigma(\mathbf{C})\).</p> <table> <tbody> <tr> <td>Case A: $$</td> <td>\zeta_0</td> <td>\ge 1 $$.</td> </tr> </tbody> </table> <p>Let \(\mathbf{v}\) be a (right) eigenvector of \(C\) associated to \(\zeta_0\): \(\mathbf{Cv} = \zeta_0 \mathbf{v}\). Consider the homogeneous solution generated by initial state \(\mathbf{w}_0 = \eta(h) \mathbf{v}\). Then</p> \[\mathbf{w}_n = \mathbf{C}^n \mathbf{w}_0 = \eta(h) \zeta_0^n \mathbf{v}\] <table> <tbody> <tr> <td>Pick $$ \eta(h) =</td> <td>\zeta_0</td> <td>^{-N(h)} \(. Then\) \eta(h) \to 0 \; \text{as} \; h \to 0 \(, ensuring the starting perturbation\) \mathbf{w}_0 \to 0 \; \text{as} \; h \to 0 \(, and thus it is an admissible family of starting perturbations under our convergence definition. But the propagated solution at the step\) n = N(h) $$ is</td> </tr> </tbody> </table> \[\mathbf{w}_{N(h)} = \eta(h) \zeta_0^{N(h)} \mathbf{v} = \left( \frac{\zeta_0}{|\zeta_0|} \right)^{N(h)} \mathbf{v}\] <table> <tbody> <tr> <td>Since \(| \mathbf{w}_{N(h)} | = | v |\), the propagated perturbation does not tend to zero, contradicting convergence (which requires that any starting perturbation tending to zero produces a final error tending to zero). Therefore, there is no root with $$</td> <td>\zeta_0</td> <td>\ge 1 $$.</td> </tr> </tbody> </table> <table> <tbody> <tr> <td>Case B: $$</td> <td>\zeta_0</td> <td>= 1 \(and algebraic multiplicity\) m \ge 2 $$.</td> </tr> </tbody> </table> <p>In this case, the companion matrix \(\mathbf{C}\) has a Jordan block for \(\zeta_0\) of size \(m \times m\):</p> \[\begin{pmatrix} \zeta_0 &amp; 1 &amp; &amp; \\ &amp; \ddots &amp; \ddots &amp; \\ &amp; &amp; \ddots &amp; 1 \\ &amp; &amp; &amp; \zeta_0 \end{pmatrix}_{m \times m}.\] <p>The nonderogatory property of the companion matrix \(\mathbf{C}\) indicates that \(\text{alg mult}(\lambda_j) = \text{index}(\lambda_j)\) for each eigenvalue \(\lambda_j \in \sigma(\mathbf{C})\), excluding the possibility of eigenvalues being semisimple, i.e., the diagonal block \(\begin{pmatrix} \lambda &amp; &amp; \\ &amp; \ddots &amp; \\ &amp; &amp; \lambda \end{pmatrix}_{m \times m}\).</p> \[\mathbf{C}^n = \mathbf{S} \begin{pmatrix} \ddots &amp; &amp; \\ &amp; \mathbf{J}^n_{\star} &amp; \\ &amp; &amp; \ddots \end{pmatrix} \mathbf{S}^{-1}\] <p>where</p> \[\mathbf{J}_{\star}^n = \begin{pmatrix} \zeta_0 &amp; 1 &amp; &amp; \\ &amp; \ddots &amp; \ddots &amp;\\ &amp; &amp; \ddots &amp; 1\\ &amp; &amp; &amp; \zeta_0 \end{pmatrix}_{m \times m}^n = \begin{pmatrix} \zeta_0^n &amp; \begin{pmatrix} n \\ 1 \end{pmatrix} \zeta_0^{n-1} &amp; \begin{pmatrix} n \\ 2 \end{pmatrix} \zeta_0^{n-2} &amp; \cdots &amp; \begin{pmatrix} n \\ m-1 \end{pmatrix} \zeta_0^{n-m+1} \\ &amp; \zeta_0^n &amp; \begin{pmatrix} n \\ 1 \end{pmatrix} \zeta_0^{n-1} &amp; \ddots &amp; \vdots \\ &amp; &amp; \ddots &amp; \ddots &amp; \begin{pmatrix} n \\ 2 \end{pmatrix} \zeta_0^{n-2} \\ &amp; &amp; &amp; \zeta_0^n &amp; \begin{pmatrix} n \\ 1 \end{pmatrix} \zeta_0^{n-1} \\ &amp; &amp; &amp; &amp; \zeta_0^n \end{pmatrix}_{m \times m}\] <table> <tbody> <tr> <td>Focus on the single \(m \times m\) Jordan block \(\mathbf{J}^n_{\star}\) associated with the eigenvalue \(\zeta_0\), and \(\mathbf{S}_{\star} = \left[\mathbf{v}_1, \mathbf{v}_2, \ldots, \mathbf{v}_m \right]\) be the portion of $$ \mathbf{S} = \left[ \cdots</td> <td>\mathbf{S}_{\star}</td> <td>\cdots \right] \(corresponding to the position of\) \mathbf{J}^n_{\star} $ in $ \mathbf{J}^n \(. Then,\) \mathbf{C}^n \mathbf{S} = \mathbf{S} \begin{pmatrix} \ddots &amp; &amp; \ &amp; \mathbf{J}^n_{\star} &amp; \ &amp; &amp; \ddots \end{pmatrix} \(implies\) \mathbf{C}^n\mathbf{S}<em>{\star} = \mathbf{S}</em>{\star} \mathbf{J}^n_{\star}(\zeta_0) $$, i.e.,</td> </tr> </tbody> </table> \[\mathbf{C}^n [\mathbf{v}_1, \mathbf{v}_2, \cdots, \mathbf{v}_m ] = [\mathbf{v}_1, \mathbf{v}_2, \cdots, \mathbf{v}_m ] \mathbf{J}_{\star}^n = [\mathbf{v}_1, \mathbf{v}_2, \cdots, \mathbf{v}_m ] \begin{pmatrix} \zeta_0^n &amp; \begin{pmatrix} n \\ 1 \end{pmatrix} \zeta_0^{n-1} &amp; \begin{pmatrix} n \\ 2 \end{pmatrix} \zeta_0^{n-2} &amp; \cdots &amp; \begin{pmatrix} n \\ m-1 \end{pmatrix} \zeta_0^{n-m+1} \\ &amp; \zeta_0^n &amp; \begin{pmatrix} n \\ 1 \end{pmatrix} \zeta_0^{n-1} &amp; \ddots &amp; \vdots \\ &amp; &amp; \ddots &amp; \ddots &amp; \begin{pmatrix} n \\ 2 \end{pmatrix} \zeta_0^{n-2} \\ &amp; &amp; &amp; \zeta_0^n &amp; \begin{pmatrix} n \\ 1 \end{pmatrix} \zeta_0^{n-1} \\ &amp; &amp; &amp; &amp; \zeta_0^n \end{pmatrix}_{m \times m}\] <p>Pick a generalised eigenvector in \(\mathbf{S}_{\star}\) of the highest order \(\mathbf{v}_m\), then</p> \[\| \mathbf{C}^n \mathbf{v}_m \|_{\infty} = \| \mathbf{v}_m \mathbf{J}^n_{\star} \|_{\infty} \propto K n^{m-1}\] <p>Now constructing starting data as follows. Set \(\eta(h) = N(h)^{-(m-1)}\). Then, take the initial perturbation as \(\mathbf{w}_0 = \eta(h)\mathbf{v}_m\), which is admissible as \(\eta(h) \to 0 \; \text{as} \; h \to 0\).</p> <p>But the propagated perturbation at step \(n = N(h)\) is</p> \[\| \mathbf{w}_{N(h)} \|_{\infty} = \| \mathbf{C}^{N(h)} \mathbf{w}_0 \|_{\infty} \propto \eta(h) K N(h)^{m-1} = K\] <p>i.e., a nonzero constant independent of \(h\). Thus the final error does not vanish, again contradicting convergence.</p> <table> <tbody> <tr> <td>Combining the conclusions obtained above shows that every root \(\zeta\) of \(\rho\) satisfies $$</td> <td>\zeta</td> <td>\le 1 \(and any root with\)</td> <td>\zeta</td> <td>= 1 $$ is simple (algebraic multiplicity 1), which is exactly the root condition.</td> </tr> </tbody> </table> <p>Next, let us take a quick look at some important linear multi-step methods.</p> <ol> <li>Adams-Bashforth methods</li> </ol> <p>Starting from the integral form:</p> \[y(t_{n+1}) = y(t_n) + \int_{t_n}^{t_{n+1}} f(s, y(s)) ds\] <p>Approximate \(f(s, y(s))\) by the Lagrange interpolating polynomials.</p> <p>AB2 (order 2)</p> <p>Interpolate \(f\) through \(t_n, t_{n-1}\):</p> \[P_1(s) = f_n \cdot \frac{s - t_{n-1}}{t_n - t_{n-1}} + f_{n-1} \cdot \frac{s - t_n}{t_{n-1} - t_n} = f_n \cdot \frac{s - t_{n-1}}{h} - f_{n-1} \cdot \frac{s - t_n}{h}\] <p>Integrate on \([t_n, t_{n+1}]\) (set \(\tau = s - t_n \in [0, h]\)):</p> \[\int_{t_n}^{t_{n+1}} P_1(s)ds = \int_0^h \left( f_n \frac{\tau + h}{h} - f_{n-1} \frac{\tau}{h} \right) d\tau = h\left(\frac{3}{2} f_n - \frac{1}{2} f_{n-1} \right)\] <p>Thus</p> \[y_{n+1} = y_n + h\left( \frac{3}{2} f_n - \frac{1}{2} f_{n-1} \right)\] <p>Shifting the index forward to conform to the general form of a k-step LMM:</p> \[y_{n+2} - y_{n+1} = h\left( \frac{3}{2} f_{n+1} - \frac{1}{2} f_n \right)\] <p>with coefficients \((\alpha_0, \alpha_1, \alpha_2) = (0, -1, 1)\) and \((\beta_0, \beta_1, \beta_2) = (-\frac{1}{2}, \frac{3}{2}, 0)\).</p> <p>It is straightforward to check the consistency and order conditions:</p> \[\sum_j \alpha_j = 0 + (-1) + 1 = 0\] \[\begin{align*} \sum_j \alpha_j &amp;= 0 + (1 \cdot -1) + (2 \cdot 1) = 1 \\ \sum_j \beta_j &amp;= - \frac{1}{2} + \frac{3}{2} + 0 =1 \end{align*}\] \[\begin{align*} \sum_j j^2 \alpha_j &amp;= 0 + (1 \cdot -1) + ( 2^2 \cdot 1) = 3\\ 2 \cdot \sum_j j \beta_j &amp;= 2\left( (0 \cdot - \frac{1}{2}) + (1 \cdot \frac{3}{2}) + (2 \cdot 0) \right) \end{align*}\] \[\begin{align*} \sum_j j^3 \alpha_j &amp;= 0 + (1 \cdot -1) + (2^3 \cdot 1) = 7 \\ 3 \cdot \sum_j j^2 \beta_j &amp;= 3\left( (0 \cdot - \frac{1}{2}) + (1 \cdot \frac{3}{2}) + (2^2 \cdot 0) \right) = \frac{9}{2} \end{align*}\] <p>The LTE of AB2 is of order \(\mathcal{O}(h^3)\), and thus AB2 is of order \(\mathcal{O}(h^2)\).</p> <ol> <li>Adams-Moulton methods</li> </ol> <p>AM2 (trapezoidal rule)</p> <p>Interpolate through \(t_n, t_{n+1}\):</p> \[P_1(s) = f_n \cdot \frac{t_{n+1} - s}{h} + f_{n+1} \cdot \frac{s - t_n}{h}\] <p>Integrate on \([t_n, t_{n+1}]\) (set \(\tau = s - t_n \in [0, h]\)):</p> \[\int_{t_n}^{t_{n+1}} P_1(s) ds = \int_0^h \left( f_n \frac{h - \tau}{h} + f_{n+1}\frac{\tau}{h} \right)d\tau = h \cdot \frac{f_n + f_{n+1}}{2}\] <p>This yields</p> \[y_{n+1} - y_n = h \left( \frac{1}{2} f_n + \frac{1}{2} f_{n+1} \right)\] <p>It is straightforward to check consistency conditions are satisfied and the \(\ell = 2\) condition holds; the LTE leading term is \(-\frac{1}{12}h^3 y^{(3)}(t_n)\). Hence, the AM2 method is of order \(\mathcal{O}(h^2)\).</p> <p>AM3 (2-step, order 3)</p> <p>Interpolate \(f\) through \(t_{n+1}, t_n, t_{n-1}\):</p> \[P_2(s) = f_{n-1} \frac{(s - t_n)(s - t_{n+1})}{(t_{n-1} - t_n)(t_{n-1} - t_{n+1})} + f_n \frac{(s - t_{n-1})(s - t_{n+1})}{(t_n - t_{n-1})(t_n - t_{n+1})} + f_{n+1} \frac{(s - t_{n-1})(s - t_n)}{(t_{n+1} - t_{n-1})(t_{n+1} - t_n)}\] <p>Integrate on \([t_n, t_{n+1}]\) (set \(\tau = s - t_n \in [0, h]\)):</p> \[\begin{align*} \int_{t_n}^{t_{n+1}} P_2(s) ds &amp;= \int_0^h \left( f_n \frac{\tau(\tau - h)}{2h^2} - f_n \frac{(\tau + h)(\tau - h)}{h^2} + f_{n+1}\frac{(\tau + h)\tau}{2h^2} \right) d\tau \\ &amp;= \frac{h}{12} \left( -f_{n-1} + 8f_n + 5f_{n+1} \right) \end{align*}\] <p>Shift index</p> \[y_{n+2} = y_{n+1} + \frac{h}{12} \left(5f_{n+2} + 8f_{n+1} - f_n \right)\] <p>It is straightforward to check consistency conditions are satisfied and the \(\ell = 2, 3\) conditions hold; the LTE leading term is \(-\frac{1}{24}h^4 y^{(4)}(t_n)\). Hence, the AM3 method (2-step) is of order \(\mathcal{O}(h^3)\).</p> <ol> <li>Backward Differentiation Formulas</li> </ol> <p>This type of linear multistep method uses a polynomial \(P_k(t)\) of degree \(k\) passing through past values \(y_{n+1}, y_n, \ldots, y_{n-k+1}\) to approximate the solution \(y(t)\) instead of its derivative \(y'(t)\) as in the Adams methods. Then differentiate \(P_k(t)\) and enforce \(P_k'(t_{n+1}) = f(t_{n+1}, y_{n+1})\).</p> <p>BDF2</p> <p>The degree 2 Lagrange interpolating polynomial through \((t_{n-1}, y_{n-1}), (t_n, y_n), (t_{n+1}, y_{n+1})\) is</p> \[P_2(t) = y_{n-1} \frac{(t - t_n)(t - t_{n+1})}{(t_{n-1} - t_n)(t_{n-1} - t_{n+1})} + y_n \frac{(t - t_{n-1})(t - t_{n+1})}{(t_n - t_{n-1})(t_n - t_{n+1})} + y_{n+1} \frac{(t - t_{n-1})(t - t_n)}{(t_{n+1}- t_{n-1})(t_{n+1}- t_n)}\] <p>Let \(\theta = \frac{t - t_{n-1}}{h}\).</p> \[P_2(t) = y_{n-1} \frac{(\theta - 1)(\theta - 2)}{2} + y_n \theta(2 - \theta) + y_{n+1} \frac{\theta (\theta - 1)}{2}\] <p>Differentiate (note \(\frac{d\theta}{dt} = \frac{1}{h}\)) and evaluate at \(t_{n+1}\) (i.e., \(\theta = 2\)):</p> \[\begin{align*} P'_2(t_{n+1}) &amp;= \frac{1}{h} \left( y_{n-1} \cdot \frac{(\theta - 1) + (\theta - 2)}{2} + y_n \cdot ((2 - \theta) - \theta) + y_{n+1} \frac{\theta + (\theta - 1)}{2} \right)|_{\theta=2} \\ &amp;= \frac{1}{h} \left( \frac{1}{2}y_{n-1} - 2y_n + \frac{3}{2} y_{n+1} \right) \end{align*}\] <p>Enforce \(P_k'(t_{n+1}) = f(t_{n+1}, y_{n+1})\) and shift index to obtain</p> \[\frac{3}{2} y_{n+2} - 2y_{n+1} + \frac{1}{2}y_n = h f_{n+2}\] <p>It is straightforward to check consistency conditions are satisfied and the \(\ell = 2\) condition holds; the LTE leading term is \(-\frac{1}{3}h^3 y^{(3)}(t_n)\). Hence, the BDF2 method is of order \(\mathcal{O}(h^2)\).</p> <h2 id="concluding-remarks">Concluding Remarks</h2> <p>Voil`{a}! This concludes an article devoted to presenting a detailed mathematical proof of the Dahlquist Equivalence Theorem for Linear Multistep Methods (LMMs), which states that Convergence \(\iff\) Consistency + Zero-Stability. The key takeaway is that the global error of LMMs can be decomposed into two distinct components:</p> \[\max_{0\le n\le N} |e_n| \le C_1 \cdot \underbrace{\max_{0 \le j \le k-1} | e_j |}_{\text{initial error}} + C_2 \cdot \underbrace{\max_{ k \le m \le N} |\tau_m |}_{\substack{\text{the sum of LTE} \\ \text{controlled by} \\ \text{zero-stability}}}.\] <p>Moreover, the mathematical proof fundamentally relies on tools from matrix analysis, particularly the equivalence between the root condition and the boundedness of matrix powers.</p> <p>Should this article serve as a source of motivation, I highly recommend Matrix Analysis and Applied Linear Algebra <d-cite key="meyer2023matrix"></d-cite> by Carl D. Meyer, a book I repeatedly consulted while writing this post, which beautifully illustrates both the utility and elegance of the subject. One of Meyer’s guiding principles in composing the book is to reveal portions of the scaffolding, narratives, examples and summaries to enhance readability. This stands in contrast to the somewhat condescending view attributed to Carl Friedrich Gauss, who once remarked that ``architects of great cathedrals do not obscure the beauty of their work by leaving the scaffolding in place after the construction has been completed.” In this sense, mathematically proficient large language models (LLMs) offer an unprecedented opportunity for learners like myself—not only to admire the beauty of the “cathedral”, but also to explore the underlying scaffolding of its construction.</p> <h2 id="acknowledgement">Acknowledgement</h2> <p>This blog was developed from a self-contained, step-by-step tutorial generated by OpenAI ChatGPT, which also assisted in refining the phrasing and expressions, as well as in suggesting suitable titles.</p> <div class="language-html highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@misc{Cai2025odelearningnotes-3,
    author = {Xin Cai},
    title = {Reconstructing the Scaffolding A Learner’s Notes on Ordinary Differential Equations: Part 3 Understanding the Dahlquist Equivalence Theorem for Linear Multistep Methods},
    howpublished = {\url{https://totalvariation.github.io/blog/2025/concise-intro-odes-part3/}},
    note = {Accessed: 2025-10-28},
    year = {2025}
}
</code></pre></div></div>]]></content><author><name>Xin Cai</name></author><summary type="html"><![CDATA[Part &#8546 Understanding the Dahlquist Equivalence Theorem for Linear Multistep Methods]]></summary></entry><entry><title type="html">Learning the Backward Pass of FlashAttention</title><link href="https://totalvariation.github.io/blog/2025/intro-flashattention-backward-part1/" rel="alternate" type="text/html" title="Learning the Backward Pass of FlashAttention"/><published>2025-07-21T00:00:00+00:00</published><updated>2025-07-21T00:00:00+00:00</updated><id>https://totalvariation.github.io/blog/2025/intro-flashattention-backward-part1</id><content type="html" xml:base="https://totalvariation.github.io/blog/2025/intro-flashattention-backward-part1/"><![CDATA[<div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/essence-of-diff-blog-flashatten-480.webp 480w,/assets/img/essence-of-diff-blog-flashatten-800.webp 800w,/assets/img/essence-of-diff-blog-flashatten-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/essence-of-diff-blog-flashatten.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Image Credit: <a href="https://arxiv.org/abs/2501.14787">Matrix Calculus (for Machine Learning and Beyond)</a>. </div> <p>Scaling Transformers to longer sequence lengths has long been hindered by the computational bottleneck of self-attention, whose runtime and memory complexity scale quadratically with sequence length. FlashAttention and its subsequent versions <d-cite key="dao2022flashattention"></d-cite> <d-cite key="dao2023flashattention"></d-cite> have achieved dramatic memory savings and wall-clock speedups through a suite of carefully engineered techniques that minimize memory reads and writes between GPU high-bandwidth memory (HBM) and on-chip SRAM—with no approximation.</p> <p>While there are many excellent tutorials available online that provide an in-depth introduction to FlashAttention, such as the <a href="https://www.youtube.com/watch?v=zy8ChVd_oTM&amp;t=22388s">YouTube video by Umar Jamil</a>, the backward pass remains relatively underexplored. For someone like me, who has grown accustomed to relying on automatic differentiation (AD) engines embedded in modern scientific computing frameworks like PyTorch, JAX, and Julia to handle derivatives, understanding the backward pass of FlashAttention can be a bit daunting.</p> <p>In this blog, I aim to give a detailed explanation about the backward pass of FlashAttention, breaking it down into two parts. In the first part, I will derive the relevant gradients using matrix calculus and validate the results using two alternative methods to highlight the elegance of matrix calculus. In the second part, we will walk through the Triton implementation of the backward pass to strengthen our understanding.</p> <h2 id="revisiting-derivatives">Revisiting Derivatives</h2> <p>Recall from single variable calculus, a real-valued function \(f\) defined in a neighbourhood of \(a \in \mathbb{R}\) is said to be differentiable at \(a\) if the limit</p> \[f^{\prime}(a) = \lim_{h \to 0} \frac{f(a+h) - f(a)}{h}\] <p>exists in \(\mathbb{R}\). However, such an expression does not easily lend itself to a more generalised definition of derivatives beyond scalar inputs, such as vectors, matrices, or functions. A more useful and fundamental way to view derivatives is the linear approximation of functions near a small neighbourhood of input values: \(\delta f = f(x + \delta x) - f(x) = f^{\prime}(x)\delta x + o(\delta x)\), or the differential form: \(df = f(x + dx) - f(x) = f^{\prime}(x)dx\). For example, let \(f : U \to \mathbb{R}\), where \(U \subseteq \mathbb{R}^n\) is open, i.e., a scalar-valued function which takes in a (column) vector \(x \in \mathbb{R}^n\) and produces a scalar in \(\mathbb{R}\). From college calculus, we know that \(df = \underbrace{\nabla f(x)^T}_{f^\prime(x)} dx\).</p> <p>More generally, by the Riesz Representation Theorem <d-footnote>I recommend checking out this self-contained article <a href="https://math.uchicago.edu/~may/REU2021/REUPapers/Adler.pdf">HILBERT SPACES AND THE RIESZ REPRESENTATION THEOREM</a> by Ben Adler, which gives a concise introduction to Hilbert spaces and the Riesz Representation theorem.</d-footnote>, for any continuous linear functional \(\phi\) on a Hilbert space <d-footnote>If a normed vector space $ V $ is a complete metric space and the norm itself is induced by an inner product on $ V $, we say $ V $ is a Hilbert space.</d-footnote> \(V\), there exists a unique \(u \in V\) such that \(\phi(v) = \langle u, v \rangle\) for all \(v \in V\), where \(\langle \cdot \rangle\) denotes inner product. Therefore, the derivative can be generalised to any Hilbert space as \(df = \langle \underbrace{\text{some vector}}_{\nabla f(x)},\: dx \rangle\).</p> <p>For the vector space consisting of matrices \(A \in \mathbb{R}^{m \times n}\), the default inner product is defined as \(\operatorname{Tr}(A^T B) = \operatorname{vec}(A)^T \operatorname{vec}(B) = \sum_{i, j}A_{ij}B_{ij}\), which is referred to as the Frobenius inner product, in order to make it a valid Hilbert space. Therefore, for a scalar-valued function that takes in matrices \(f(A)\), its derivative can be expressed as \(df = \operatorname{Tr}(f^\prime(A)^T dA)\).</p> <p>Next, we will leverage this trick from matrix calculus to derive the formulas for the backpropagation of standard attention.</p> <h2 id="deriving-gradients-of-standard-attention">Deriving Gradients of Standard Attention</h2> <h3 id="by-matrix-calculus">By Matrix Calculus</h3> <p>Given input sequences \(Q,\: K,\: V,\: \in \mathbb{R}^{N\times d}\) where \(N\) is the sequence length and \(d\) is the head dimension, the standard attention output \(O \in \mathbb{R}^{N\times d}\) is calculated as follows (forward pass):</p> \[S=QK^T \in \mathbb{R}^{N\times N}\quad P = \operatorname{softmax}(S) \quad O=PV \in \mathbb{R}^{N\times d}\] <p>where \(\operatorname{softmax}\) is applied row-wise.</p> <p>Then, assuming a scalar-valued loss function \(L\), by the backpropagation (i.e., reverse mode of automatic differentiation (AD)), the gradients of \(L\) w.r.t various inputs are calculated as follows:</p> \[\frac{\partial L}{\partial V} = P^T \frac{\partial L}{\partial O} \in \mathbb{R}^{N\times d}\] \[\frac{\partial L}{\partial P} = \frac{\partial L}{\partial O} V^T \in \mathbb{R}^{N\times N}\] \[\frac{\partial L}{\partial S} = \operatorname{dsoftmax}(\frac{\partial L}{\partial P}) \in \mathbb{R}^{N\times N}\] \[\frac{\partial L}{\partial Q} = \frac{\partial L}{\partial S}K \in \mathbb{R}^{N\times d}\] \[\frac{\partial L}{\partial K} = \frac{\partial L}{\partial S}^T Q \in \mathbb{R}^{N\times d}\] <p>First, we calculate the gradient w.r.t. \(V\) (\(\frac{\partial L}{\partial V} = P^T \frac{\partial L}{\partial O} \in \mathbb{R}^{N\times d}\))</p> <p>Fix \(P\) and vary \(V\). From above, \(dO = P(V + dV) - PV = P dV\). The differential of \(dL\) becomes:</p> \[dL = \text{Tr} \left( \left( \frac{\partial L}{\partial O} \right)^T dO \right) = \text{Tr} \left( \left( \frac{\partial L}{\partial O} \right)^T (P dV) \right).\] <p>Substitute \(dO = P dV\):</p> \[dL = \text{Tr} \left( \left( \frac{\partial L}{\partial O} \right)^T P dV \right) = \text{Tr} \left( \underbrace{\left( P^T \frac{\partial L}{\partial O} \right)^T}_{\frac{\partial L}{\partial V}} dV \right)\] <p>Therefore, we get \(\frac{\partial L}{\partial V} = P^T \frac{\partial L}{\partial O} \in \mathbb{R}^{N\times d}\).</p> <p>Similarly, the gradient w.r.t \(P\) (\(\frac{\partial L}{\partial P} = \frac{\partial L}{\partial O} V^T \in \mathbb{R}^{N\times N}\)) is derived as, given \(dO = (P + dP)V - PV = dP V\):</p> \[dL = \text{Tr} \left( \left( \frac{\partial L}{\partial O} \right)^T dO \right) = \text{Tr} \left( \left( \frac{\partial L}{\partial O} \right)^T dP V \right) = \text{Tr} \left( V \left( \frac{\partial L}{\partial O} \right)^T dP \right) = \text{Tr} \left( \underbrace{\left( \frac{\partial L}{\partial O} V^T \right)^T}_{\frac{\partial L}{\partial P}} dP \right)\] <p>where the cyclic property of trace is applied \(\operatorname{Tr}(ABC) = \operatorname{Tr}(BCA)\). Therefore, we get \(\frac{\partial L}{\partial P} = \frac{\partial L}{\partial O} V^T \in \mathbb{R}^{N\times N}\).</p> <p>In \(P = \operatorname{softmax}(S)\), as \(\operatorname{softmax}\) is applied row-wise, it is more appropriate to consider the input as each row of \(S\), i.e., \(P_{i, :} = \operatorname{softmax}(S_{i, :})\) when deriving the gradient formula.</p> \[dL = \left( \frac{\partial L}{\partial P_{i, :}} \right)^T dP_{i, :} = \left( \frac{\partial L}{\partial P_{i, :}} \right)^T \operatorname{dsoftmax}(dS_{i, :})\] <p>The derivative \(df_x\) of a function \(f\) mapping vectors from \(\mathbb{R}^n \to \mathbb{R}^n\) is a linear transformation \(df_x \in \mathcal{L}(\mathbb{R}^n, \mathbb{R}^n)\), which can be expressed in its matrix form (Jacobian matrix). The Jocobian matrix (i.e., the derivative) of \(y = \operatorname{softmax}(x)\), where \(x \in \mathbb{R}^n\) is a column vector, is an \(n \times n\) symmetric matrix, \(\operatorname{dsoftmax}=\text{diag}(y) - yy^T=\operatorname{dsoftmax}^T\).</p> <p>With the above derivation, we can proceed as follows:</p> \[dL = \left( \frac{\partial L}{\partial P_{i, :}} \right)^T \operatorname{dsoftmax}(dS_{i, :}) = \left( \operatorname{dsoftmax}^T \frac{\partial L}{\partial P_{i, :}} \right)^T dS_{i, :} = \left( \operatorname{dsoftmax} \frac{\partial L}{\partial P_{i, :}} \right)^T dS_{i, :}\] <p>Therefore, we arrive at \(\frac{\partial L}{\partial S_{i, :}} = \operatorname{dsoftmax}(\frac{\partial L}{\partial P_{i, :}})\). With a slight abuse of notation, it can be compactly written as \(\frac{\partial L}{\partial S} = \operatorname{dsoftmax}(\frac{\partial L}{\partial P}) \in \mathbb{R}^{N \times N}\).</p> <h3 id="by-two-alternative-methods">By Two Alternative Methods</h3> <p>Next, we will resort to two alternative methods to verify the correctness of our previous derivations. Yet you will find them a bit more cumbersome.</p> <p><strong>Component-wise</strong></p> <p>Recall that in multivariable calculus, Let \(U \subseteq \mathbb{R}^n\) and \(V \subseteq \mathbb{R}^m\) be open and \(f:\: U \to \mathbb{R}^m,\: g:\: V \to \mathbb{R}^k\) with \(f(U) \subseteq V\). Let \(f\) be differentiable on \(U\) and \(g\) differentiable on \(V\). Set \(y = f(x)\) and \(z = (g \circ f)(x) = g(y)\). Then the chain rule \((g \circ f)^\prime(x) = g^\prime(y)f^\prime(x)\) can be written in its matrix form:</p> \[\begin{bmatrix}\frac{\partial z_1}{\partial x_1} &amp; \dots &amp;\frac{\partial z_1}{\partial x_n}\\ \vdots &amp; &amp; \vdots\\ \frac{\partial z_k}{\partial x_1} &amp; \dots &amp;\frac{\partial z_k}{\partial x_n} \end{bmatrix} = \begin{bmatrix}\frac{\partial z_1}{\partial y_1} &amp; \dots &amp;\frac{\partial z_1}{\partial y_m}\\ \vdots &amp; &amp; \vdots\\ \frac{\partial z_k}{\partial y_1} &amp; \dots &amp;\frac{\partial z_k}{\partial y_m} \end{bmatrix} \begin{bmatrix}\frac{\partial y_1}{\partial x_1} &amp; \dots &amp;\frac{\partial y_1}{\partial x_n}\\ \vdots &amp; &amp; \vdots\\ \frac{\partial y_m}{\partial x_1} &amp; \dots &amp;\frac{\partial y_m}{\partial x_n} \end{bmatrix}\] <p>In the special case where \(g:\: V \to \mathbb{R}\), the matrix form of the chain rule is expressed as follows:</p> \[\left[\frac{\partial L}{\partial x_1}, \dots, \frac{\partial L}{\partial x_n} \right] = \left[ \frac{\partial L}{\partial y_1}, \dots, \frac{\partial L}{\partial y_m} \right] \begin{bmatrix}\frac{\partial y_1}{\partial x_1} &amp; \dots &amp;\frac{\partial y_1}{\partial x_n}\\ \vdots &amp; &amp; \vdots\\ \frac{\partial y_m}{\partial x_1} &amp; \dots &amp;\frac{\partial y_m}{\partial x_n} \end{bmatrix}\] <p>If written in component-wise form, we arrive at the familiar formula:</p> \[\frac{\partial L}{\partial x_j} = \sum_{k=1}^m \frac{\partial L}{\partial y_k}\frac{\partial y_k}{\partial x_j}\] <p>To prove \(\frac{\partial L}{\partial V} = P^T \frac{\partial L}{\partial O} \in \mathbb{R}^{N\times d}\) using the (multivariable) chain rule, as it only works for vectors as inputs, we should consider each column \(V_{:, j}\) seperately, i.e., \(O_{:, j} = P V_{:, j}\). Yet the derived outcomes from such a workaround can be effortlessly transferred to the matrix \(V\), as each column \(V_{:, j}\) shares the same Jacobian matrix \(P\).</p> <p>As \(O_{kj} = \sum_{m=1}^N P_{km} V_{mj}\), we get \(\frac{\partial O_{kj}}{\partial V_{ij}} = P_{ki}\). Or, it can be read off from the Jacobian matrix \(P\) as the k-th row and i-th column element (\(\frac{\partial O[:, j]_{k}}{\partial V[:, j]_{i}}\)).</p> <p>Applying the chain rule, we get</p> \[\frac{\partial L}{\partial V_{ij}} = \sum_{k=1}^N \frac{\partial L}{\partial O_{kj}} \frac{\partial O_{kj}}{\partial V_{ij}} = \sum_{k=1}^N \frac{\partial L}{\partial O_{kj}} P_{ki} = (P^T \frac{\partial L}{\partial O})_{ij}\] <p>Similarly, we can prove \(\frac{\partial L}{\partial P} = \frac{\partial L}{\partial O} V^T \in \mathbb{R}^{N\times N}\) by treating each row \(P_{i,:}\) independently which shares the same Jacobian matrix \(V^T\) (\(O^T = V^T P^T\)): \(O_{ik} = \sum_{m=1}^N P_{im} V_{mk}\), so \(\frac{\partial O_{ik}}{\partial P_{ij}} = V_{jk}\). Or, it can be read off from the Jacobian matrix \(V^T\) as the k-th row and j-th column element \(V^T_{kj}\) (\(\frac{\partial O[i, :]_{k}}{\partial P[i, :]_{j}}\)).</p> \[\frac{\partial L}{\partial P_{ij}} = \sum_{k=1}^d \frac{\partial L}{\partial O_{ik}} \frac{\partial O_{ik}}{\partial P_{ij}} = \sum_{k=1}^d \frac{\partial L}{\partial O_{ik}} V_{jk} = \left( \frac{\partial L}{\partial O} V^T \right)_{ij}\] <p><strong>Matrix vectorisation and the Kronecker product</strong></p> <p>Actually, it is legitimate to directly work with the Jacobian matrix of matrix inputs/outputs, as any linear operator that transforms vectors between finite-dimensional vector spaces can be expressed in its matrix form once the bases for the input and output vector spaces have been selected. The most common way to achieve this involves matrix vectorisation and the Kronecker product.</p> <p>The vectorization \(\text{vec}(A) \in \mathbb{R}^{mn}\) of any \(m \times n\) matrix \(A \in \mathbb{R}^{m \times n}\) is defined by simply stacking the columns of \(A\), from left to right, into a column vector \(\text{vec}(A)\). That is, if we denote the \(n\) columns of \(A\) by m-component vectors \(\overrightarrow{a_1}, \overrightarrow{a_2}, \dots, \overrightarrow{a_n} \in \mathbb{R}^m\), then</p> \[\text{vec}(A) = \text{vec}(\left[ \overrightarrow{a_1}, \overrightarrow{a_2}, \dots, \overrightarrow{a_n} \right]) = \begin{pmatrix} \overrightarrow{a_1}\\ \overrightarrow{a_2}\\ \vdots \\ \overrightarrow{a_n} \end{pmatrix} \in \mathbb{R}^{mn}\] <p>Then, the Kronecker-product identity that plays a key role in the derivation is:</p> \[(A \otimes B) \text{vec}(C) = \text{vec}(BCA^T)\] <p>where \(A, B, C\) are compactly sized matrices. There are two special cases derived from the above Kronecker-product identity:</p> \[(I \otimes B) \text{vec}(C) = \text{vec}(BC)\] \[(A \otimes I) \text{vec}(C) = \text{vec}(CA^T )\] <details><summary>Click here to know more</summary> <p>Proof:</p> <p>To prove \((I \otimes B) \text{vec}(C) = \text{vec}(BC)\), assume \(B \in \mathbb{R}^{n \times m}\) and \(C \in \mathbb{R}^{m \times k}\),</p> \[BC = B \left[ \overrightarrow{c_1}, \overrightarrow{c_2}, \dots, \overrightarrow{c_k} \right] = \left[ B\overrightarrow{c_1}, B\overrightarrow{c_2}, \dots, B\overrightarrow{c_k} \right]\] \[\text{vec}(BC) = \begin{pmatrix} B\overrightarrow{c_1}\\ B\overrightarrow{c_2}\\ \vdots \\ B\overrightarrow{c_k} \end{pmatrix} = \underbrace{\begin{pmatrix} B &amp; &amp; &amp; \\ &amp; B &amp; &amp; \\ &amp; &amp; \ddots &amp; \\ &amp; &amp; &amp; B \end{pmatrix}}_{\left( I \otimes B \right)} \begin{pmatrix} \overrightarrow{c_1}\\ \overrightarrow{c_2}\\ \vdots \\ \overrightarrow{c_k} \end{pmatrix}\] <p>where \(I \in \mathbb{R}^{k \times k}\).</p> <p>To prove \((A \otimes I) \text{vec}(C) = \text{vec}(CA^T )\), assume \(A \in \mathbb{R}^{n \times k}\) and \(C \in \mathbb{R}^{m \times k}\),</p> \[CA^T = \left[ \sum_{j=1}^k a_{1j} \overrightarrow{c_j}, \sum_{j=1}^k a_{2j} \overrightarrow{c_j}, \dots, \sum_{j=1}^k a_{nj} \overrightarrow{c_j} \right]\] \[\text{vec}(CA^T) = \begin{pmatrix} \sum_{j=1}^k a_{1j} \overrightarrow{c_j}\\ \sum_{j=1}^k a_{2j} \overrightarrow{c_j}\\ \vdots \\ \sum_{j=1}^k a_{nj} \overrightarrow{c_j} \end{pmatrix} = \underbrace{\begin{pmatrix} a_{11}I &amp; a_{12}I &amp; \cdots &amp; a_{1k}I \\ a_{21}I &amp; a_{22}I &amp; \cdots &amp; a_{2k}I \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ a_{n1}I &amp; a_{n2}I &amp; \cdots &amp; a_{nk}I \end{pmatrix}}_{(A \otimes I)} \begin{pmatrix} \overrightarrow{c_1} \\ \overrightarrow{c_2}\\ \vdots \\ \overrightarrow{c_k} \end{pmatrix}\] <p>where \(I \in \mathbb{R}^{m \times m}\).</p> <p>To prove \((A \otimes B) \text{vec}(C) = \text{vec}(BCA^T)\),</p> \[\text{vec}(BCA^T) = (I \otimes B) \text{vec}(CA^T) = (I \otimes B)(A \otimes I) \text{vec}(C) = (A \otimes B) \text{vec}(C)\] <p>where we used the property of the Kronecker product \((A \otimes B)(C \otimes D) = (AC \otimes BD)\).</p> </details> <p>We are now equipped with all the necessary tools to provide a slightly more elegant way to prove gradients involving matrix inputs/outputs than chunking matrices into column vectors as done previously.</p> <p>To prove \(\frac{\partial L}{\partial V} = P^T \frac{\partial L}{\partial O} \in \mathbb{R}^{N\times d}\), as \(dO = P dV\), by vectorisation,</p> \[\text{vec}(dO) = \text{vec}(P dV) = \underbrace{(I \otimes P)}_{\text{Jacobian matrix :}\; \frac{\partial O}{\partial V}} \text{vec}(dV)\] <p>Recall the special case of multivariable chain rule \(z = (g \circ f)(x) = g(y)\) where \(g:\: V \to \mathbb{R}\),</p> \[\left[\frac{\partial L}{\partial x_1}, \dots, \frac{\partial L}{\partial x_n} \right] = \left[ \frac{\partial L}{\partial y_1}, \dots, \frac{\partial L}{\partial y_m} \right] \begin{bmatrix}\frac{\partial y_1}{\partial x_1} &amp; \dots &amp;\frac{\partial y_1}{\partial x_n}\\ \vdots &amp; &amp; \vdots\\ \frac{\partial y_m}{\partial x_1} &amp; \dots &amp;\frac{\partial y_m}{\partial x_n} \end{bmatrix}\] <p>We get</p> \[\text{vec}({\frac{\partial L}{\partial V}})^T = \text{vec}({\frac{\partial L}{\partial O}})^T (I \otimes P)\] \[\text{vec}({\frac{\partial L}{\partial V}}) = (I \otimes P)^T \text{vec}({\frac{\partial L}{\partial O}}) = (I \otimes P^T) \text{vec}({\frac{\partial L}{\partial O}}) = \text{vec}(P^T \frac{\partial L}{\partial O}) \quad \text{QED}\] <p>where we used one property of Kronecker-product \((A \otimes B)^T = A^T \otimes B^T\).</p> <p>Similarly, the proof of \(\frac{\partial L}{\partial P} = \frac{\partial L}{\partial O} V^T \in \mathbb{R}^{N\times N}\) is shown as follows:</p> \[dO = dP V\] \[\text{vec}(dO) = \text{vec}(dP V) = \underbrace{(V^T \otimes I)}_{\text{Jacobian matrix :}\; \frac{\partial O}{\partial P}} \text{vec}(dP)\] \[\text{vec}({\frac{\partial L}{\partial P}})^T = \text{vec}({\frac{\partial L}{\partial O}})^T (V^T \otimes I)\] \[\text{vec}({\frac{\partial L}{\partial P}}) = (V^T \otimes I)^T \text{vec}({\frac{\partial L}{\partial O}}) = (V \otimes I) \text{vec}({\frac{\partial L}{\partial O}}) = \text{vec}({\frac{\partial L}{\partial O}} V^T) \quad \text{QED}\] <p>It can be seen that using the trick of matrix vectorisation is more conceptually clear than dealing with gradients involving matrices in a component-wise manner. Yet it is still a bit cumbersome compared to techniques in matrix calculus.</p> <h2 id="concluding-remarks">Concluding Remarks</h2> <p>While matrix calculus plays a pivotal role in machine learning and large-scale optimisation, the advent of automatic differentiation (AD) engines in modern scientific computing libraries has largely relieved practitioners from the burden of manually computing derivatives of complex structures like matrices and higher-order tensors. Nevertheless, matrix calculus remains a powerful tool, well worthy to be included in your analytical arsenal.</p> <p>In <a href="https://totalvariation.github.io/blog/2025/intro-flashattention-backward-part2/">part 2</a> of this blog, we will walk through the implementation of the backward pass of FlashAttention in Triton.</p> <h2 id="acknowledgement">Acknowledgement</h2> <p>We express our gratitude to Matrix Calculus (for Machine Learning and Beyond) <d-cite key="bright2025matrix"></d-cite>, a set of lecture notes compiled from the MIT OpenCourse <a href="https://ocw.mit.edu/courses/18-s096-matrix-calculus-for-machine-learning-and-beyond-january-iap-2023/">Matrix Calculus for Machine Learning and Beyond</a>. We warmly recommend these materials to readers seeking for a deeper, principle-driven understanding of differentiation in the context of large-scale computing.</p> <div class="language-html highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@misc{Cai2025flashattnbackward-1,
    author = {Xin Cai},
    title = {Learning the Backward Pass of FlashAttention: Part I Derivations},
    howpublished = {\url{https://totalvariation.github.io/blog/2025/intro-flashattention-backward-part1/}},
    note = {Accessed: 2025-07-21},
    year = {2025}
}
</code></pre></div></div>]]></content><author><name>Xin Cai</name></author><summary type="html"><![CDATA[Part I Derivations]]></summary></entry><entry><title type="html">Learning the Backward Pass of FlashAttention</title><link href="https://totalvariation.github.io/blog/2025/intro-flashattention-backward-part2/" rel="alternate" type="text/html" title="Learning the Backward Pass of FlashAttention"/><published>2025-07-21T00:00:00+00:00</published><updated>2025-07-21T00:00:00+00:00</updated><id>https://totalvariation.github.io/blog/2025/intro-flashattention-backward-part2</id><content type="html" xml:base="https://totalvariation.github.io/blog/2025/intro-flashattention-backward-part2/"><![CDATA[<h2 id="recap-forward-and-backward-passes-of-standard-attention">Recap Forward and Backward Passes of Standard Attention</h2> <p>In the first part of this tutorial, we have walked through a detailed derivation of formulas used in the backward pass of standard attention. For ease of reference, they are included as follows:</p> <p>Given input sequences \(Q,\: K,\: V,\: \in \mathbb{R}^{N\times d}\) where \(N\) is the sequence length and \(d\) is the head dimension, the standard attention output \(O \in \mathbb{R}^{N\times d}\) is calculated as follows (forward pass):</p> \[S=QK^T \in \mathbb{R}^{N\times N}\quad P = \operatorname{softmax}(S) \quad O=PV \in \mathbb{R}^{N\times d}\] <p>where \(\operatorname{softmax}\) is applied row-wise.</p> <p>Then, assuming a scalar-valued loss function \(L\), by the backpropagation (i.e., reverse mode of automatic differentiation (AD)), the gradients of \(L\) w.r.t various inputs are calculated as follows:</p> \[\frac{\partial L}{\partial V} = P^T \frac{\partial L}{\partial O} \in \mathbb{R}^{N\times d}\] \[\frac{\partial L}{\partial P} = \frac{\partial L}{\partial O} V^T \in \mathbb{R}^{N\times N}\] \[\frac{\partial L}{\partial S} = \operatorname{dsoftmax}(\frac{\partial L}{\partial P}) \in \mathbb{R}^{N\times N}\] \[\frac{\partial L}{\partial Q} = \frac{\partial L}{\partial S}K \in \mathbb{R}^{N\times d}\] \[\frac{\partial L}{\partial K} = \frac{\partial L}{\partial S}^T Q \in \mathbb{R}^{N\times d}\] <h2 id="the-implementation-of-the-backward-pass-of-flashattention-in-triton">The Implementation of the Backward Pass of FlashAttention in Triton</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/flashattn-backward-pseudocode-480.webp 480w,/assets/img/flashattn-backward-pseudocode-800.webp 800w,/assets/img/flashattn-backward-pseudocode-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/flashattn-backward-pseudocode.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Image Credit: FlashAttention2 paper. </div> <p>To construct a direct correspondence between the mathematical equations and Triton code, we replace \(\frac{\partial L}{\partial V}\) with \(dV\) with a slight abuse of notation <d-footnote>Please note that $ dV $ hereafter will no longer denote differential.</d-footnote>, as in the backward pass, the matrix \(dV\) contains the gradient of scalar-valued loss function \(L\) w.r.t. \(V\), i.e., \(\frac{\partial L}{\partial V}\). By applying similar replacements to all the other variables, we therefore obtain the following equations adopted in the FlashAttention2 paper <d-cite key="dao2023flashattention"></d-cite>:</p> \[dV = P^T dO \in \mathbb{R}^{N\times d}\] \[dP = dOV^T \in \mathbb{R}^{N\times N}\] \[dS = \operatorname{dsoftmax}(dP) \in \mathbb{R}^{N\times N}\] \[dQ = dSK \in \mathbb{R}^{N\times d}\] \[dK = dS^T Q \in \mathbb{R}^{N\times d}\] <p>Another trick adopted in the FlashAttention paper <d-cite key="dao2022flashattention"></d-cite> is to simplify the calculation of \(dS = \operatorname{dsoftmax}(dP)\), which is clearly derived in its appendix.</p> <p>For self-containedness, it is included as follows, (Please note \(dS_{i,:}, dP_{i,:}\) are all column vectors):</p> \[dS_{i,:} = \operatorname{dsoftmax}dP_{i,:} = (\text{diag}(P_{i,:}) - P_{i,:}P_{i,:}^T)dP_{i,:} = P_{i,:} \circ dP_{i,:} - \left( P_{i,:}^T dP_{i,:} \right) P_{i,:}\] <p>where \(\circ\) denotes Hadamard product (i.e., pointwise multiplication).</p> <p>Recall that \(dP = dO V^T\), written in element-wise form, \(dP_{ij} = do_i^T v_j\), (Please note \(do_j, v_j, k_j\) here denote the j-th row of \(dO, V, K\) respectively, acting as a column vector.)</p> <p>Now, we can define</p> \[D_i = P_{i,:}^T dP_{i,:} = \sum_j \frac{\exp(q_i^T k_j)}{L_i} do_i^T v_j = do_i^T \sum_j \frac{\exp(q_i^T k_j)}{L_i} v_j = do_i^T o_i\] <p>then \(dS_{i,:} = P_{i,:} \circ dP_{i,:} - D_i P_{i,:}\).</p> <p>Readers seeking a comprehensive treatment (e.g., the online-softmax trick in the forward pass) of FlashAttention are encouraged to refer to the original papers <d-cite key="dao2022flashattention"></d-cite> <d-cite key="dao2023flashattention"></d-cite> or other tutorials available online focusing on the forward pass.</p> <p>Now, we are in a position to dive into the Triton implementation of the backward pass of FlashAttention2.</p> <p>We assume readers have a basic familiarity with Triton. Otherwise, there are many excellent Triton tutorials, including the <a href="https://triton-lang.org/main/getting-started/tutorials/index.html">official ones</a>, available online for your reference. In my view, figuring out how to move pointers to accurately access blocks of elements (i.e., load and store) in parallelly launched Triton programs is sufficient to grasp the core mechanisms of custom kernels developed in Triton.</p> <p>Instead of using <code class="language-plaintext highlighter-rouge">block pointer</code> defined by <code class="language-plaintext highlighter-rouge">make_block_ptr</code>, I find that directly working with N-dimensional pointers to access elements in memory is more straightforward. Furthermore, <code class="language-plaintext highlighter-rouge">mask</code> and <code class="language-plaintext highlighter-rouge">other</code> are implicitly broadcast to <code class="language-plaintext highlighter-rouge">pointer.shape</code> when using N-dimensional pointers, which can be conveniently used to handle boundary conditions.</p> <p>In the following, I will give some visual illustrations to facilitate your understanding of how <code class="language-plaintext highlighter-rouge">tl.load()</code> works, as there is no difference in read (<code class="language-plaintext highlighter-rouge">tl.load()</code>) and write (<code class="language-plaintext highlighter-rouge">tl.store()</code>) operations as long as their indexes are specified correctly.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

  <span class="n">N</span> <span class="o">=</span> <span class="mi">8</span>
  <span class="c1"># Here, the content of the array is made intentionally to be the exact same as offsets relative to the base pointer.
</span>  <span class="c1"># Please note that in Triton language, all Pytorch tensors are implicitly converted to base pointers.
</span>
  <span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">N</span> <span class="o">*</span> <span class="n">N</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span>
  <span class="nf">print</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
  
  <span class="p">[[</span> <span class="mi">0</span>  <span class="mi">1</span>  <span class="mi">2</span>  <span class="mi">3</span>  <span class="mi">4</span>  <span class="mi">5</span>  <span class="mi">6</span>  <span class="mi">7</span><span class="p">]</span>
   <span class="p">[</span> <span class="mi">8</span>  <span class="mi">9</span> <span class="mi">10</span> <span class="mi">11</span> <span class="mi">12</span> <span class="mi">13</span> <span class="mi">14</span> <span class="mi">15</span><span class="p">]</span>
   <span class="p">[</span><span class="mi">16</span> <span class="mi">17</span> <span class="mi">18</span> <span class="mi">19</span> <span class="mi">20</span> <span class="mi">21</span> <span class="mi">22</span> <span class="mi">23</span><span class="p">]</span>
   <span class="p">[</span><span class="mi">24</span> <span class="mi">25</span> <span class="mi">26</span> <span class="mi">27</span> <span class="mi">28</span> <span class="mi">29</span> <span class="mi">30</span> <span class="mi">31</span><span class="p">]</span>
   <span class="p">[</span><span class="mi">32</span> <span class="mi">33</span> <span class="mi">34</span> <span class="mi">35</span> <span class="mi">36</span> <span class="mi">37</span> <span class="mi">38</span> <span class="mi">39</span><span class="p">]</span>
   <span class="p">[</span><span class="mi">40</span> <span class="mi">41</span> <span class="mi">42</span> <span class="mi">43</span> <span class="mi">44</span> <span class="mi">45</span> <span class="mi">46</span> <span class="mi">47</span><span class="p">]</span>
   <span class="p">[</span><span class="mi">48</span> <span class="mi">49</span> <span class="mi">50</span> <span class="mi">51</span> <span class="mi">52</span> <span class="mi">53</span> <span class="mi">54</span> <span class="mi">55</span><span class="p">]</span>
   <span class="p">[</span><span class="mi">56</span> <span class="mi">57</span> <span class="mi">58</span> <span class="mi">59</span> <span class="mi">60</span> <span class="mi">61</span> <span class="mi">62</span> <span class="mi">63</span><span class="p">]]</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="n">BLOCK_M</span> <span class="o">=</span> <span class="mi">2</span>
  <span class="n">col_dim</span> <span class="o">=</span> <span class="n">N</span>

  <span class="n">stride_row</span> <span class="o">=</span> <span class="n">N</span>
  <span class="n">stride_col</span> <span class="o">=</span> <span class="mi">1</span>

  <span class="n">offs_m</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">BLOCK_M</span><span class="p">)[:,</span> <span class="bp">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">stride_row</span> <span class="o">+</span> <span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">col_dim</span><span class="p">)[</span><span class="bp">None</span><span class="p">,</span> <span class="p">:]</span> <span class="o">*</span> <span class="n">stride_col</span>

  <span class="c1"># N-dimensional tensors are stored contiguously in memory. 
</span>  <span class="c1"># Otherwise, it would be recommended to call x.contiguous() before taking any tensor operations. 
</span>  <span class="c1"># Here, we mimic this feature with np.ndarray.flatten.
</span>
  <span class="c1"># illustrate loading tensors from memory
</span>  <span class="nf">print</span><span class="p">(</span><span class="n">A</span><span class="p">.</span><span class="nf">flatten</span><span class="p">()[</span><span class="n">offs_m</span><span class="p">])</span>

  <span class="p">[[</span> <span class="mi">0</span>  <span class="mi">1</span>  <span class="mi">2</span>  <span class="mi">3</span>  <span class="mi">4</span>  <span class="mi">5</span>  <span class="mi">6</span>  <span class="mi">7</span><span class="p">]</span>
   <span class="p">[</span> <span class="mi">8</span>  <span class="mi">9</span> <span class="mi">10</span> <span class="mi">11</span> <span class="mi">12</span> <span class="mi">13</span> <span class="mi">14</span> <span class="mi">15</span><span class="p">]]</span>

</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="c1"># illustrate moving blocks `step_size` rows down, which will be used in the for loop to 
</span>  <span class="c1"># traverse over one dimension of a tensor.
</span>  <span class="n">step_size</span> <span class="o">=</span> <span class="mi">2</span>
  <span class="nf">print</span><span class="p">(</span><span class="n">A</span><span class="p">.</span><span class="nf">flatten</span><span class="p">()[</span><span class="n">offs_m</span> <span class="o">+</span> <span class="n">step_size</span> <span class="o">*</span> <span class="n">N</span><span class="p">])</span>

  <span class="p">[[</span><span class="mi">16</span> <span class="mi">17</span> <span class="mi">18</span> <span class="mi">19</span> <span class="mi">20</span> <span class="mi">21</span> <span class="mi">22</span> <span class="mi">23</span><span class="p">]</span>
   <span class="p">[</span><span class="mi">24</span> <span class="mi">25</span> <span class="mi">26</span> <span class="mi">27</span> <span class="mi">28</span> <span class="mi">29</span> <span class="mi">30</span> <span class="mi">31</span><span class="p">]]</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="c1"># illustrate loading tensors directly in its transposed version and moving blocks accordingly
</span>  <span class="n">offs_m_T</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">BLOCK_M</span><span class="p">)[</span><span class="bp">None</span><span class="p">,</span> <span class="p">:]</span> <span class="o">*</span> <span class="n">stride_row</span> <span class="o">+</span> <span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">col_dim</span><span class="p">)[:,</span> <span class="bp">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">stride_col</span>
  <span class="nf">print</span><span class="p">(</span><span class="n">A</span><span class="p">.</span><span class="nf">flatten</span><span class="p">()[</span><span class="n">offs_m_T</span><span class="p">])</span>
  <span class="nf">print</span><span class="p">(</span><span class="n">A</span><span class="p">.</span><span class="nf">flatten</span><span class="p">()[</span><span class="n">offs_m_T</span> <span class="o">+</span> <span class="n">step_size</span> <span class="o">*</span> <span class="n">N</span><span class="p">])</span>

  <span class="p">[[</span> <span class="mi">0</span>  <span class="mi">8</span><span class="p">]</span>
   <span class="p">[</span> <span class="mi">1</span>  <span class="mi">9</span><span class="p">]</span>
   <span class="p">[</span> <span class="mi">2</span> <span class="mi">10</span><span class="p">]</span>
   <span class="p">[</span> <span class="mi">3</span> <span class="mi">11</span><span class="p">]</span>
   <span class="p">[</span> <span class="mi">4</span> <span class="mi">12</span><span class="p">]</span>
   <span class="p">[</span> <span class="mi">5</span> <span class="mi">13</span><span class="p">]</span>
   <span class="p">[</span> <span class="mi">6</span> <span class="mi">14</span><span class="p">]</span>
   <span class="p">[</span> <span class="mi">7</span> <span class="mi">15</span><span class="p">]]</span>

  <span class="p">[[</span><span class="mi">16</span> <span class="mi">24</span><span class="p">]</span>
   <span class="p">[</span><span class="mi">17</span> <span class="mi">25</span><span class="p">]</span>
   <span class="p">[</span><span class="mi">18</span> <span class="mi">26</span><span class="p">]</span>
   <span class="p">[</span><span class="mi">19</span> <span class="mi">27</span><span class="p">]</span>
   <span class="p">[</span><span class="mi">20</span> <span class="mi">28</span><span class="p">]</span>
   <span class="p">[</span><span class="mi">21</span> <span class="mi">29</span><span class="p">]</span>
   <span class="p">[</span><span class="mi">22</span> <span class="mi">30</span><span class="p">]</span>
   <span class="p">[</span><span class="mi">23</span> <span class="mi">31</span><span class="p">]]</span>
</code></pre></div></div> <p>Here, we analyse a simplified version of FlashAttention (technically, FlashAttention2) adapted from the official Triton tutorial <a href="https://triton-lang.org/main/getting-started/tutorials/06-fused-attention.html#fused-attention">Fused Attention</a>, accounting for both the ‘Causal’ and ‘Non-Causal’ modes.</p> <p>The implementation of the backward pass of FlashAttention can be generally grouped into three stages:</p> <ol> <li> <p>Calculate the matrix \(D\) first as a preprocessing step, where \(D_i = do_i^T o_i\), which corresponds to the variable <code class="language-plaintext highlighter-rouge">delta = torch.empty_like(M)</code>. Its size is <code class="language-plaintext highlighter-rouge">(Batch, Num_Heads, N_CTX)</code>, and is realised in the function <code class="language-plaintext highlighter-rouge">_attn_bwd_preprocess()</code>.</p> </li> <li> <p>Calculate \(dV, dK\) via the function <code class="language-plaintext highlighter-rouge">_attn_bwd_dkdv()</code>.</p> </li> <li> <p>Calculate \(dQ\) via the function <code class="language-plaintext highlighter-rouge">_attn_bwd_dq()</code>.</p> </li> </ol> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="nd">@triton.jit</span>
  <span class="k">def</span> <span class="nf">_attn_bwd_preprocess</span><span class="p">(</span><span class="n">O</span><span class="p">,</span> <span class="n">DO</span><span class="p">,</span>  <span class="c1">#
</span>                           <span class="n">Delta</span><span class="p">,</span>  <span class="c1">#
</span>                           <span class="n">Z</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">N_CTX</span><span class="p">,</span>  <span class="c1">#
</span>                           <span class="n">BLOCK_M</span><span class="p">:</span> <span class="n">tl</span><span class="p">.</span><span class="n">constexpr</span><span class="p">,</span> <span class="n">HEAD_DIM</span><span class="p">:</span> <span class="n">tl</span><span class="p">.</span><span class="n">constexpr</span>  <span class="c1">#
</span>                           <span class="p">):</span>
      <span class="n">off_m</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">program_id</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="o">*</span> <span class="n">BLOCK_M</span> <span class="o">+</span> <span class="n">tl</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">BLOCK_M</span><span class="p">)</span>
      <span class="n">off_hz</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">program_id</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
      <span class="n">off_n</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">HEAD_DIM</span><span class="p">)</span>
      <span class="c1"># load
</span>      <span class="n">o</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="n">O</span> <span class="o">+</span> <span class="n">off_hz</span> <span class="o">*</span> <span class="n">HEAD_DIM</span> <span class="o">*</span> <span class="n">N_CTX</span> <span class="o">+</span> <span class="n">off_m</span><span class="p">[:,</span> <span class="bp">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">HEAD_DIM</span> <span class="o">+</span> <span class="n">off_n</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="p">:]).</span><span class="nf">to</span><span class="p">(</span><span class="n">tl</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
      <span class="n">do</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="n">DO</span> <span class="o">+</span> <span class="n">off_hz</span> <span class="o">*</span> <span class="n">HEAD_DIM</span> <span class="o">*</span> <span class="n">N_CTX</span> <span class="o">+</span> <span class="n">off_m</span><span class="p">[:,</span> <span class="bp">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">HEAD_DIM</span> <span class="o">+</span> <span class="n">off_n</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="p">:]).</span><span class="nf">to</span><span class="p">(</span><span class="n">tl</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
      <span class="n">delta</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">o</span> <span class="o">*</span> <span class="n">do</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  
      <span class="n">tl</span><span class="p">.</span><span class="nf">store</span><span class="p">(</span><span class="n">Delta</span> <span class="o">+</span> <span class="n">off_hz</span> <span class="o">*</span> <span class="n">N_CTX</span> <span class="o">+</span> <span class="n">off_m</span><span class="p">,</span> <span class="n">delta</span><span class="p">)</span>

</code></pre></div></div> <p>where <code class="language-plaintext highlighter-rouge">delta = tl.sum(o * do, axis=1)</code> implements the equation \(D_i = do_i^T o_i\).</p> <p>To calculate \(dV, dK\), a block of elements of <code class="language-plaintext highlighter-rouge">k, v</code> is first loaded (sequence parallelisation), and then carries out a loop over the length dimension of <code class="language-plaintext highlighter-rouge">q</code>.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="n">start_n</span> <span class="o">=</span> <span class="n">pid</span> <span class="o">*</span> <span class="n">BLOCK_N1</span>
  <span class="n">offs_n</span> <span class="o">=</span> <span class="n">start_n</span> <span class="o">+</span> <span class="n">tl</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">BLOCK_N1</span><span class="p">)</span>
  <span class="c1"># load K and V: they stay in SRAM throughout the inner loop.
</span>  <span class="n">k</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="n">K</span> <span class="o">+</span> <span class="n">offs_n</span><span class="p">[:,</span> <span class="bp">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">stride_tok</span> <span class="o">+</span> <span class="n">offs_k</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="p">:]</span> <span class="o">*</span> <span class="n">stride_d</span><span class="p">)</span>
  <span class="n">v</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="n">V</span> <span class="o">+</span> <span class="n">offs_n</span><span class="p">[:,</span> <span class="bp">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">stride_tok</span> <span class="o">+</span> <span class="n">offs_k</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="p">:]</span> <span class="o">*</span> <span class="n">stride_d</span><span class="p">)</span>
</code></pre></div></div> <p>For the non-causal case, it is straightforward,</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="n">start_m</span> <span class="o">=</span> <span class="mi">0</span>
  <span class="n">num_steps</span> <span class="o">=</span> <span class="p">(</span><span class="n">N_CTX</span> <span class="o">-</span> <span class="n">start_m</span><span class="p">)</span> <span class="o">//</span> <span class="n">BLOCK_M1</span>
</code></pre></div></div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0" id="figure-1"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/kq_dotprod_mat-480.webp 480w,/assets/img/kq_dotprod_mat-800.webp 800w,/assets/img/kq_dotprod_mat-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/kq_dotprod_mat.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Fig-1 An illustration of $ S^T = KQ^T $. </div> <p>For the causal case (please note that causal modelling is only used in self-attention), the procedure is split into two steps:</p> <ol> <li>Calculate the non-masked blocks (yellow squares in the <a href="#figure-1">Fig-1</a>) by only changing <code>start_m = start_n + BLOCK_N1</code>.</li> <li> <p>Calculate the diagonal block (the green square in the <a href="#figure-1">Fig-1</a>) by setting</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="n">start_m</span> <span class="o">=</span> <span class="n">start_n</span>
  <span class="n">MASK_BLOCK_M1</span><span class="p">:</span> <span class="n">tl</span><span class="p">.</span><span class="n">constexpr</span> <span class="o">=</span> <span class="n">BLOCK_M1</span> <span class="o">//</span> <span class="n">BLK_SLICE_FACTOR</span>
  <span class="n">num_steps</span> <span class="o">=</span> <span class="n">BLOCK_N1</span> <span class="o">//</span> <span class="n">MASK_BLOCK_M1</span>
</code></pre></div> </div> </li> </ol> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="c1"># The main inner-loop logic for computing dK and dV.
</span>  <span class="nd">@triton.jit</span>
  <span class="k">def</span> <span class="nf">_attn_bwd_dkdv</span><span class="p">(</span><span class="n">dk</span><span class="p">,</span> <span class="n">dv</span><span class="p">,</span>  <span class="c1">#
</span>                     <span class="n">Q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">sm_scale</span><span class="p">,</span>  <span class="c1">#
</span>                     <span class="n">DO</span><span class="p">,</span>  <span class="c1">#
</span>                     <span class="n">M</span><span class="p">,</span> <span class="n">D</span><span class="p">,</span>  <span class="c1">#
</span>                     <span class="c1"># shared by Q/K/V/DO.
</span>                     <span class="n">stride_tok</span><span class="p">,</span> <span class="n">stride_d</span><span class="p">,</span>  <span class="c1">#
</span>                     <span class="n">H</span><span class="p">,</span> <span class="n">N_CTX</span><span class="p">,</span> <span class="n">BLOCK_M1</span><span class="p">:</span> <span class="n">tl</span><span class="p">.</span><span class="n">constexpr</span><span class="p">,</span>  <span class="c1">#
</span>                     <span class="n">BLOCK_N1</span><span class="p">:</span> <span class="n">tl</span><span class="p">.</span><span class="n">constexpr</span><span class="p">,</span>  <span class="c1">#
</span>                     <span class="n">HEAD_DIM</span><span class="p">:</span> <span class="n">tl</span><span class="p">.</span><span class="n">constexpr</span><span class="p">,</span>  <span class="c1">#
</span>                     <span class="c1"># Filled in by the wrapper.
</span>                     <span class="n">start_n</span><span class="p">,</span> <span class="n">start_m</span><span class="p">,</span> <span class="n">num_steps</span><span class="p">,</span>  <span class="c1">#
</span>                     <span class="n">MASK</span><span class="p">:</span> <span class="n">tl</span><span class="p">.</span><span class="n">constexpr</span><span class="p">):</span>
      <span class="n">offs_m</span> <span class="o">=</span> <span class="n">start_m</span> <span class="o">+</span> <span class="n">tl</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">BLOCK_M1</span><span class="p">)</span>
      <span class="n">offs_n</span> <span class="o">=</span> <span class="n">start_n</span> <span class="o">+</span> <span class="n">tl</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">BLOCK_N1</span><span class="p">)</span>
      <span class="n">offs_k</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">HEAD_DIM</span><span class="p">)</span>
      <span class="n">qT_ptrs</span> <span class="o">=</span> <span class="n">Q</span> <span class="o">+</span> <span class="n">offs_m</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="p">:]</span> <span class="o">*</span> <span class="n">stride_tok</span> <span class="o">+</span> <span class="n">offs_k</span><span class="p">[:,</span> <span class="bp">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">stride_d</span>
      <span class="n">do_ptrs</span> <span class="o">=</span> <span class="n">DO</span> <span class="o">+</span> <span class="n">offs_m</span><span class="p">[:,</span> <span class="bp">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">stride_tok</span> <span class="o">+</span> <span class="n">offs_k</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="p">:]</span> <span class="o">*</span> <span class="n">stride_d</span>
      <span class="c1"># BLOCK_N1 must be a multiple of BLOCK_M1, otherwise the code wouldn't work.
</span>      <span class="n">tl</span><span class="p">.</span><span class="nf">static_assert</span><span class="p">(</span><span class="n">BLOCK_N1</span> <span class="o">%</span> <span class="n">BLOCK_M1</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span>
      <span class="n">curr_m</span> <span class="o">=</span> <span class="n">start_m</span>
      <span class="n">step_m</span> <span class="o">=</span> <span class="n">BLOCK_M1</span>
      <span class="k">for</span> <span class="n">blk_idx</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_steps</span><span class="p">):</span>
          <span class="n">qT</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="n">qT_ptrs</span><span class="p">)</span>
          <span class="c1"># Load m before computing qk to reduce pipeline stall.
</span>          <span class="n">offs_m</span> <span class="o">=</span> <span class="n">curr_m</span> <span class="o">+</span> <span class="n">tl</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">BLOCK_M1</span><span class="p">)</span>
          <span class="n">m</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="n">M</span> <span class="o">+</span> <span class="n">offs_m</span><span class="p">)</span>
          <span class="n">sT</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">qT</span><span class="p">)</span>
          <span class="n">pT</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="n">math</span><span class="p">.</span><span class="nf">exp2</span><span class="p">(</span><span class="n">sT</span> <span class="o">-</span> <span class="n">m</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="p">:])</span>
          <span class="c1"># Autoregressive masking.
</span>          <span class="k">if</span> <span class="n">MASK</span><span class="p">:</span>
              <span class="n">mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">offs_m</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="p">:]</span> <span class="o">&gt;=</span> <span class="n">offs_n</span><span class="p">[:,</span> <span class="bp">None</span><span class="p">])</span>
              <span class="n">pT</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">where</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="n">pT</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span>
          <span class="n">do</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="n">do_ptrs</span><span class="p">)</span>
          <span class="c1"># Compute dV.
</span>          <span class="n">ppT</span> <span class="o">=</span> <span class="n">pT</span>
          <span class="n">ppT</span> <span class="o">=</span> <span class="n">ppT</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">tl</span><span class="p">.</span><span class="n">float16</span><span class="p">)</span>
          <span class="n">dv</span> <span class="o">+=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">ppT</span><span class="p">,</span> <span class="n">do</span><span class="p">)</span>
          <span class="c1"># D (= delta) is pre-divided by ds_scale.
</span>          <span class="n">Di</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="n">D</span> <span class="o">+</span> <span class="n">offs_m</span><span class="p">)</span>
          <span class="c1"># Compute dP and dS.
</span>          <span class="n">dpT</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">tl</span><span class="p">.</span><span class="nf">trans</span><span class="p">(</span><span class="n">do</span><span class="p">)).</span><span class="nf">to</span><span class="p">(</span><span class="n">tl</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
          <span class="n">dsT</span> <span class="o">=</span> <span class="n">pT</span> <span class="o">*</span> <span class="p">(</span><span class="n">dpT</span> <span class="o">-</span> <span class="n">Di</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="p">:])</span>
          <span class="n">dsT</span> <span class="o">=</span> <span class="n">dsT</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">tl</span><span class="p">.</span><span class="n">float16</span><span class="p">)</span>
          <span class="n">dk</span> <span class="o">+=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">dsT</span><span class="p">,</span> <span class="n">tl</span><span class="p">.</span><span class="nf">trans</span><span class="p">(</span><span class="n">qT</span><span class="p">))</span>
          <span class="c1"># Increment pointers.
</span>          <span class="n">curr_m</span> <span class="o">+=</span> <span class="n">step_m</span>
          <span class="n">qT_ptrs</span> <span class="o">+=</span> <span class="n">step_m</span> <span class="o">*</span> <span class="n">stride_tok</span>
          <span class="n">do_ptrs</span> <span class="o">+=</span> <span class="n">step_m</span> <span class="o">*</span> <span class="n">stride_tok</span>
      <span class="k">return</span> <span class="n">dk</span><span class="p">,</span> <span class="n">dv</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="n">qT</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="n">qT_ptrs</span><span class="p">)</span>
  <span class="c1"># Load m before computing qk to reduce pipeline stall.
</span>  <span class="n">offs_m</span> <span class="o">=</span> <span class="n">curr_m</span> <span class="o">+</span> <span class="n">tl</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">BLOCK_M1</span><span class="p">)</span>
  <span class="n">m</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="n">M</span> <span class="o">+</span> <span class="n">offs_m</span><span class="p">)</span>
  <span class="n">sT</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">qT</span><span class="p">)</span>
  <span class="n">pT</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="n">math</span><span class="p">.</span><span class="nf">exp2</span><span class="p">(</span><span class="n">sT</span> <span class="o">-</span> <span class="n">m</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="p">:])</span>
</code></pre></div></div> <p>This part of code recomputes \(S = QK^T\) and \(P = \operatorname{softmax}(S)\) (actually its transposed version, and therefore it needs to pay attention to the broadcast rule <code class="language-plaintext highlighter-rouge">m[None, :]</code>. <code class="language-plaintext highlighter-rouge">m</code> is stored in the forward pass for calculating softmax in a numerical stable manner.).</p> <p><code class="language-plaintext highlighter-rouge">dv += tl.dot(ppT, do)</code> implements the equation \(dV = P^T dO\). As the calculation \(dv_j = \sum_i P_{ij} do_i\), where \(dv_j, do_i\) denote the j-th and i-th row of \(V, O\) respectively, is chunked into multiple blocks, so do not forget the accumulation sum.</p> <p><code class="language-plaintext highlighter-rouge">dpT = tl.dot(v, tl.trans(do)).to(tl.float32)</code> implements the equation \(dP = dO V^T\) (its transposed version).</p> <p><code class="language-plaintext highlighter-rouge">dsT = pT * (dpT - Di[None, :])</code> implements the equation \(dS = \operatorname{dsoftmax}(dP) \in \mathbb{R}^{N\times N}\), which is further simplified to</p> \[dS_{i,:} = \operatorname{dsoftmax}dP_{i,:} = (\text{diag}(P_{i,:}) - P_{i,:}P_{i,:}^T)dP_{i,:} = P_{i,:} \circ dP_{i,:} - \left( P_{i,:}^T dP_{i,:} \right) P_{i,:} = P_{i,:} \circ dP_{i,:} - D_i P_{i,:}\] <p>as discussed above (its transposed version).</p> <p><code class="language-plaintext highlighter-rouge">dk += tl.dot(dsT, tl.trans(qT))</code> implements the equation \(dK = dS^T Q\).</p> <p>\(dQ\) is calculated similarly: a block of elements of <code class="language-plaintext highlighter-rouge">q</code> is first loaded (sequence parallelisation), and then carries out a loop over the length dimension of <code class="language-plaintext highlighter-rouge">k, v</code>.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="n">start_m</span> <span class="o">=</span> <span class="n">pid</span> <span class="o">*</span> <span class="n">BLOCK_M2</span>
  <span class="n">offs_m</span> <span class="o">=</span> <span class="n">start_m</span> <span class="o">+</span> <span class="n">tl</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">BLOCK_M2</span><span class="p">)</span>
  <span class="c1"># load q, do, m and Di: they stay in SRAM throughout the inner loop.
</span>  <span class="n">q</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="n">Q</span> <span class="o">+</span> <span class="n">offs_m</span><span class="p">[:,</span> <span class="bp">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">stride_tok</span> <span class="o">+</span> <span class="n">offs_k</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="p">:]</span> <span class="o">*</span> <span class="n">stride_d</span><span class="p">)</span>
  <span class="n">do</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="n">DO</span> <span class="o">+</span> <span class="n">offs_m</span><span class="p">[:,</span> <span class="bp">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">stride_tok</span> <span class="o">+</span> <span class="n">offs_k</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="p">:]</span> <span class="o">*</span> <span class="n">stride_d</span><span class="p">)</span>

  <span class="n">m</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="n">M</span> <span class="o">+</span> <span class="n">offs_m</span><span class="p">)</span>
  <span class="n">m</span> <span class="o">=</span> <span class="n">m</span><span class="p">[:,</span> <span class="bp">None</span><span class="p">]</span>

  <span class="n">Di</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="n">D</span> <span class="o">+</span> <span class="n">offs_m</span><span class="p">)</span>
  <span class="n">Di</span> <span class="o">=</span> <span class="n">Di</span><span class="p">[:,</span> <span class="bp">None</span><span class="p">]</span>
</code></pre></div></div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0" id="figure-2"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/qk_dotprod_mat-480.webp 480w,/assets/img/qk_dotprod_mat-800.webp 800w,/assets/img/qk_dotprod_mat-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/qk_dotprod_mat.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Fig-2 An illustration of $ S = QK^T $. </div> <p>For the causal case, the procedure is split into two steps:</p> <ol> <li>Calculate the non-masked blocks (yellow squares in the <a href="#figure-2">Fig-2</a>) by setting <code>end_n = start_m</code> <code>num_steps = end_n // BLOCK_N2</code> . So in the inner loop over <code>k, v</code>, the start and end indexes are <code>0</code> and <code>start_m</code>, respectively.</li> <li> <p>Calculate the diagonal block (the green square in the <a href="#figure-2">Fig-2</a>) by setting</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="n">MASK_BLOCK_N2</span><span class="p">:</span> <span class="n">tl</span><span class="p">.</span><span class="n">constexpr</span> <span class="o">=</span> <span class="n">BLOCK_N2</span> <span class="o">//</span> <span class="n">BLK_SLICE_FACTOR</span>
  <span class="n">num_steps</span> <span class="o">=</span> <span class="n">BLOCK_M2</span> <span class="o">//</span> <span class="n">MASK_BLOCK_N2</span>
</code></pre></div> </div> <p>And the start and end indexes are <code>start_m</code> and <code>start_m + BLOCK_M2</code> respectively.</p> </li> </ol> <p>For the non-causal case, in the inner loop over <code class="language-plaintext highlighter-rouge">k, v</code>, the start and end indexes are simply <code class="language-plaintext highlighter-rouge">0</code> and <code class="language-plaintext highlighter-rouge">N_CTX</code>, respectively. However, in my implementation, it is also split into two steps: 1) from <code class="language-plaintext highlighter-rouge">0</code> to <code class="language-plaintext highlighter-rouge">start_m</code>, and 2) from <code class="language-plaintext highlighter-rouge">start_m</code> to <code class="language-plaintext highlighter-rouge">N_CTX</code>.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="nd">@triton.jit</span>
  <span class="k">def</span> <span class="nf">_attn_bwd_dq</span><span class="p">(</span><span class="n">dq</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span>  <span class="c1">#
</span>                   <span class="n">do</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">Di</span><span class="p">,</span>
                   <span class="c1"># shared by Q/K/V/DO.
</span>                   <span class="n">stride_tok</span><span class="p">,</span> <span class="n">stride_d</span><span class="p">,</span>  <span class="c1">#
</span>                   <span class="n">H</span><span class="p">,</span> <span class="n">N_CTX</span><span class="p">,</span>  <span class="c1">#
</span>                   <span class="n">BLOCK_M2</span><span class="p">:</span> <span class="n">tl</span><span class="p">.</span><span class="n">constexpr</span><span class="p">,</span>  <span class="c1">#
</span>                   <span class="n">BLOCK_N2</span><span class="p">:</span> <span class="n">tl</span><span class="p">.</span><span class="n">constexpr</span><span class="p">,</span>  <span class="c1">#
</span>                   <span class="n">HEAD_DIM</span><span class="p">:</span> <span class="n">tl</span><span class="p">.</span><span class="n">constexpr</span><span class="p">,</span>  <span class="c1">#
</span>                   <span class="c1"># Filled in by the wrapper.
</span>                   <span class="n">start_m</span><span class="p">,</span> <span class="n">start_n</span><span class="p">,</span> <span class="n">num_steps</span><span class="p">,</span>  <span class="c1">#
</span>                   <span class="n">MASK</span><span class="p">:</span> <span class="n">tl</span><span class="p">.</span><span class="n">constexpr</span><span class="p">):</span>
      <span class="n">offs_m</span> <span class="o">=</span> <span class="n">start_m</span> <span class="o">+</span> <span class="n">tl</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">BLOCK_M2</span><span class="p">)</span>
      <span class="n">offs_n</span> <span class="o">=</span> <span class="n">start_n</span> <span class="o">+</span> <span class="n">tl</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">BLOCK_N2</span><span class="p">)</span>
      <span class="n">offs_k</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">HEAD_DIM</span><span class="p">)</span>
      <span class="n">kT_ptrs</span> <span class="o">=</span> <span class="n">K</span> <span class="o">+</span> <span class="n">offs_n</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="p">:]</span> <span class="o">*</span> <span class="n">stride_tok</span> <span class="o">+</span> <span class="n">offs_k</span><span class="p">[:,</span> <span class="bp">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">stride_d</span>
      <span class="n">vT_ptrs</span> <span class="o">=</span> <span class="n">V</span> <span class="o">+</span> <span class="n">offs_n</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="p">:]</span> <span class="o">*</span> <span class="n">stride_tok</span> <span class="o">+</span> <span class="n">offs_k</span><span class="p">[:,</span> <span class="bp">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">stride_d</span>
      <span class="c1"># BLOCK_M2 must be a multiple of BLOCK_N2, otherwise the code wouldn't work.
</span>      <span class="n">tl</span><span class="p">.</span><span class="nf">static_assert</span><span class="p">(</span><span class="n">BLOCK_M2</span> <span class="o">%</span> <span class="n">BLOCK_N2</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span>
      <span class="n">curr_n</span> <span class="o">=</span> <span class="n">start_n</span>
      <span class="n">step_n</span> <span class="o">=</span> <span class="n">BLOCK_N2</span>
      <span class="k">for</span> <span class="n">blk_idx</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_steps</span><span class="p">):</span>
          <span class="n">kT</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="n">kT_ptrs</span><span class="p">)</span>
          <span class="n">vT</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="n">vT_ptrs</span><span class="p">)</span>
          <span class="n">s</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">kT</span><span class="p">)</span>
          <span class="n">p</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="n">math</span><span class="p">.</span><span class="nf">exp2</span><span class="p">(</span><span class="n">s</span> <span class="o">-</span> <span class="n">m</span><span class="p">)</span>
          <span class="c1"># Autoregressive masking.
</span>          <span class="k">if</span> <span class="n">MASK</span><span class="p">:</span>
              <span class="n">offs_n</span> <span class="o">=</span> <span class="n">curr_n</span> <span class="o">+</span> <span class="n">tl</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">BLOCK_N2</span><span class="p">)</span>
              <span class="n">mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">offs_m</span><span class="p">[:,</span> <span class="bp">None</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="n">offs_n</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="p">:])</span>
              <span class="n">p</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">where</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span>
          <span class="c1"># Compute dP and dS.
</span>          <span class="n">dp</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">do</span><span class="p">,</span> <span class="n">vT</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">tl</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
          <span class="n">ds</span> <span class="o">=</span> <span class="n">p</span> <span class="o">*</span> <span class="p">(</span><span class="n">dp</span> <span class="o">-</span> <span class="n">Di</span><span class="p">)</span>
          <span class="n">ds</span> <span class="o">=</span> <span class="n">ds</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">tl</span><span class="p">.</span><span class="n">float16</span><span class="p">)</span>
          <span class="c1"># Compute dQ.
</span>          <span class="c1"># NOTE: We need to de-scale dq in the end, because kT was pre-scaled.
</span>          <span class="n">dq</span> <span class="o">+=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">ds</span><span class="p">,</span> <span class="n">tl</span><span class="p">.</span><span class="nf">trans</span><span class="p">(</span><span class="n">kT</span><span class="p">))</span>
          <span class="c1"># Increment pointers.
</span>          <span class="n">curr_n</span> <span class="o">+=</span> <span class="n">step_n</span>
          <span class="n">kT_ptrs</span> <span class="o">+=</span> <span class="n">step_n</span> <span class="o">*</span> <span class="n">stride_tok</span>
          <span class="n">vT_ptrs</span> <span class="o">+=</span> <span class="n">step_n</span> <span class="o">*</span> <span class="n">stride_tok</span>
      <span class="k">return</span> <span class="n">dq</span>

</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="n">kT</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="n">kT_ptrs</span><span class="p">)</span>
  <span class="n">vT</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="n">vT_ptrs</span><span class="p">)</span>
  <span class="n">s</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">kT</span><span class="p">)</span>
  <span class="n">p</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="n">math</span><span class="p">.</span><span class="nf">exp2</span><span class="p">(</span><span class="n">s</span> <span class="o">-</span> <span class="n">m</span><span class="p">)</span>
</code></pre></div></div> <p>This part of code recomputes \(S = QK^T\) and \(P = \operatorname{softmax}(S)\).</p> <p><code class="language-plaintext highlighter-rouge">dp = tl.dot(do, vT).to(tl.float32)</code> implements the equation \(dP = dO V^T\).</p> <p><code class="language-plaintext highlighter-rouge">ds = p * (dp - Di)</code> implements the equation \(dS_{i,:} = P_{i,:} \circ dP_{i,:} - D_i P_{i,:}\).</p> <p><code class="language-plaintext highlighter-rouge">dq += tl.dot(ds, tl.trans(kT))</code> implements the equation \(dQ = dS K\).</p> <h2 id="concluding-remarks">Concluding Remarks</h2> <p>Voila! We have walked through the core implementation of the backward pass of FlashAttention, where the Triton code shares a high similarity with matrix calculus equations. You can check out the Github repo <a href="https://github.com/TotalVariation/Learning-to-Learn-DL/tree/main/flash_attention">Learning-to-Learn-DL</a> containing an IPython notebook which is supposed to offer a more enhanced interactive experience and another notebook where a more flexible implementation of FlashAttention2 is given, which can handle both self-attention and cross-attention with arbitrary lengths. However, for practical usage, I recommend using the official <a href="https://github.com/Dao-AILab/flash-attention">FlashAttention Repo</a> written in CUDA. Furthermore, I believe this post will facilitate your understanding of the Triton implementation given in the official <a href="https://github.com/Dao-AILab/flash-attention">FlashAttention Repo</a>.</p> <div class="language-html highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@misc{Cai2025flashattnbackward-2,
    author = {Xin Cai},
    title = {Learning the Backward Pass of FlashAttention: Part II Implementation in Triton},
    howpublished = {\url{https://totalvariation.github.io/blog/2025/intro-flashattention-backward-part2/},
    note = {Accessed: 2025-07-21},
    year = {2025}
}
</code></pre></div></div>]]></content><author><name>Xin Cai</name></author><summary type="html"><![CDATA[Part II Implementation in Triton]]></summary></entry><entry><title type="html">Michael Oher’s Story at Scale Why Industry-Led Academies May Be the Future</title><link href="https://totalvariation.github.io/blog/2025/rethink-education/" rel="alternate" type="text/html" title="Michael Oher’s Story at Scale Why Industry-Led Academies May Be the Future"/><published>2025-06-19T00:00:00+00:00</published><updated>2025-06-19T00:00:00+00:00</updated><id>https://totalvariation.github.io/blog/2025/rethink-education</id><content type="html" xml:base="https://totalvariation.github.io/blog/2025/rethink-education/"><![CDATA[<div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Quotefancy-2016666-3840x2160-480.webp 480w,/assets/img/Quotefancy-2016666-3840x2160-800.webp 800w,/assets/img/Quotefancy-2016666-3840x2160-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/Quotefancy-2016666-3840x2160.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Photo by <a href="https://quotefancy.com/quote/757737/Sydney-J-Harris-The-whole-purpose-of-education-is-to-turn-mirrors-into-windows">quotefancy</a> </div> <div class="jekyll-twitter-plugin"><blockquote class="twitter-tweet"><p lang="en" dir="ltr">As we progress along the Kardashev Scale, energy harnessed on Earth will increase a hundredfold and will mostly be solar aka fusion aka starlight. <br/><br/>Then energy harnessed will increase perhaps a billionfold if we make it to Kardashev II, with space solar power, and another… <a href="https://t.co/0cHovopB9l">https://t.co/0cHovopB9l</a></p>&mdash; Elon Musk (@elonmusk) <a href="https://twitter.com/elonmusk/status/1921848768208453887?ref_src=twsrc%5Etfw">May 12, 2025</a></blockquote> <script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> </div> <p>I stumbled across an X post from Elon Musk, envisioning the future of humans entering into Kardashev Type II civilization, which prompts me to rethink about today’s education system. Achieving technological feats like reaching a Kardashev Type II civilization would not have been possible without unlocking the potential of at least 80% individuals on this planet—a factor too often overlooked in human history, where the focus has largely been on exploiting various energy sources. Would we have reached today’s technological heights without the Renaissance, revealing what humanity is capable of when we recognize human potential and value?</p> <p>This mission could hardly be fulfilled under today’s brittle education system. Industries already possess the resources—and, more importantly, a concentration of brilliant minds—to build their own academies and train 18-year-olds with high school diplomas through highly optimized curricula, bypassing traditional universities entirely. Would not it be a genuine waste if those brilliant minds played no role in nurturing the next generation? Imagine tech giants like Google or Tesla running academies tailored to their ambitions, taught by their top talent. Universities, after all, have long started to resemble businesses. Yet the privileges they enjoy often seem to adversely affect their ability to cultivate young minds. (It is not universal, but access to those remaining as exceptions is limited.) Self-oversight leads to complacency. What if universities operated like S&amp;P 500 companies—would it be a burden, or would it push them to become truly effective educational institutions? A profit-driven mindset might actually drive education providers to offer peak learning experiences to their customers. Aren’t those unprofitable but scientifically or socially fundamental programs the sources of competitive edge and the lasting value of traditional universities?</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Hg30RBeTJ2nBvcd1ctWp0g-480.webp 480w,/assets/img/Hg30RBeTJ2nBvcd1ctWp0g-800.webp 800w,/assets/img/Hg30RBeTJ2nBvcd1ctWp0g-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/Hg30RBeTJ2nBvcd1ctWp0g.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Photo by <a href="https://medium.com/@marknutter/the-matrix-plot-that-should-have-been-54bcddc60e2b">The Matrix Plot That Should Have Been</a> </div> <p>In The Matrix, a chilling image shows humans encased as batteries to power AI. A more scientifically plausible scenario (temporarily omitting ethical concerns) is one where human minds become distributed simulating environments for AI’s continual learning. Once text and multimodal data are exhausted, the next phase of AI development may depend on mining human thinking traces—an inexhaustible source of training data, assuming human diversity and creativity survive oppressive forces. Until breakthroughs in brain–computer interface technologies allowing effective conversion of biochemical signals in human brains into tokens, these thinking traces could be harvested through human–LLM interactions. A centrally trained LLM serves as a baseline with reasonable cognitive capacity for meaningful human-AI interactions, but its future potential hinges on decentralized learning powered by billions of end users, each acting as a unique, parallel simulation environment.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/0b5fde96-c426-442e-b595-9cc954954314-480.webp 480w,/assets/img/0b5fde96-c426-442e-b595-9cc954954314-800.webp 800w,/assets/img/0b5fde96-c426-442e-b595-9cc954954314-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/0b5fde96-c426-442e-b595-9cc954954314.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Photo by <a href="https://casdbeavertales.org/28879/arts-entertainment/the-blind-side-an-inspiring-story-about-helping-others/">BEAVER TALES</a> </div> <p>To me, The Blind Side captures the true purpose of education: to uplift and empower individuals. Yet today’s educational institutions have gone astray, sinking into a swamp of reward hacking—where funding takes precedence over genuine learning. In contrast, high-tech companies dedicated to education services, combining AI tutors and human tutors and no longer hiding profit motives behind a glittering facade, hold the potential to revolutionize the system, unlocking each individual’s potential with personalized learning experiences at scale. Platforms like Coursera and edX have begun dipping toes, but so far only as supplements to traditional paths. Transformative stories like Michael Oher’s cannot be replicated by the current system—but with personalized AI tutors, they might have become scalable. More importantly, industry-led academies should emerge as a true parallel system, not just an auxiliary option.</p> <h2 id="acknowledgement">Acknowledgement</h2> <p>The completion of this blog post was assisted by OpenAI ChatGPT in refining phrasing, expression, and recommending titles and sayings.</p> <div class="language-html highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@misc{Cai2025rethinked,
    author = {Xin Cai},
    title = {Michael Oher's Story at Scale: Why Industry-Led Academies May Be the Future},
    howpublished = {\url{https://totalvariation.github.io/blog/2025/rethink-education/}},
    note = {Accessed: 2025-06-19},
    year = {2025}
}
</code></pre></div></div>]]></content><author><name>Xin Cai</name></author><summary type="html"><![CDATA[A Perspective by Someone Who Couldn't Hack Academia]]></summary></entry><entry><title type="html">A Primer on Multimodal Large Language Models</title><link href="https://totalvariation.github.io/blog/2024/a-primer-on-mllms/" rel="alternate" type="text/html" title="A Primer on Multimodal Large Language Models"/><published>2024-05-20T00:00:00+00:00</published><updated>2024-05-20T00:00:00+00:00</updated><id>https://totalvariation.github.io/blog/2024/a-primer-on-mllms</id><content type="html" xml:base="https://totalvariation.github.io/blog/2024/a-primer-on-mllms/"><![CDATA[<blockquote> <p><b>The best material model of a cat is another, or preferably the same, cat.</b></p> <footer>- Norbert Wiener</footer> </blockquote> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0" id="figure-1"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mllm_primer_post_robot_cat_sdxl-480.webp 480w,/assets/img/mllm_primer_post_robot_cat_sdxl-800.webp 800w,/assets/img/mllm_primer_post_robot_cat_sdxl-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/mllm_primer_post_robot_cat_sdxl.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Fig-1 A sketch of general purpose AI assistants. Image Source: <a href="https://huggingface.co/docs/diffusers/en/using-diffusers/sdxl" target="_blank">Stable Diffusion XL</a>. </div> <h2 id="background">Background</h2> <p>Large Language Models (LLMs) <d-cite key="zhao2023survey"></d-cite> have achieved remarkable milestones, especially the demonstrated emergent capabilities, e.g., In-Context Learning (ICL), Chain of Thought (CoT) reasoning, and instruction following. It is now an inevitable trend to integrate various modalities with LLMs to mimic the way humans interact with the open world, thus ushering in the new era of Multimodal Large Language Models (MLLMs <d-footnote>Please note that the term MLLMs hereafter is used interchangeably with Large Vision-Language Models (LVLMs) or Large Multimodal Models (LMMs) given that the transitive property of modality embeddings showcased in ImageBind. <d-cite key="girdhar2023imagebind"></d-cite> </d-footnote>) <d-cite key="yin2023survey"></d-cite>.</p> <p>Recent work <d-cite key="li2023multimodal"></d-cite> has demonstrated that the development of MLLMs signifies a transition from specialist models to general-purpose visual assistants. This transition is regarded as a crucial step towards realizing the grand vision of a general-purpose multimodal AI agent, akin to bringing the beloved Japanese cartoon character Doraemon to life. It is further unveiled in <d-cite key="li2023multimodal"></d-cite> that a converging point for many development trajectories of vision-language/multimodal foundation models is <em>“the creation of general-purpose models and systems capable of following human intents and effortlessly executing a diverse array of vision and vision-language tasks in the wild.”</em></p> <p>The key features that markedly differentiate MLLMs from deep learning models developed beforehand are condensed into the name itself: <mark>General Purpose Assistants</mark>, which include <strong>versatility</strong>, <strong>interactivity</strong>, and <strong>controllability</strong>. Specifically, the quest for a unified architecture capable of accomplishing a diverse range of discriminative and generative tasks has witnessed great success in Natural Language Processing (NLP), as exemplified by GPT-3, inspiring similar research endeavours in the field of computer vision. However, as pointed out in <d-cite key="li2023multimodal"></d-cite>, the development of unified vision systems significantly lags behind due to fragmented vision tasks and the difficulty of scaling up visual data. Furthermore, to enable seamless communication between machines and humans, natural language serves as a core component in building a general interactive interface, which can be further complemented by multimodal prompts such as clicks or scribbles provided by users. Additionally, researchers have made strides in steering the output behaviour of MLLMs by instruction tuning or aligning with human preferences, marking a hallmark in elevating machine intelligence to the next level as humans begin to positively intervene in the machine learning process. By leveraging human feedback and rich experience, machines can evolve more effectively and efficiently, leading to better outcomes for both machines and humans. Inspired by the interactions between Doraemon and Nobita, the ultimate goal of general-purpose AI assistants is to establish a <strong>symbiotic relationship</strong> between humans and AI systems, where AI systems not only enhance the overall well-being and quality of human life but also evolve by engaging in humans’ daily activities.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0" id="figure-2"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mllm_primer_post_genaisys-480.webp 480w,/assets/img/mllm_primer_post_genaisys-800.webp 800w,/assets/img/mllm_primer_post_genaisys-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/mllm_primer_post_genaisys.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Fig-2 The general architecture of GenAISys. Image Source: <a href="https://jmtomczak.github.io/blog/21/21_genaisys.html" target="_blank">Jakub Tomczak</a>. </div> <p>Before delving into the technical aspects of MLLMs, I would like to introduce an intriguing and inspiring concept called Generative AI Systems (GenAISys), as shown in <a href="#figure-2">Fig-2</a>, recently introduced in a <a href="https://jmtomczak.github.io/blog/21/21_genaisys.html">blog post</a> by Jakub Tomczak, which offers a novel perspective on MLLMs. In essence, GenAISys can be considered as end-to-end trainable MLLMs enhanced by external tools or databases, resonating with a similar idea proposed in Chapter 6 of the work <d-cite key="li2023multimodal"></d-cite>, where the authors indicate the possibility of combining strengths from two existing modelling paradigms of MLLMs: training or chaining tools with LLMs. For more insights into GenAISys and its future directions, I encourage you to refer to the original post by Jakub Tomczak.</p> <h2 id="technical-overview">Technical Overview</h2> <p>The recent progress of MLLMs <d-footnote>Technically, the progress reviewed here excludes that related to Multimodal Agents, i.e., chaining tools with LLMs.</d-footnote> can be roughly divided into the following three aspects: 1) architectures, 2) training strategies and data, 3) evaluation benchmarks.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0" id="figure-3"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mllm_primer_post_mllm_arch-480.webp 480w,/assets/img/mllm_primer_post_mllm_arch-800.webp 800w,/assets/img/mllm_primer_post_mllm_arch-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/mllm_primer_post_mllm_arch.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Fig-3 An overview of MLLM architecture. </div> <h3 id="architectures">Architectures</h3> <p>A typical neural architecture of MLLMs, as illustrated in <a href="#figure-3">Fig-3</a>, consists of pre-trained modality encoders, pre-trained LLMs, learnable modality adapters/connectors, and optional modality decoders.</p> <p><strong>Modality Encoders</strong> When compressing raw images, it is common practice to utilize pre-trained CLIP <d-cite key="radford2021learning"></d-cite> image encoders or other similar variants <d-cite key="sun2023eva"></d-cite>, leveraging a pre-aligned albeit coarse vision-semantic space. The underlying principle is to discretize/standardize modality embeddings with reference to language vocabulary, which can be extended to encoding other kinds of multimodal signals. For instance, the ImageBind <d-cite key="girdhar2023imagebind"></d-cite> encoder, used in ImageBind-LLM <d-cite key="han2023imagebind"></d-cite>, can construct a joint embedding space for six different modalities—images, text, audio, depth, thermal, and Inertial Measurement Unit (IMU) data—established by aligning a pair of modalities each time with InfoNCE <d-cite key="oord2018representation"></d-cite> loss.</p> <p>Another critical factor in enriching visual embeddings in MLLMs and mitigating multimodal hallucination is increasing input resolution <d-cite key="liu2023improved"></d-cite> while not incurring significant computational overheads. Following this principle, dual vision encoders are employed in Mini-Gemini <d-cite key="li2024mini"></d-cite>, where the coarse embeddings from the pre-trained CLIP image encoder are complemented by dense visual features output from the LAION-pretrained <d-cite key="schuhmann2022laion"></d-cite> ConvNeXt <d-cite key="liu2022convnet"></d-cite>, which serves as the high-resolution vision encoder.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0" id="figure-4"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mllm_primer_post_adc-480.webp 480w,/assets/img/mllm_primer_post_adc-800.webp 800w,/assets/img/mllm_primer_post_adc-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/mllm_primer_post_adc.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Fig-4 An illustration of workings of Analog-to-Digital Converters (ADC). Image Source: <a href="https://www.doeeet.com/content/eee-components/actives/analog-to-digital-converters-the-resolution-dilemma/" target="_blank">DOEEET</a>. </div> <p>Finally, I would like to highlight an intriguing perspective by drawing a parallel between the modality encoder in MLLMs and the Analog-to-Digital Converter (ADC), as shown in <a href="#figure-4">Fig-4</a>, which converts continuous analog signals into discrete digital signals through sampling and quantization. By establishing such a rough equivalence, we can gain deeper insights into the design rationale of the modality encoder. Specifically, bandwidth and Signal-to-Noise Ratio (SNR) play a crucial role in determining the overall performance and quality of an ADC. Analogously, the range of frequencies in input signals that can be captured by the modality encoder contributes to the overall performance of MLLMs, justifying the utilization of dual vision encoders, e.g., in Mini-Gemini <d-cite key="li2024mini"></d-cite>. As demonstrated in <d-cite key="park2022vision"></d-cite>, multi-head self-attention (MSAs) in Vision Transformers (ViT) <d-cite key="dosovitskiy2020image"></d-cite> function as a low-pass filter, while convolution operations act oppositely as a high-pass filter. Therefore, the convolutional branch supplements the ViT with high-frequency information, expanding the bandwidth of the modality encoder. Furthermore, image-text contrastive pre-training can be seen as a quantization process, where discrete codes (i.e., vocabulary indices) are allocated to encoded visual signals by filtering out noise and low-level information while maximally preserving mutual information. By analogy with a higher SNR leading to finer resolution of an ADC, the ability to discern the smallest semantic variation of the modality encoder can be improved by adopting fine-grained contrastive loss <d-cite key="li2022grounded"></d-cite> or detailed captioning <d-cite key="yu2022coca"></d-cite>.</p> <p><strong>LLMs</strong> Regarding pre-trained LLMs, open-source models such as the LLaMA series <d-cite key="touvron2023llama"></d-cite> <d-cite key="touvron2023llama2"></d-cite> and Vicuna family <d-cite key="chiang2023vicuna"></d-cite> have gained widespread popularity in academic research. These models are often combined with Parameter-Efficient Fine-Tuning (PEFT) techniques <d-cite key="han2024parameter"></d-cite>, such as Low-Rank Adaptation (LoRA) <d-cite key="hu2021lora"></d-cite>, in the Supervised Fine-Tuning (SFT) phase to enhance instruction following and alignment with human preference. Furthermore, LLMs constructed with a Mixture of Experts (MoE) <d-cite key="shen2023mixture"></d-cite> <d-cite key="jiang2024mixtral"></d-cite> have attracted increasing attention due to the reduced computational cost of the sparse architecture.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0" id="figure-5"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mllm_primer_post_microcontroller-480.webp 480w,/assets/img/mllm_primer_post_microcontroller-800.webp 800w,/assets/img/mllm_primer_post_microcontroller-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/mllm_primer_post_microcontroller.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Fig-5 A schematic diagram of microcontrollers, where the central unit CPU interacts with various components to process data, control and communicate with other devices. Image Source: <a href="https://www.hnhcart.com/blogs/microcontrollers/what-are-the-major-factors-on-which-microcontrollers-are-differentiated" target="_blank">The Major Factors On Which The Microcontrollers Are Differentiated</a>. </div> <p>Compared to human-like text generation, the emergent capabilities of LLMs, such as ICL and CoT reasoning, unlock the vast potential of machines in handling complex and flexible human instructions. When LLMs are extended with interfaces to sense and interact with the physical world, i.e., MLLMs, they can be analogized to microcontrollers, as shown in <a href="#figure-5">Fig-5</a>, provided disregarding differences in energy consumption and application scenarios. A microcontroller is essentially a small computer, comprising a processor core, memory, and input/output (I/O) peripherals. Similarly, LLMs act as the processor core, executing human instructions rather than pre-programmed commands. Modality encoders and decoders enable LLMs to interact with the external world, analogous to ADCs or Digital-to-Analog Converters (DACs). The key distinction between microcontrollers and MLLMs lies in the latter’s capability for learning and adaptation, ushering in a new era where machines can continuously evolve through direct interaction with humans. Human language is distinguished by its complexity, generativity, and ability to convey abstract and displaced concepts, which supports the language-centric design pattern of MLLMs. However, this design is yet to be definitively validated, as discussed in Chapter 4 of <d-cite key="li2023multimodal"></d-cite>, particularly considering the distinct differences between vision and language.</p> <p><strong>Modality Connectors</strong> Due to the noticeable gap between embeddings of natural languages and other modalities, particularly considering the difference in bandwidth, it is essential to establish reliable alignment with reference to textual embeddings to enable LLMs to understand sensory inputs. This alignment involves projecting embeddings from modality encoders into a space comprehensible to LLMs, ensuring unambiguous interpretation. This can be achieved using a trainable modality adapter or connector, bridging the frozen modality encoder and the LLM, which is computationally efficient and more accessible.</p> <p>Currently, multimodal alignment can be achieved at either the token- or feature-level. At the token-level, modality-specific embeddings are converted into corresponding tokens, which are then concatenated with textual tokens and fed into LLMs for SFT. The modules commonly used for modality conversion include Q-Former <d-cite key="li2023blip"></d-cite> <d-cite key="zhang2023video"></d-cite>, MLP <d-cite key="liu2024visual"></d-cite> <d-cite key="liu2023improved"></d-cite>, and cross-attention layers <d-cite key="alayrac2022flamingo"></d-cite>. In contrast to lightweight modality adapters, a pre-trained LLaMA is utilized as the modality connector in InternVL <d-cite key="chen2023internvl"></d-cite>, acting as a language middleware. This approach may allow for the utilization of frozen LLMs in the SFT stage. Feature-level fusion involves deep interaction between modality-specific and language features, usually achieved by inserting extra trainable modules into a frozen LLM <d-cite key="alayrac2022flamingo"></d-cite> <d-cite key="zhang2023llama"></d-cite>, which may compromise the innate language capabilities.</p> <p>The difficulty in multimodal alignment may stem from the next-token prediction loss used in optimizing LLMs, which may favor an over-reliance on the LLM’s parametric knowledge, as discussed in <d-cite key="zhai2023halle"></d-cite>, rather than faithfully grounding predictions in multimodal signals. Consequently, this can lead to multimodal hallucinations, where the model generates outputs that are inconsistent with the concrete content contained in sensory inputs. Another factor significantly influencing the generation of LLMs is the adopted decoding strategies. Recent work <d-cite key="leng2023mitigating"></d-cite> has shown that multimodal hallucination can be alleviated by adapting advanced decoding methods, such as contrastive decoding <d-cite key="li2022contrastive"></d-cite>, for MLLMs as a training-free corrective mechanism. Apart from using modality connectors, another viable solution is to textualize non-language signals using expert models, as exemplified by the VideoChat-Text model <d-cite key="li2023videochat"></d-cite>, though this approach comes with the downsides of information loss and a pipeline that is not end-to-end optimized.</p> <p><strong>Modality Decoders</strong> In addition to the aforementioned three compulsory modules, MLLMs can be further extended with modality-specific decoders, particularly in the Any-to-Any workflow, to overcome the limitation of MLLMs only perceiving multimodal data.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0" id="figure-6"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mllm_primer_post_nextgpt-480.webp 480w,/assets/img/mllm_primer_post_nextgpt-800.webp 800w,/assets/img/mllm_primer_post_nextgpt-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/mllm_primer_post_nextgpt.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Fig-6 An overview of an Any-to-Any multimodal system NExT-GPT that can perceive inputs and generate outputs in arbitrary combinations of text, images, videos, and audio. Image Source: <a href="https://arxiv.org/abs/2309.05519" target="_blank">NExT-GPT: Any-to-Any Multimodal LLM</a>. </div> <p>MLLMs with multimodal responsive capabilities have recently spurred a surge of interest in the area of Generative AI, regarded as a significant step towards Artificial General Intelligence (AGI). For image generation, recent works such as Emu <d-cite key="sun2023generative"></d-cite> and Mini-Gemini <d-cite key="li2024mini"></d-cite> default to using diffusion-based generative models <d-cite key="luo2022understanding"></d-cite> <d-cite key="ribeiro2024demystifying"></d-cite> <d-cite key="chan2024tutorial"></d-cite>, such as the Stable Diffusion family <d-cite key="rombach2022high"></d-cite>, due to their unparalleled text-to-image generation performance. The work NExT-GPT <d-cite key="wu2023next"></d-cite>, as illustrated in <a href="#figure-6">Fig-6</a>, further extends LLMs to include image, audio, and video diffusion models, enabling content creation in arbitrary combinations of image, text, audio, and video. Similar to the modality connectors used in the encoding phase, it is reasonable to bridge the gap between textual embeddings generated by LLMs and those used as conditional guidance in diffusion models, which can be achieved through decoder-side alignment.</p> <h3 id="training-strategies-and-data">Training Strategies and Data</h3> <p>The training process of MLLMs can be decomposed into three stages: 1) pre-alignment, 2) instruction tuning, 3) alignment with human preferences. The fulfilment of training objective in each phase heavily relies on specific data, necessitating cost-effective data scaling-up methods and high quality data.</p> <p><strong>Pre-Alignment</strong> The pre-alignment stage often requires an enormous amount of text-paired data, such as images, videos, or audio files with associated textual descriptions, usually gathered from the Internet. However, the innate nature of LLMs as generative pre-trained models (next-token-prediction) casts doubt on the validity of using noisy and short text-paired data, which is originally used for contrastive alignment. As demonstrated in works such as ShareGPT4V <d-cite key="chen2023sharegpt4v"></d-cite>, both the pre-alignment and SFT stages significantly benefit from highly descriptive caption data. Additionally, the progressive alignment scheme adopted in InternVL <d-cite key="chen2023internvl"></d-cite>, transitioning from vision-language contrastive learning to generative pre-training, has proven to be more effective in accommodating the heterogeneous sources of pre-training data.</p> <p><strong>Instruction Tuning</strong> Instruction tuning (IT) <d-cite key="zhang2023instruction"></d-cite> aims to steer LLMs to respond more faithfully to human instructions by fine-tuning on instruction following datasets, which consist of (Instruction, Input, Output) triplets. This technique has proven to be effective and computationally efficient in enhancing the controllability of LLMs’ output behavior, thereby eliciting knowledge from LLMs that is well-aligned with human intents. Furthermore, IT has been identified as a critical factor in unlocking the few-shot (ICL) or zero-shot generalization capability of LLMs on solving novel tasks.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0" id="figure-7"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mllm_primer_post_self_instruct-480.webp 480w,/assets/img/mllm_primer_post_self_instruct-800.webp 800w,/assets/img/mllm_primer_post_self_instruct-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/mllm_primer_post_self_instruct.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Fig-7 An illustration of the overall process of Self-Instruct. Image Source: <a href="https://arxiv.org/abs/2212.10560" target="_blank">SELF-INSTRUCT: Aligning Language Models with Self-Generated Instructions</a>. </div> <p>The current trend in scaling up IT data involves leveraging semi-automatic data engines to minimize human intervention. Specifically, the majority of IT datasets are constructed using a paradigm known as Self-Instruct <d-cite key="wang2022self"></d-cite>, as depicted in <a href="#figure-7">Fig-7</a>, where a few manually-crafted seed examples serve as demonstrations for LLMs, such as GPT-4, to generate additional similar samples by harnessing the ICL capability. This approach can be extended to generate multimodal IT data. For instance, in the pioneering work LLaVA <d-cite key="liu2024visual"></d-cite>, images are converted into text descriptions and bounding box coordinates, thereby endowing the text-only GPT-4 with an imaginary vision capability, which are then combined with diverse task prompts to create visual instruction-following datasets. However, IT data built on top of GPT-4 (text-only LLMs) often suffer from multimodal hallucination, compromising the quality of the generated samples, which can be remedied with the advent of powerful MLLMs. For instance, in the work ALLaVA <d-cite key="chen2024allava"></d-cite>, GPT-4V, a multimodal variant of GPT-4, is utilized to create high-quality IT data by following a captioning-questioning-answering pipeline. Moreover, recent works, such as ALLaVA <d-cite key="chen2024allava"></d-cite> and Lynx <d-cite key="zeng2023matters"></d-cite>, emphasize that the quality of IT datasets—specifically the complexity and diversity of instructions and the detail of responses—is more crucial than their quantity.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0" id="figure-8"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mllm_primer_post_data_engine-480.webp 480w,/assets/img/mllm_primer_post_data_engine-800.webp 800w,/assets/img/mllm_primer_post_data_engine-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/mllm_primer_post_data_engine.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Fig-8 A schematic diagram of semi-automatic data engine. </div> <p>Despite the promising performance gains achieved through IT, there have been criticisms regarding its efficacy. For instance, it has been shown that instruction-tuned LLMs merely mimic the style of proprietary models like ChatGPT and fall short in those critical dimensions such as factuality, coding, and problem solving <d-cite key="kung2023models"></d-cite> <d-cite key="gudibande2023false"></d-cite>, which has led to the conclusion that bridging the capabilities gap between open-source LLMs and proprietary ones through IT is considered a false promise <d-cite key="gudibande2023false"></d-cite>. Nevertheless, leveraging a scalable semi-automatic data engine (<a href="#figure-8">Fig-8</a>)-an iteratively refined pseudo data labelling/generation pipeline with human in the loop-is a key enabler for advancing MLLMs. For example, the All-Seeing Project <d-cite key="wang2023all"></d-cite>, a recent effort in building an open-world visual recognition and understanding model, created approximately 1 billion instance-level annotations of open-world visual concepts with detailed captions and question-answer pairs, which would have been prohibitive without the assistance of semi-automatic data engines.</p> <p><strong>Preference Alignment</strong> Alignment with human preferences aims to further refine the output behaviour of MLLMs that have undergone SFT or instruction tuning by leveraging human/AI feedback on model responses. The two mainstream solutions currently in use are Reinforcement Learning with Human Feedback (RLHF) <d-cite key="ziegler2019fine"></d-cite> <d-cite key="ouyang2022training"></d-cite> and Direct Preference Optimization (DPO) <d-cite key="rafailov2024direct"></d-cite>.</p> <h3 id="evaluation">Evaluation</h3> <p>The evaluation of MLLMs is generally categorized into closed-ended and open-ended questions. For open-ended questions, the assessment criteria may involve human or GPT scoring. In contrast to evaluation methods developed before the era of LLMs, designing evaluation toolkits for MLLMs demands attention and efforts comparable to or even surpass those required for model development. As pointed out in <d-cite key="gudibande2023false"></d-cite>, human evaluators without domain expertise or significant time investment can be easily deceived by LLMs due to their fluent, confident, and well-structured responses. Therefore, acquiring quantitative measurements that can objectively and reliably reflect the multifaceted aspects of MLLMs is crucial, as these measurements facilitate pinpointing critical factors contributing to improvements along specified skill dimensions, identifying failure cases, and providing deeper insights into the inner workings of MLLMs.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0" id="figure-9"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mllm_primer_post_ai_dev_cycle-480.webp 480w,/assets/img/mllm_primer_post_ai_dev_cycle-800.webp 800w,/assets/img/mllm_primer_post_ai_dev_cycle-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/mllm_primer_post_ai_dev_cycle.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Fig-9 An illustration of AI development cycle: a dynamic and iterative process where data creation, model development, and model assessment continuously inform and improve each other. </div> <p>We should be wary of increasingly potent MLLMs that can take shortcuts to inflate performance metrics and deliver delusive results, which requires researchers to act as adversaries, critically analyzing and probing the models’ responses for weaknesses. Consequently, evaluation frameworks that combine both human and AI efforts and can continuously evolve offer promising avenues to mitigate these issues, as exemplified in Dynabench <d-cite key="kiela2021dynabench"></d-cite>, where dataset creation, model development, and model evaluation co-evolve and reinforce each other, as shown in <a href="#figure-9">Fig-9</a>, serving as a counterbalance to the phenomenon where state-of-the-art AI systems quickly saturate benchmarks but fail on real-world challenges.</p> <h2 id="concluding-remarks">Concluding Remarks</h2> <p>The development of LLMs and their extension to multimodal understanding (MLLMs) reveals a promising pathway for machines to approach human-level intelligence: by actively interacting with humans and engaging in human daily activities, reminiscent of how we nurture and educate our offspring. In essence, the emergence of MLLMs marks a watershed moment in the evolution of machine learning systems, ushering in the era of Human-Centered AI (HCAI), which begins with the creation of a more natural and intuitive human-machine interaction interface. Reflecting on the saying of Richard Feynman that “What I cannot create, I do not understand,” we can anticipate significant breakthroughs in neuroscience when AI systems can faithfully emulate human cognitive capabilities by generating responses indistinguishable from those of humans. The development of AI assistants akin to Doraemon can provide profound insights into the neural and cognitive mechanisms underlying human intelligence and foster a closer synergy between neuroscience and AI research. While Norbert Wiener’s saying, “The best material model of a cat is another, or preferably the same, cat,” underscores the immense challenge of precisely simulating the complexity and intricacies inherent in biological systems with artificial models, it is exciting to contemplate the potential of an increasingly close collaborative relationship between humans and AI.</p> <h2 id="acknowledgement">Acknowledgement</h2> <p>This blog post draws significant inspiration from two outstanding works reviewing the latest advances in MLLMs:</p> <ol> <li> <p>“A Survey on Multimodal Large Language Models” <d-cite key="yin2023survey"></d-cite>, which provides a timely and comprehensive overview of recent progress in MLLMs, supplemented by a continuously updated GitHub webpage: <a href="https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models">Awesome-Multimodal-Large-Language-Models</a>.</p> </li> <li> <p>“Multimodal Foundation Models: From Specialists to General-Purpose Assistants” <d-cite key="li2023multimodal"></d-cite>, where leading researchers in the field present an insightful dissection, offering both a historical perspective on the development of multimodal foundation models and an inspiring vision for their future.</p> </li> </ol> <p>Additionally, the completion of this blog post was assisted by OpenAI ChatGPT in refining phrasing, expression, and tone.</p> <div class="language-html highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@misc{Cai2024mllm_primer,
    author = {Xin Cai},
    title = {A Primer on Multimodal Large Language Models: Towards Building Doraemon-like AI Assistants},
    howpublished = {\url{https://totalvariation.github.io/blog/2024/a-primer-on-mllms/}},
    note = {Accessed: 2024-05-20},
    year = {2024}
}
</code></pre></div></div>]]></content><author><name>Xin Cai</name></author><category term="MLLMs,"/><category term="LVLMs,"/><category term="General-Purpose-Assistants,"/><category term="GenAI"/><summary type="html"><![CDATA[Towards Building Doraemon-like AI Assistants]]></summary></entry><entry><title type="html">Random Thoughts on Open World Vision Systems</title><link href="https://totalvariation.github.io/blog/2024/open-world-vision-systems/" rel="alternate" type="text/html" title="Random Thoughts on Open World Vision Systems"/><published>2024-02-01T00:00:00+00:00</published><updated>2024-02-01T00:00:00+00:00</updated><id>https://totalvariation.github.io/blog/2024/open-world-vision-systems</id><content type="html" xml:base="https://totalvariation.github.io/blog/2024/open-world-vision-systems/"><![CDATA[<blockquote> <p><b>It is not the strongest of the species that survives, nor the most intelligent that survives. It is the one that is most adaptable to change.</b></p> <footer>- Charles Darwin</footer> </blockquote> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ante-hamersmit-qM8zX1celvc-unsplash-480.webp 480w,/assets/img/ante-hamersmit-qM8zX1celvc-unsplash-800.webp 800w,/assets/img/ante-hamersmit-qM8zX1celvc-unsplash-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/ante-hamersmit-qM8zX1celvc-unsplash.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Photo by <a href="https://unsplash.com/@ante_kante?utm_content=creditCopyText&amp;utm_medium=referral&amp;utm_source=unsplash">Ante Hamersmit</a> on <a href="https://unsplash.com/photos/person-holding-reptile-qM8zX1celvc?utm_content=creditCopyText&amp;utm_medium=referral&amp;utm_source=unsplash">Unsplash</a> </div> <p>Over the last decade, there has been remarkable progress in visual perception algorithms, driven by the development of layered differentiable models optimized in an end-to-end fashion. Despite these advancements, the deployment of such algorithms in open-world scenarios poses significant challenges. To effectively operate in open-world settings, these algorithms must incorporate capabilities for open-set recognition and open vocabulary learning, while also being robust to distribution shifts. Moreover, Open World Visual Understanding Systems (OWVUS) are expected to handle continuous streams of real-world data, adapting to non-stationary environments with minimal labelling requirements. This suggests the need for semi- or fully-automatic data engines that facilitate model learning alongside incremental data curation and labelling, potentially assisted with human intervention. The foundational pillars of building OWVUS lie in data and annotation efficiency, robustness, and generalization. These challenges can potentially be addressed through a unified framework known as Open-set Unsupervised Domain Adaptation (OUDA).</p> <p>In general, visual signals can be decomposed into low- and high-frequency components. The former enables the unlocking of open-vocabulary capabilities in Vision-Language Models (VLMs) <d-cite key="radford2021learning"></d-cite> <d-cite key="li2021align"></d-cite> by establishing a well-aligned visual-semantic space. Conversely, the latter plagues the generalizability of deep neural networks on unseen or novel domains, as they may overfit to specific domain styles and therefore capture spurious correlations. OUDA serves as a nexus that connects a broad spectrum of research fields, including generative modelling <d-cite key="hoffman2018cycada"></d-cite> <d-cite key="ilse2020diva"></d-cite> <d-cite key="mahajan2020latent"></d-cite>, semi-supervised learning <d-cite key="berthelot2021adamatch"></d-cite> <d-cite key="zhang2020label"></d-cite>, meta-learning <d-cite key="shu2021open"></d-cite> <d-cite key="kim2022pin"></d-cite> <d-cite key="zhao2021learning"></d-cite>, open-set recognition <d-cite key="saito2021ovanet"></d-cite> <d-cite key="saito2020universal"></d-cite>, open-vocabulary learning <d-cite key="yu2023open"></d-cite> <d-cite key="zara2023autolabel"></d-cite> <d-cite key="wu2023towards"></d-cite>, out-of-distribution detection <d-cite key="shu2023clipood"></d-cite> <d-cite key="wang2023clipn"></d-cite>, few-/zero-shot learning <d-cite key="wu2022style"></d-cite> <d-cite key="fahes2023poda"></d-cite>, contrastive learning <d-cite key="da2022dual"></d-cite> <d-cite key="sahoo2021contrast"></d-cite> <d-cite key="zara2023simplifying"></d-cite>, unsupervised representation learning <d-cite key="hoyer2023mic"></d-cite> <d-cite key="vray2024distill"></d-cite>, active learning <d-cite key="zhang2022bamboo"></d-cite> <d-cite key="wu2022entropy"></d-cite>, continual learning <d-cite key="atanyan2024continuous"></d-cite> <d-cite key="lin2022prototype"></d-cite> <d-cite key="wang2302comprehensive"></d-cite>, and disentanglement of factors of variation <d-cite key="wu2022single"></d-cite> <d-cite key="wei2022unsupervised"></d-cite>. Its primary objective is to transfer knowledge from annotated source domains or pre-trained models (in the case of source-free UDA <d-cite key="liang2020we"></d-cite>) to unlabelled target domains. This transfer process requires addressing both covariate and semantic shifts, which correspond to variations in the high- and low-frequency components of visual signals, respectively. In essence, OUDA leverages methods developed within the aforementioned research areas to combat covariate or semantic shifts. On the other hand, addressing distribution shifts becomes unavoidable when extending any research problem from these areas to a more general and practical context.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0" id="figure-1"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/open_world_vision_post_uda_methods-480.webp 480w,/assets/img/open_world_vision_post_uda_methods-800.webp 800w,/assets/img/open_world_vision_post_uda_methods-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/open_world_vision_post_uda_methods.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Fig-1 The taxonomy of UDA methods. </div> <p>OUDA integrates open-set recognition/open-vocabulary learning and domain adaptation/generalization within a unified framework, aiming to address both high-level semantic shift and low-level covariate shift simultaneously and therefore presenting compounded challenges that stem from both research domains. Recent advancements have demonstrated rapid progress in constructing sophisticated open-vocabulary detectors or segmentors <d-cite key="zhu2023survey"></d-cite>, facilitated by VLMs trained on web-scale image-text pairs which offer a comprehensive prior of the real-world. Central to this advancement is the endeavour to bridge the granularity gap between coarse-grained visual-semantic associations and fine-grained visual understanding objectives. Predominant solutions can fall into three main categories: (1) incorporating fine-grained awareness into pre-training recipes <d-cite key="li2022grounded"></d-cite> <d-cite key="zhong2022regionclip"></d-cite> <d-cite key="rao2022denseclip"></d-cite>; (2) transferring knowledge from VLMs to downstream fine-grained visual understanding tasks <d-cite key="gu2021open"></d-cite> <d-cite key="kuo2022f"></d-cite> <d-cite key="wu2023cora"></d-cite> <d-cite key="he2023clip"></d-cite>; (3) the amalgamation of Vision Foundation Models (VFMs) by leveraging their complementary expertise resulting from distinct pretraining objectives <d-cite key="han2023boosting"></d-cite> <d-cite key="wang2023sam"></d-cite>. Besides, the pursuit of handling diverse vision tasks with a unified architecture and a single suite of weights in the open-set scenario <d-cite key="zou2023generalized"></d-cite> <d-cite key="zhang2023simple"></d-cite> has garnered increasing attention as a step towards constructing general-purpose vision foundation models.</p> <p>In the emerging task of Open-World Object Detection (OWOD) <d-cite key="joseph2021towards"></d-cite> <d-cite key="gupta2022ow"></d-cite> <d-cite key="wang2023detecting"></d-cite>, which combines open-set recognition with class incremental learning, the inherent open-vocabulary capability of VLMs offers convenience in identifying unknown classes. However, specialized components remain indispensable for the discovery of novel classes. Essentially, <strong>equipping a neural network with the ability to say NO when facing unfamiliar input</strong>, even with models like CLIP <d-cite key="wang2023clipn"></d-cite>, presents significant challenges. Particularly in vision tasks, developing a class-agnostic object localizer capable of generalizing to novel classes remains an open question <d-cite key="kim2022learning"></d-cite>. This challenge proves critical for two-stage open-world detectors or segmentors, as the generation of high-quality proposals for unknown classes is pivotal. Recently, there is a promising trend where class-agnostic object discovery is tackled without requiring any manual annotation by leveraging pre-trained features of self-supervised Vision Transformers (ViTs) <d-cite key="simeoni2023unsupervised"></d-cite>. However, these algorithms still struggle in complex scene-centric scenarios. OUDA introduces a more complicated pipeline compared to UDA with a close-set assumption, requiring the detection or rejection of unknown classes followed by cross-domain alignment with dominant solutions illustrated in <a href="#figure-1">Fig-1</a>. It has been demonstrated that overlooking unknown classes during domain alignment can lead to negative transfer or even catastrophic misalignment. As far as I know, existing methods for open-set recognition or novel class discovery largely rely on heuristic approaches, such as one-vs-all classifiers <d-cite key="saito2021ovanet"></d-cite>, entropy-based separation <d-cite key="saito2020universal"></d-cite>, inter-class distance or margin-based methods <d-cite key="miller2021class"></d-cite>, and leveraging zero-shot predictions from VLMs <d-cite key="yu2023open"></d-cite> <d-cite key="zara2023autolabel"></d-cite>. Furthermore, effectively separating (target-)private samples into semantically coherent clusters <d-cite key="zara2023autolabel"></d-cite>, rather than treating them indiscriminately as a generic unknown class, presents an even more formidable challenge, requiring the utilization of intrinsic structures within unseen or novel classes.</p> <p>Scaling up pre-training data with minimal human intervention has proven to be critical to foundation models, such as GLIP <d-cite key="li2022grounded"></d-cite> and SAM <d-cite key="kirillov2023segment"></d-cite>. Particularly in scenarios where manual annotation is resource-intensive <d-cite key="delatolas2024learning"></d-cite>, there’s a pressing need for an automatic data annotation framework. Such a framework should not only generate reliable pseudo labels but also continually expand the concept pool, thereby necessitating resilience to domain shifts stemming from heterogeneous data sources and open-set recognition capability. In the context of video action recognition, this task is referred to as Open-set Video Domain Adaptation (OUVDA) <d-cite key="zara2023simplifying"></d-cite> <d-cite key="zara2023autolabel"></d-cite>, which remains largely unexplored. This emerging research direction is inherently more complex due to the additional temporal dimension and the scarcity of large-scale and diverse datasets, presenting unique challenges that warrant further investigation. The closed learning loop, which involves the simultaneous evolution of model updating and dataset expansion, lays the groundwork for OWVUS capable of continual self-development over time. From a data-centric standpoint, the challenge revolves around constructing a dynamic dataset capable of consistently absorbing novel semantic categories and selecting relevant samples continually <d-cite key="de2021continual"></d-cite> and actively <d-cite key="zhang2022bamboo"></d-cite> with human-machine synergy. Continual learning <d-cite key="wang2302comprehensive"></d-cite>, characterized by rapid adaptation to evolving data distributions and the potential encounter with unseen classes while avoiding catastrophic forgetting, can thus be integrated with OUDA to fulfil this objective.</p> <p>To conclude, the prospect of unfolding possibilities and burgeoning potential in the field of OUDA and its synergies with other areas like continual learning and active learning fills me with anticipation and enthusiasm.</p> <div class="language-html highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@misc{Cai2024OpenWorld,
    author = {Xin Cai},
    title = {Random Thoughts on Open World Vision Systems},
    howpublished = {\url{https://totalvariation.github.io/blog/2024/open-world-vision-systems/}},
    note = {Accessed: 2024-02-08},
    year = {2024}
}
</code></pre></div></div>]]></content><author><name>Xin Cai</name></author><category term="Open-set"/><category term="Open-vocabulary"/><category term="UDA"/><summary type="html"><![CDATA[It is not the strongest of the species that survives, nor the most intelligent that survives. It is the one that is most adaptable to change. - Charles Darwin]]></summary></entry></feed>